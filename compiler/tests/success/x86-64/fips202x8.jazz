require "params.jinc"
u512 rho56 = 0x383F3E3D3C3B3A393037363533323131282F2E2D2C2B2A292027262524232221181F1E1D1C1B1A191017161514131211080F0E0D0C0B0A090007060504030201;
u512 rho8 = 0x3E3D3C3B3A39383F3635343332313037322E2D2C2B2A29282F2625242322212027271E1D1C1B1A19181F16151413121110170E0D0C0B0A09080F0605040302010007;
param int SHAKE128_RATE = 168;
param int SHAKE256_RATE = 136;
param int SHA3_256_RATE = 136;
param int SHA3_512_RATE = 72;

param int ba=0;
param int be=1;
param int bi=2;
param int bo=3;
param int bu=4;
param int ga=5;
param int ge=6;
param int gi=7;
param int go=8;
param int gu=9;
param int ka=10;
param int ke=11;
param int ki=12;
param int ko=13;
param int ku=14;
param int ma=15;
param int me=16;
param int mi=17;
param int mo=18;
param int mu=19;
param int sa=20;
param int se=21;
param int si=22;
param int so=23;
param int su=24;


u512[24] KeccakF1600RoundConstants = {
    0x00000000000000010000000000000001000000000000000100000000000000010000000000000001000000000000000100000000000000010000000000000001,
    0x00000000000080820000000000008082000000000000808200000000000080820000000000008082000000000000808200000000000080820000000000008082,
    0x800000000000808a800000000000808a800000000000808a800000000000808a800000000000808a800000000000808a800000000000808a800000000000808a,
    0x80000000800080008000000080008000800000008000800080000000800080008000000080008000800000008000800080000000800080008000000080008000,
    0x000000000000808b000000000000808b000000000000808b000000000000808b000000000000808b000000000000808b000000000000808b000000000000808b,
    0x00000000800000010000000080000001000000008000000100000000800000010000000080000001000000008000000100000000800000010000000080000001,
    0x80000000800080818000000080008081800000008000808180000000800080818000000080008081800000008000808180000000800080818000000080008081,
    0x80000000000080098000000000008009800000000000800980000000000080098000000000008009800000000000800980000000000080098000000000008009,
    0x000000000000008a000000000000008a000000000000008a000000000000008a000000000000008a000000000000008a000000000000008a000000000000008a,
    0x00000000000000880000000000000088000000000000008800000000000000880000000000000088000000000000008800000000000000880000000000000088,
    0x00000000800080090000000080008009000000008000800900000000800080090000000080008009000000008000800900000000800080090000000080008009,
    0x000000008000000a000000008000000a000000008000000a000000008000000a000000008000000a000000008000000a000000008000000a000000008000000a,
    0x000000008000808b000000008000808b000000008000808b000000008000808b000000008000808b000000008000808b000000008000808b000000008000808b,
    0x800000000000008b800000000000008b800000000000008b800000000000008b800000000000008b800000000000008b800000000000008b800000000000008b,
    0x80000000000080898000000000008089800000000000808980000000000080898000000000008089800000000000808980000000000080898000000000008089,
    0x80000000000080038000000000008003800000000000800380000000000080038000000000008003800000000000800380000000000080038000000000008003,
    0x80000000000080028000000000008002800000000000800280000000000080028000000000008002800000000000800280000000000080028000000000008002,
    0x80000000000000808000000000000080800000000000008080000000000000808000000000000080800000000000008080000000000000808000000000000080,
    0x000000000000800a000000000000800a000000000000800a000000000000800a000000000000800a000000000000800a000000000000800a000000000000800a,
    0x800000008000000a800000008000000a800000008000000a800000008000000a800000008000000a800000008000000a800000008000000a800000008000000a,
    0x80000000800080818000000080008081800000008000808180000000800080818000000080008081800000008000808180000000800080818000000080008081,
    0x80000000000080808000000000008080800000000000808080000000000080808000000000008080800000000000808080000000000080808000000000008080,
    0x00000000800000010000000080000001000000008000000100000000800000010000000080000001000000008000000100000000800000010000000080000001,
    0x80000000800080088000000080008008800000008000800880000000800080088000000080008008800000008000800880000000800080088000000080008008
};
inline fn __prepare_theta(reg ptr u512[25] A_8x) -> reg u512, reg u512, reg u512, reg u512, reg u512
{
    reg u512 Ca, Ce, Ci, Co, Cu;

    Ca = A_8x[sa];
    Ca = #VPXOR_512(Ca, A_8x[ma]);
    Ca = #VPXOR_512(Ca, A_8x[ka]);
    Ca = #VPXOR_512(Ca, A_8x[ga]);
    Ca = #VPXOR_512(Ca, A_8x[ba]);

    Ce = A_8x[se];
    Ce = #VPXOR_512(Ce, A_8x[me]);
    Ce = #VPXOR_512(Ce, A_8x[ke]);
    Ce = #VPXOR_512(Ce, A_8x[ge]);
    Ce = #VPXOR_512(Ce, A_8x[be]);

    Ci = A_8x[si];
    Ci = #VPXOR_512(Ci, A_8x[mi]);
    Ci = #VPXOR_512(Ci, A_8x[ki]);
    Ci = #VPXOR_512(Ci, A_8x[gi]);
    Ci = #VPXOR_512(Ci, A_8x[bi]);

    Co = A_8x[so];
    Co = #VPXOR_512(Co, A_8x[mo]);
    Co = #VPXOR_512(Co, A_8x[ko]);
    Co = #VPXOR_512(Co, A_8x[go]);
    Co = #VPXOR_512(Co, A_8x[bo]);

    Cu = A_8x[su];
    Cu = #VPXOR_512(Cu, A_8x[mu]);
    Cu = #VPXOR_512(Cu, A_8x[ku]);
    Cu = #VPXOR_512(Cu, A_8x[gu]);
    Cu = #VPXOR_512(Cu, A_8x[bu]);

    return Ca, Ce, Ci, Co, Cu;

}

inline fn __rol_8u64_rho56(reg u512 a) -> reg u512
{
	reg u512 r;

	r = #VPSHUFB_512(a, rho56);

	return r; 
}

inline fn __rol_8u64_rho8(reg u512 a) -> reg u512
{
	reg u512 r;

	r = #VPSHUFB_512(a, rho8);

	return r; 
}
inline fn __rol_8u64(reg u512 a, inline int o) -> reg u512
{
	reg u512 r;
	reg u512 t512;

	r = #VPSLL_8u64(a, o);
	t512 = #VPSRL_8u64(a, 64 - o);

	r = #VPOR_512(r,t512);

	return r; 
}

inline fn __first(reg u512 Ca, reg u512 Ce, reg u512 Ci, reg u512 Co, reg u512 Cu) ->  reg u512, reg u512, reg u512, reg u512, reg u512
{
    reg u512 Da, De, Di, Do, Du;
    reg u512 Ca1, Ce1, Ci1, Co1, Cu1;

    Ce1 = __rol_8u64(Ce, 1);
    Da = Cu ^ Ce1;

    Ci1 = __rol_8u64(Ci, 1);
    De = Ca ^ Ci1;

    Co1 = __rol_8u64(Co, 1);
    Di = Ce ^ Co1;

    Cu1 = __rol_8u64(Cu, 1);
    Do = Ci ^ Cu1;

    Ca1 = __rol_8u64(Ca, 1);
    Du = Co ^ Ca1;

    return Da, De, Di, Do, Du;
}
inline fn __second_even(
reg ptr u512[25] A_8x, reg ptr u512[25] E_8x, inline int index,
reg u512 Ca, reg u512 Ce, reg u512 Ci, reg u512 Co, reg u512 Cu,
reg u512 Da, reg u512 De, reg u512 Di, reg u512 Do, reg u512 Du) 
-> reg ptr u512[25], reg ptr u512[25], reg u512, reg u512, reg u512, reg u512, reg u512
{
    reg u512 Bba, Bbe, Bbi, Bbo, Bbu;
    reg u512 t512;

    t512 = A_8x[ba];
    t512 ^= Da;
    A_8x[ba] = t512;
    Bba = t512;

    t512 = A_8x[ge];
    t512 ^= De;
    A_8x[ge] = t512;
    Bbe = __rol_8u64(t512, 44);

    t512 = A_8x[ki];
    t512 ^= Di;
    A_8x[ki] = t512;
    Bbi = __rol_8u64(t512, 43);

    t512 = #VPANDN_512(Bbe, Bbi);
    t512 ^= Bba;
    t512 ^= KeccakF1600RoundConstants[index];
    E_8x[ba] = t512;

    Ca = t512;

    t512 = A_8x[mo];
    t512 ^= Do;
    A_8x[mo] = t512;
    Bbo = __rol_8u64(t512, 21);

    t512 = #VPANDN_512(Bbi, Bbo);
    t512 ^= Bbe;
    E_8x[be] = t512;

    Ce = t512;

    t512 = A_8x[su];
    t512 ^= Du;
    A_8x[su] = t512;
    Bbu = __rol_8u64(t512, 14);

    t512 = #VPANDN_512(Bbo, Bbu);
    t512 ^= Bbi;
    E_8x[bi] = t512;

    Ci = t512;

    t512 = #VPANDN_512(Bbu, Bba);
    t512 ^= Bbo;
    E_8x[bo] = t512; 
    
    Co = t512; 
    
  
    t512 = #VPANDN_512(Bba, Bbe);
    t512 ^= Bbu; 
    E_8x[bu] = t512;

    Cu = t512;

    return A_8x, E_8x, Ca, Ce, Ci, Co, Cu;
}
inline fn __third_even(
reg ptr u512[25] A_8x, reg ptr u512[25] E_8x,
reg u512 Ca, reg u512 Ce, reg u512 Ci, reg u512 Co, reg u512 Cu,
reg u512 Da, reg u512 De, reg u512 Di, reg u512 Do, reg u512 Du) 
-> reg ptr u512[25], reg ptr u512[25], reg u512, reg u512, reg u512, reg u512, reg u512
{
    reg u512 Bga, Bge, Bgi, Bgo, Bgu;
    reg u512 t512;

    t512 = A_8x[bo];
    t512 ^= Do;
    A_8x[bo] = t512;
    Bga = __rol_8u64(t512, 28);

    t512 = A_8x[gu];
    t512 ^= Du;
    A_8x[gu] = t512;
    Bge = __rol_8u64(t512, 20);

    t512 = A_8x[ka];
    t512 ^= Da;
    A_8x[ka] = t512;
    Bgi = __rol_8u64(t512, 3);

    // E##ga = XOR512(Bga, ANDnu512(Bge, Bgi))
    t512 = #VPANDN_512(Bge, Bgi);
    t512 ^= Bga;
    E_8x[ga] = t512;

    Ca ^= t512;

    t512 = A_8x[me];
    t512 ^= De;
    A_8x[me] = t512;
    Bgo = __rol_8u64(t512, 45);

    // E##ge = XOR512(Bge, ANDnu512(Bgi, Bgo))
    t512 = #VPANDN_512(Bgi, Bgo);
    t512 ^= Bge;
    E_8x[ge] = t512;

    Ce ^= t512;

    t512 = A_8x[si];
    t512 ^= Di;
    A_8x[si] = t512;
    Bgu = __rol_8u64(t512, 61);

    //  E##gi = XOR512(Bgi, ANDnu512(Bgo, Bgu))
    t512 = #VPANDN_512(Bgo, Bgu);
    t512 ^= Bgi;
    E_8x[gi] = t512;
    
    Ci ^= t512;

    // E##go = XOR512(Bgo, ANDnu512(Bgu, Bga));
    t512 = #VPANDN_512(Bgu, Bga);
    t512 ^= Bgo;
    E_8x[go] = t512;
    
    Co ^= t512;

    // E##gu = XOR512(Bgu, ANDnu512(Bga, Bge));
    t512 = #VPANDN_512(Bga, Bge);
    t512 ^= Bgu;
    E_8x[gu] = t512;
    
    Cu ^= t512;

    return A_8x, E_8x, Ca, Ce, Ci, Co, Cu;
}

inline fn __fourth_even(
reg ptr u512[25] A_8x, reg ptr u512[25] E_8x,
reg u512 Ca, reg u512 Ce, reg u512 Ci, reg u512 Co, reg u512 Cu,
reg u512 Da, reg u512 De, reg u512 Di, reg u512 Do, reg u512 Du) 
-> reg ptr u512[25], reg ptr u512[25], reg u512, reg u512, reg u512, reg u512, reg u512
{
    reg u512 Bka, Bke, Bki, Bko, Bku;
    reg u512 t512;

    t512 = A_8x[be];
    t512 ^= De;
    A_8x[be] = t512;
    Bka = __rol_8u64(t512, 1);

    t512 = A_8x[gi];
    t512 ^= Di;
    A_8x[gi] = t512;
    Bke = __rol_8u64(t512, 6);

    t512 = A_8x[ko];
    t512 ^= Do;
    A_8x[ko] = t512;
    Bki = __rol_8u64(t512, 25);

    // E##ka = XOR512(Bka, ANDnu512(Bke, Bki));
    t512 = #VPANDN_512(Bke, Bki);
    t512 ^= Bka;
    E_8x[ka] = t512;

    Ca ^= t512;
    
    t512 = A_8x[mu];
    t512 ^= Du;
    A_8x[mu] = t512;
    Bko = __rol_8u64_rho8(t512);

    // E##ke = XOR512(Bke, ANDnu512(Bki, Bko));
    t512 = #VPANDN_512(Bki, Bko);
    t512 ^= Bke;
    E_8x[ke] = t512;

    Ce ^= t512;
    
    t512 = A_8x[sa];
    t512 ^= Da;
    A_8x[sa] = t512;
    Bku = __rol_8u64(t512, 18);

    // E##ki = XOR512(Bki, ANDnu512(Bko, Bku))
    t512 = #VPANDN_512(Bko, Bku);
    t512 ^= Bki;
    E_8x[ki] = t512;

    Ci ^= t512;

    //  E##ko = XOR512(Bko, ANDnu512(Bku, Bka));
    t512 = #VPANDN_512(Bku, Bka);
    t512 ^= Bko;
    E_8x[ko] = t512;

    Co ^= t512;

    //  E##ku = XOR512(Bku, ANDnu512(Bka, Bke));
    t512 = #VPANDN_512(Bka, Bke);
    t512 ^= Bku;
    E_8x[ku] = t512;

    Cu ^= t512;

    return A_8x, E_8x, Ca, Ce, Ci, Co, Cu;
}


inline fn __fifth_even(
reg ptr u512[25] A_8x, reg ptr u512[25] E_8x,
reg u512 Ca, reg u512 Ce, reg u512 Ci, reg u512 Co, reg u512 Cu,
reg u512 Da, reg u512 De, reg u512 Di, reg u512 Do, reg u512 Du) 
-> reg ptr u512[25], reg ptr u512[25], reg u512, reg u512, reg u512, reg u512, reg u512
{
    reg u512 Bma, Bme, Bmi, Bmo, Bmu;
    reg u512 t512;

    t512 = A_8x[bu];
    t512 ^= Du;
    A_8x[bu] = t512;
    Bma = __rol_8u64(t512, 27);

    t512 = A_8x[ga];
    t512 ^= Da;
    A_8x[ga] = t512;
    Bme = __rol_8u64(t512, 36);

    t512 = A_8x[ke];
    t512 ^= De;
    A_8x[ke] = t512;
    Bmi = __rol_8u64(t512, 10);

    // E##ma = XOR512(Bma, ANDnu512(Bme, Bmi));
    t512 = #VPANDN_512(Bme, Bmi);
    t512 ^= Bma;
    E_8x[ma] = t512;

    Ca ^= t512;

    t512 = A_8x[mi];
    t512 ^= Di;
    A_8x[mi] = t512;
    Bmo = __rol_8u64(t512, 15);

    // E##me = XOR512(Bme, ANDnu512(Bmi, Bmo));
    t512 = #VPANDN_512(Bmi, Bmo);
    t512 ^= Bme;
    E_8x[me] = t512;

    Ce ^= t512;

    t512 = A_8x[so];
    t512 ^= Do;
    A_8x[so] = t512;
    Bmu = __rol_8u64_rho56(t512);

    // E##mi = XOR512(Bmi, ANDnu512(Bmo, Bmu));
    t512 = #VPANDN_512(Bmo, Bmu);
    t512 ^= Bmi;
    E_8x[mi] = t512;

    Ci ^= t512;

    // E##mo = XOR512(Bmo, ANDnu512(Bmu, Bma));
    t512 = #VPANDN_512(Bmu, Bma);
    t512 ^= Bmo;
    E_8x[mo] = t512;

    Co ^= t512;

    // E##mu = XOR512(Bmu, ANDnu512(Bma, Bme));
    t512 = #VPANDN_512(Bma, Bme);
    t512 ^= Bmu;
    E_8x[mu] = t512;

    Cu ^= t512;

    return A_8x, E_8x, Ca, Ce, Ci, Co, Cu;
}

inline fn __sixth_even(
reg ptr u512[25] A_8x, reg ptr u512[25] E_8x,
reg u512 Ca, reg u512 Ce, reg u512 Ci, reg u512 Co, reg u512 Cu,
reg u512 Da, reg u512 De, reg u512 Di, reg u512 Do, reg u512 Du) 
-> reg ptr u512[25], reg ptr u512[25], reg u512, reg u512, reg u512, reg u512, reg u512
{
    reg u512 Bsa, Bse, Bsi, Bso, Bsu;
    reg u512 t512;

    t512 = A_8x[bi];
    t512 ^= Di;
    A_8x[bi] = t512;
    Bsa = __rol_8u64(t512, 62);

    t512 = A_8x[go];
    t512 ^= Do;
    A_8x[go] = t512;
    Bse = __rol_8u64(t512, 55);

    t512 = A_8x[ku];
    t512 ^= Du;
    A_8x[ku] = t512;
    Bsi = __rol_8u64(t512, 39);

    // E##sa = XOR512(Bsa, ANDnu512(Bse, Bsi));
    t512 = #VPANDN_512(Bse, Bsi);
    t512 ^= Bsa;
    E_8x[sa] = t512;

    Ca ^= t512;

    t512 = A_8x[ma];
    t512 ^= Da;
    A_8x[ma] = t512;
    Bso = __rol_8u64(t512, 41);

    // E##se = XOR512(Bse, ANDnu512(Bsi, Bso))
    t512 = #VPANDN_512(Bsi, Bso);
    t512 ^= Bse;
    E_8x[se] = t512;  

    Ce ^= t512;

    t512 = A_8x[se];
    t512 ^= De;
    A_8x[se] = t512;
    Bsu = __rol_8u64(t512, 2);

    // E##si = XOR512(Bsi, ANDnu512(Bso, Bsu)); 
    t512 = #VPANDN_512(Bso, Bsu);
    t512 ^= Bsi;
    E_8x[si] = t512;

    Ci ^= t512;

    // E##so = XOR512(Bso, ANDnu512(Bsu, Bsa));
    t512 = #VPANDN_512(Bsu, Bsa);
    t512 ^= Bso;
    E_8x[so] = t512;

    Co ^= t512;

    // E##su = XOR512(Bsu, ANDnu512(Bsa, Bse));
    t512 = #VPANDN_512(Bsa, Bse);
    t512 ^= Bsu;
    E_8x[su] = t512;

    Cu ^= t512;

    return A_8x, E_8x, Ca, Ce, Ci, Co, Cu;
}


inline fn __theta_rho_pi_chi_iota_prepare_theta_even(
reg ptr u512[25] A_8x, reg ptr u512[25] E_8x, inline int index,
reg u512 Ca, reg u512 Ce, reg u512 Ci, reg u512 Co, reg u512 Cu) 
-> reg ptr u512[25], reg ptr u512[25], reg u512, reg u512, reg u512, reg u512, reg u512
{   
    reg u512 Da, De, Di, Do, Du;

    Da, De, Di, Do, Du = __first(Ca, Ce, Ci, Co, Cu);

    A_8x, E_8x, Ca, Ce, Ci, Co, Cu = __second_even(A_8x, E_8x, index, Ca, Ce, Ci, Co, Cu, Da, De, Di, Do, Du);

    A_8x, E_8x, Ca, Ce, Ci, Co, Cu = __third_even(A_8x, E_8x, Ca, Ce, Ci, Co, Cu, Da, De, Di, Do, Du);
  
    A_8x, E_8x, Ca, Ce, Ci, Co, Cu = __fourth_even(A_8x, E_8x, Ca, Ce, Ci, Co, Cu, Da, De, Di, Do, Du);

    A_8x, E_8x, Ca, Ce, Ci, Co, Cu = __fifth_even(A_8x, E_8x, Ca, Ce, Ci, Co, Cu, Da, De, Di, Do, Du);

    A_8x, E_8x, Ca, Ce, Ci, Co, Cu = __sixth_even(A_8x, E_8x, Ca, Ce, Ci, Co, Cu, Da, De, Di, Do, Du);

    return A_8x, E_8x, Ca, Ce, Ci, Co, Cu;
}

inline fn __second_odd(
reg ptr u512[25] A_8x, reg ptr u512[25] E_8x, inline int index,
reg u512 Ca, reg u512 Ce, reg u512 Ci, reg u512 Co, reg u512 Cu,
reg u512 Da, reg u512 De, reg u512 Di, reg u512 Do, reg u512 Du) 
-> reg ptr u512[25], reg ptr u512[25], reg u512, reg u512, reg u512, reg u512, reg u512
{
    reg u512 Bba, Bbe, Bbi, Bbo, Bbu;
    reg u512 t512;

    t512 = A_8x[ba];
    t512 ^= Da;
    A_8x[ba] = t512;
    Bba = t512;

    t512 = A_8x[ge];
    t512 ^= De;
    A_8x[ge] = t512;
    Bbe = __rol_8u64(t512, 44);

    t512 = A_8x[ki];
    t512 ^= Di;
    A_8x[ki] = t512;
    Bbi = __rol_8u64(t512, 43);

    // E##ba = XOR512(Bba, ANDnu512(Bbe, Bbi)); XOReq512(E##ba, CONST512_64(KeccakF1600RoundConstants[i]));
    t512 = #VPANDN_512(Bbe, Bbi);
    t512 ^= Bba;
    t512 ^= KeccakF1600RoundConstants[index];
    E_8x[ba] = t512;

    Ca = t512;

    t512 = A_8x[mo];
    t512 ^= Do;
    A_8x[mo] = t512;
    Bbo = __rol_8u64(t512, 21);

    //  E##be = XOR512(Bbe, ANDnu512(Bbi, Bbo));
    t512 = #VPANDN_512(Bbi, Bbo);
    t512 ^= Bbe;
    E_8x[be] = t512;

    Ce = t512;

    t512 = A_8x[su];
    t512 ^= Du;
    A_8x[su] = t512;
    Bbu = __rol_8u64(t512, 14);

    // E##bi = XOR512(Bbi, ANDnu512(Bbo, Bbu)); 
    t512 = #VPANDN_512(Bbo, Bbu);
    t512 ^= Bbi;
    E_8x[bi] = t512;

    Ci = t512;

    // E##bo = XOR512(Bbo, ANDnu512(Bbu, Bba));
    t512 = #VPANDN_512(Bbu, Bba);
    t512 ^= Bbo;
    E_8x[bo] = t512; 
    
    Co = t512; 
    
    // E##bu = XOR512(Bbu, ANDnu512(Bba, Bbe));
    t512 = #VPANDN_512(Bba, Bbe);
    t512 ^= Bbu; 
    E_8x[bu] = t512;

    Cu = t512;

    return A_8x, E_8x, Ca, Ce, Ci, Co, Cu;
}

inline fn __third_odd(
reg ptr u512[25] A_8x, reg ptr u512[25] E_8x,
reg u512 Ca, reg u512 Ce, reg u512 Ci, reg u512 Co, reg u512 Cu,
reg u512 Da, reg u512 De, reg u512 Di, reg u512 Do, reg u512 Du) 
-> reg ptr u512[25], reg ptr u512[25], reg u512, reg u512, reg u512, reg u512, reg u512
{
    reg u512 Bga, Bge, Bgi, Bgo, Bgu;
    reg u512 t512;

    t512 = A_8x[bo];
    t512 ^= Do;
    A_8x[bo] = t512;
    Bga = __rol_8u64(t512, 28);

    t512 = A_8x[gu];
    t512 ^= Du;
    A_8x[gu] = t512;
    Bge = __rol_8u64(t512, 20);

    t512 = A_8x[ka];
    t512 ^= Da;
    A_8x[ka] = t512;
    Bgi = __rol_8u64(t512, 3);   

    // E##ga = XOR512(Bga, ANDnu512(Bge, Bgi))
    t512 = #VPANDN_512(Bge, Bgi);
    t512 ^= Bga;
    E_8x[ga] = t512;

    Ca ^= t512;

    t512 = A_8x[me];
    t512 ^= De;
    A_8x[me] = t512;
    Bgo = __rol_8u64(t512, 45);

    // E##ge = XOR512(Bge, ANDnu512(Bgi, Bgo))
    t512 = #VPANDN_512(Bgi, Bgo);
    t512 ^= Bge;
    E_8x[ge] = t512;

    Ce ^= t512;

    t512 = A_8x[si];
    t512 ^= Di;
    A_8x[si] = t512;
    Bgu = __rol_8u64(t512, 61);

    //  E##gi = XOR512(Bgi, ANDnu512(Bgo, Bgu))
    t512 = #VPANDN_512(Bgo, Bgu);
    t512 ^= Bgi;
    E_8x[gi] = t512;
    
    Ci ^= t512;

    // E##go = XOR512(Bgo, ANDnu512(Bgu, Bga));
    t512 = #VPANDN_512(Bgu, Bga);
    t512 ^= Bgo;
    E_8x[go] = t512;
    
    Co ^= t512;

    // E##gu = XOR512(Bgu, ANDnu512(Bga, Bge));
    t512 = #VPANDN_512(Bga, Bge);
    t512 ^= Bgu;
    E_8x[gu] = t512;
    
    Cu ^= t512;

    return A_8x, E_8x, Ca, Ce, Ci, Co, Cu;
}


inline fn __fourth_odd(
reg ptr u512[25] A_8x, reg ptr u512[25] E_8x,
reg u512 Ca, reg u512 Ce, reg u512 Ci, reg u512 Co, reg u512 Cu,
reg u512 Da, reg u512 De, reg u512 Di, reg u512 Do, reg u512 Du) 
-> reg ptr u512[25], reg ptr u512[25], reg u512, reg u512, reg u512, reg u512, reg u512
{
    reg u512 Bka, Bke, Bki, Bko, Bku;
    reg u512 t512;

    t512 = A_8x[be];
    t512 ^= De;
    A_8x[be] = t512;
    Bka = __rol_8u64(t512, 1);

    t512 = A_8x[gi];
    t512 ^= Di;
    A_8x[gi] = t512;
    Bke = __rol_8u64(t512, 6);

    t512 = A_8x[ko];
    t512 ^= Do;
    A_8x[ko] = t512;
    Bki = __rol_8u64(t512, 25);

    // E##ka = XOR512(Bka, ANDnu512(Bke, Bki));
    t512 = #VPANDN_512(Bke, Bki);
    t512 ^= Bka;
    E_8x[ka] = t512;

    Ca ^= t512;
    
    t512 = A_8x[mu];
    t512 ^= Du;
    A_8x[mu] = t512;
    Bko = __rol_8u64_rho8(t512);

    // E##ke = XOR512(Bke, ANDnu512(Bki, Bko));
    t512 = #VPANDN_512(Bki, Bko);
    t512 ^= Bke;
    E_8x[ke] = t512;

    Ce ^= t512;
    
    t512 = A_8x[sa];
    t512 ^= Da;
    A_8x[sa] = t512;
    Bku = __rol_8u64(t512, 18);

    // E##ki = XOR512(Bki, ANDnu512(Bko, Bku))
    t512 = #VPANDN_512(Bko, Bku);
    t512 ^= Bki;
    E_8x[ki] = t512;

    Ci ^= t512;

    //  E##ko = XOR512(Bko, ANDnu512(Bku, Bka));
    t512 = #VPANDN_512(Bku, Bka);
    t512 ^= Bko;
    E_8x[ko] = t512;

    Co ^= t512;

    //  E##ku = XOR512(Bku, ANDnu512(Bka, Bke));
    t512 = #VPANDN_512(Bka, Bke);
    t512 ^= Bku;
    E_8x[ku] = t512;

    Cu ^= t512;

    return A_8x, E_8x, Ca, Ce, Ci, Co, Cu;
}

inline fn __fifth_odd(
reg ptr u512[25] A_8x, reg ptr u512[25] E_8x,
reg u512 Ca, reg u512 Ce, reg u512 Ci, reg u512 Co, reg u512 Cu,
reg u512 Da, reg u512 De, reg u512 Di, reg u512 Do, reg u512 Du) 
-> reg ptr u512[25], reg ptr u512[25], reg u512, reg u512, reg u512, reg u512, reg u512
{
    reg u512 Bma, Bme, Bmi, Bmo, Bmu;
    reg u512 t512;

    t512 = A_8x[bu];
    t512 ^= Du;
    A_8x[bu] = t512;
    Bma = __rol_8u64(t512, 27);

    t512 = A_8x[ga];
    t512 ^= Da;
    A_8x[ga] = t512;
    Bme = __rol_8u64(t512, 36);

    t512 = A_8x[ke];
    t512 ^= De;
    A_8x[ke] = t512;
    Bmi = __rol_8u64(t512, 10);

    // E##ma = XOR512(Bma, ANDnu512(Bme, Bmi));
    t512 = #VPANDN_512(Bme, Bmi);
    t512 ^= Bma;
    E_8x[ma] = t512;

    Ca ^= t512;

    t512 = A_8x[mi];
    t512 ^= Di;
    A_8x[mi] = t512;
    Bmo = __rol_8u64(t512, 15);

    // E##me = XOR512(Bme, ANDnu512(Bmi, Bmo));
    t512 = #VPANDN_512(Bmi, Bmo);
    t512 ^= Bme;
    E_8x[me] = t512;

    Ce ^= t512;

    t512 = A_8x[so];
    t512 ^= Do;
    A_8x[so] = t512;
    Bmu = __rol_8u64_rho56(t512);

    // E##mi = XOR512(Bmi, ANDnu512(Bmo, Bmu));
    t512 = #VPANDN_512(Bmo, Bmu);
    t512 ^= Bmi;
    E_8x[mi] = t512;

    Ci ^= t512;

    // E##mo = XOR512(Bmo, ANDnu512(Bmu, Bma));
    t512 = #VPANDN_512(Bmu, Bma);
    t512 ^= Bmo;
    E_8x[mo] = t512;

    Co ^= t512;

    // E##mu = XOR512(Bmu, ANDnu512(Bma, Bme));
    t512 = #VPANDN_512(Bma, Bme);
    t512 ^= Bmu;
    E_8x[mu] = t512;

    Cu ^= t512;

    return A_8x, E_8x, Ca, Ce, Ci, Co, Cu;
}

inline fn __sixth_odd(
reg ptr u512[25] A_8x, reg ptr u512[25] E_8x,
reg u512 Ca, reg u512 Ce, reg u512 Ci, reg u512 Co, reg u512 Cu,
reg u512 Da, reg u512 De, reg u512 Di, reg u512 Do, reg u512 Du) 
-> reg ptr u512[25], reg ptr u512[25], reg u512, reg u512, reg u512, reg u512, reg u512
{
    reg u512 Bsa, Bse, Bsi, Bso, Bsu;
    reg u512 t512;

    t512 = A_8x[bi];
    t512 ^= Di;
    A_8x[bi] = t512;
    Bsa = __rol_8u64(t512, 62);

    t512 = A_8x[go];
    t512 ^= Do;
    A_8x[go] = t512;
    Bse = __rol_8u64(t512, 55);

    t512 = A_8x[ku];
    t512 ^= Du;
    A_8x[ku] = t512;
    Bsi = __rol_8u64(t512, 39);

    // E##sa = XOR512(Bsa, ANDnu512(Bse, Bsi));
    t512 = #VPANDN_512(Bse, Bsi);
    t512 ^= Bsa;
    E_8x[sa] = t512;

    Ca ^= t512;

    t512 = A_8x[ma];
    t512 ^= Da;
    A_8x[ma] = t512;
    Bso = __rol_8u64(t512, 41);

    // E##se = XOR512(Bse, ANDnu512(Bsi, Bso))
    t512 = #VPANDN_512(Bsi, Bso);
    t512 ^= Bse;
    E_8x[se] = t512;  

    Ce ^= t512;

    t512 = A_8x[se];
    t512 ^= De;
    A_8x[se] = t512;
    Bsu = __rol_8u64(t512, 2);

    // E##si = XOR512(Bsi, ANDnu512(Bso, Bsu)); 
    t512 = #VPANDN_512(Bso, Bsu);
    t512 ^= Bsi;
    E_8x[si] = t512;

    Ci ^= t512;

    // E##so = XOR512(Bso, ANDnu512(Bsu, Bsa));
    t512 = #VPANDN_512(Bsu, Bsa);
    t512 ^= Bso;
    E_8x[so] = t512;

    Co ^= t512;

    // E##su = XOR512(Bsu, ANDnu512(Bsa, Bse));
    t512 = #VPANDN_512(Bsa, Bse);
    t512 ^= Bsu;
    E_8x[su] = t512;

    Cu ^= t512;

    return A_8x, E_8x, Ca, Ce, Ci, Co, Cu;
}


inline fn __theta_rho_pi_chi_iota_prepare_theta_odd(
reg ptr u512[25] A_8x, reg ptr u512[25] E_8x, inline int index,
reg u512 Ca, reg u512 Ce, reg u512 Ci, reg u512 Co, reg u512 Cu) 
-> reg ptr u512[25], reg ptr u512[25], reg u512, reg u512, reg u512, reg u512, reg u512
{   
    reg u512 Da, De, Di, Do, Du;

    Da, De, Di, Do, Du = __first(Ca, Ce, Ci, Co, Cu);

    A_8x, E_8x, Ca, Ce, Ci, Co, Cu = __second_odd(A_8x, E_8x, index, Ca, Ce, Ci, Co, Cu, Da, De, Di, Do, Du);

    A_8x, E_8x, Ca, Ce, Ci, Co, Cu = __third_odd(A_8x, E_8x, Ca, Ce, Ci, Co, Cu, Da, De, Di, Do, Du);
  
    A_8x, E_8x, Ca, Ce, Ci, Co, Cu = __fourth_odd(A_8x, E_8x, Ca, Ce, Ci, Co, Cu, Da, De, Di, Do, Du);

    A_8x, E_8x, Ca, Ce, Ci, Co, Cu = __fifth_odd(A_8x, E_8x, Ca, Ce, Ci, Co, Cu, Da, De, Di, Do, Du);

    A_8x, E_8x, Ca, Ce, Ci, Co, Cu = __sixth_odd(A_8x, E_8x, Ca, Ce, Ci, Co, Cu, Da, De, Di, Do, Du);

    return A_8x, E_8x, Ca, Ce, Ci, Co, Cu;
}
inline fn __second_last(
reg ptr u512[25] A_8x, reg ptr u512[25] E_8x, inline int index,
reg u512 Da, reg u512 De, reg u512 Di, reg u512 Do, reg u512 Du) 
-> reg ptr u512[25], reg ptr u512[25]
{
    reg u512 Bba, Bbe, Bbi, Bbo, Bbu;
    reg u512 t512;

    t512 = A_8x[ba];
    t512 ^= Da;
    A_8x[ba] = t512;
    Bba = t512;

    t512 = A_8x[ge];
    t512 ^= De;
    A_8x[ge] = t512;
    Bbe = __rol_8u64(t512, 44);

    t512 = A_8x[ki];
    t512 ^= Di;
    A_8x[ki] = t512;
    Bbi = __rol_8u64(t512, 43);

    // E##ba = XOR512(Bba, ANDnu512(Bbe, Bbi)); XOReq512(E##ba, CONSt512_64(KeccakF1600RoundConstants[i]));
    t512 = #VPANDN_512(Bbe, Bbi);
    t512 ^= Bba;
    t512 ^= KeccakF1600RoundConstants[index];
    E_8x[ba] = t512;

    t512 = A_8x[mo];
    t512 ^= Do;
    A_8x[mo] = t512;
    Bbo = __rol_8u64(t512, 21);

    //  E##be = XOR512(Bbe, ANDnu512(Bbi, Bbo));
    t512 = #VPANDN_512(Bbi, Bbo);
    t512 ^= Bbe;
    E_8x[be] = t512;

    t512 = A_8x[su];
    t512 ^= Du;
    A_8x[su] = t512;
    Bbu = __rol_8u64(t512, 14);

    // E##bi = XOR512(Bbi, ANDnu512(Bbo, Bbu)); 
    t512 = #VPANDN_512(Bbo, Bbu);
    t512 ^= Bbi;
    E_8x[bi] = t512;

    // E##bo = XOR512(Bbo, ANDnu512(Bbu, Bba));
    t512 = #VPANDN_512(Bbu, Bba);
    t512 ^= Bbo;
    E_8x[bo] = t512;
    
    // E##bu = XOR512(Bbu, ANDnu512(Bba, Bbe));
    t512 = #VPANDN_512(Bba, Bbe);
    t512 ^= Bbu; 
    E_8x[bu] = t512;

    return A_8x, E_8x;
}

inline fn __third_last(
reg ptr u512[25] A_8x, reg ptr u512[25] E_8x,
reg u512 Da, reg u512 De, reg u512 Di, reg u512 Do, reg u512 Du) 
-> reg ptr u512[25], reg ptr u512[25]
{
    reg u512 Bga, Bge, Bgi, Bgo, Bgu;
    reg u512 t512;

    t512 = A_8x[bo];
    t512 ^= Do;
    A_8x[bo] = t512;
    Bga = __rol_8u64(t512, 28);

    t512 = A_8x[gu];
    t512 ^= Du;
    A_8x[gu] = t512;
    Bge = __rol_8u64(t512, 20);

    t512 = A_8x[ka];
    t512 ^= Da;
    A_8x[ka] = t512;
    Bgi = __rol_8u64(t512, 3);   

    // E##ga = XOR512(Bga, ANDnu512(Bge, Bgi))
    t512 = #VPANDN_512(Bge, Bgi);
    t512 ^= Bga;
    E_8x[ga] = t512;

    t512 = A_8x[me];
    t512 ^= De;
    A_8x[me] = t512;
    Bgo = __rol_8u64(t512, 45);

    // E##ge = XOR512(Bge, ANDnu512(Bgi, Bgo))
    t512 = #VPANDN_512(Bgi, Bgo);
    t512 ^= Bge;
    E_8x[ge] = t512;

    t512 = A_8x[si];
    t512 ^= Di;
    A_8x[si] = t512;
    Bgu = __rol_8u64(t512, 61);

    //  E##gi = XOR512(Bgi, ANDnu512(Bgo, Bgu))
    t512 = #VPANDN_512(Bgo, Bgu);
    t512 ^= Bgi;
    E_8x[gi] = t512;

    // E##go = XOR512(Bgo, ANDnu512(Bgu, Bga));
    t512 = #VPANDN_512(Bgu, Bga);
    t512 ^= Bgo;
    E_8x[go] = t512;

    // E##gu = XOR512(Bgu, ANDnu512(Bga, Bge));
    t512 = #VPANDN_512(Bga, Bge);
    t512 ^= Bgu;
    E_8x[gu] = t512;

    return A_8x, E_8x;
}

inline fn __fourth_last(
reg ptr u512[25] A_8x, reg ptr u512[25] E_8x,
reg u512 Da, reg u512 De, reg u512 Di, reg u512 Do, reg u512 Du) 
-> reg ptr u512[25], reg ptr u512[25]
{
    reg u512 Bka, Bke, Bki, Bko, Bku;
    reg u512 t512;

    t512 = A_8x[be];
    t512 ^= De;
    A_8x[be] = t512;
    Bka = __rol_8u64(t512, 1);

    t512 = A_8x[gi];
    t512 ^= Di;
    A_8x[gi] = t512;
    Bke = __rol_8u64(t512, 6);

    t512 = A_8x[ko];
    t512 ^= Do;
    A_8x[ko] = t512;
    Bki = __rol_8u64(t512, 25);

    // E##ka = XOR512(Bka, ANDnu512(Bke, Bki));
    t512 = #VPANDN_512(Bke, Bki);
    t512 ^= Bka;
    E_8x[ka] = t512;
    
    t512 = A_8x[mu];
    t512 ^= Du;
    A_8x[mu] = t512;
    Bko = __rol_8u64_rho8(t512);

    // E##ke = XOR512(Bke, ANDnu512(Bki, Bko));
    t512 = #VPANDN_512(Bki, Bko);
    t512 ^= Bke;
    E_8x[ke] = t512;
    
    t512 = A_8x[sa];
    t512 ^= Da;
    A_8x[sa] = t512;
    Bku = __rol_8u64(t512, 18);

    // E##ki = XOR512(Bki, ANDnu512(Bko, Bku))
    t512 = #VPANDN_512(Bko, Bku);
    t512 ^= Bki;
    E_8x[ki] = t512;

    //  E##ko = XOR512(Bko, ANDnu512(Bku, Bka));
    t512 = #VPANDN_512(Bku, Bka);
    t512 ^= Bko;
    E_8x[ko] = t512;

    //  E##ku = XOR512(Bku, ANDnu512(Bka, Bke));
    t512 = #VPANDN_512(Bka, Bke);
    t512 ^= Bku;
    E_8x[ku] = t512;

    return A_8x, E_8x;
}

inline fn __fifth_last(
reg ptr u512[25] A_8x, reg ptr u512[25] E_8x,
reg u512 Da, reg u512 De, reg u512 Di, reg u512 Do, reg u512 Du) 
-> reg ptr u512[25], reg ptr u512[25]
{
    reg u512 Bma, Bme, Bmi, Bmo, Bmu;
    reg u512 t512;

    t512 = A_8x[bu];
    t512 ^= Du;
    A_8x[bu] = t512;
    Bma = __rol_8u64(t512, 27);

    t512 = A_8x[ga];
    t512 ^= Da;
    A_8x[ga] = t512;
    Bme = __rol_8u64(t512, 36);

    t512 = A_8x[ke];
    t512 ^= De;
    A_8x[ke] = t512;
    Bmi = __rol_8u64(t512, 10);

    // E##ma = XOR512(Bma, ANDnu512(Bme, Bmi));
    t512 = #VPANDN_512(Bme, Bmi);
    t512 ^= Bma;
    E_8x[ma] = t512;

    t512 = A_8x[mi];
    t512 ^= Di;
    A_8x[mi] = t512;
    Bmo = __rol_8u64(t512, 15);

    // E##me = XOR512(Bme, ANDnu512(Bmi, Bmo));
    t512 = #VPANDN_512(Bmi, Bmo);
    t512 ^= Bme;
    E_8x[me] = t512;

    t512 = A_8x[so];
    t512 ^= Do;
    A_8x[so] = t512;
    Bmu = __rol_8u64_rho56(t512);

    // E##mi = XOR512(Bmi, ANDnu512(Bmo, Bmu));
    t512 = #VPANDN_512(Bmo, Bmu);
    t512 ^= Bmi;
    E_8x[mi] = t512;

    // E##mo = XOR512(Bmo, ANDnu512(Bmu, Bma));
    t512 = #VPANDN_512(Bmu, Bma);
    t512 ^= Bmo;
    E_8x[mo] = t512;

    // E##mu = XOR512(Bmu, ANDnu512(Bma, Bme));
    t512 = #VPANDN_512(Bma, Bme);
    t512 ^= Bmu;
    E_8x[mu] = t512;

    return A_8x, E_8x;
}

inline fn __sixth_last(
reg ptr u512[25] A_8x, reg ptr u512[25] E_8x,
reg u512 Da, reg u512 De, reg u512 Di, reg u512 Do, reg u512 Du) 
-> reg ptr u512[25], reg ptr u512[25]
{
    reg u512 Bsa, Bse, Bsi, Bso, Bsu;
    reg u512 t512;

    t512 = A_8x[bi];
    t512 ^= Di;
    A_8x[bi] = t512;
    Bsa = __rol_8u64(t512, 62);

    t512 = A_8x[go];
    t512 ^= Do;
    A_8x[go] = t512;
    Bse = __rol_8u64(t512, 55);

    t512 = A_8x[ku];
    t512 ^= Du;
    A_8x[ku] = t512;
    Bsi = __rol_8u64(t512, 39);

    // E##sa = XOR512(Bsa, ANDnu512(Bse, Bsi));
    t512 = #VPANDN_512(Bse, Bsi);
    t512 ^= Bsa;
    E_8x[sa] = t512;

    t512 = A_8x[ma];
    t512 ^= Da;
    A_8x[ma] = t512;
    Bso = __rol_8u64(t512, 41);

    // E##se = XOR512(Bse, ANDnu512(Bsi, Bso))
    t512 = #VPANDN_512(Bsi, Bso);
    t512 ^= Bse;
    E_8x[se] = t512;

    t512 = A_8x[se];
    t512 ^= De;
    A_8x[se] = t512;
    Bsu = __rol_8u64(t512, 2);

    // E##si = XOR512(Bsi, ANDnu512(Bso, Bsu)); 
    t512 = #VPANDN_512(Bso, Bsu);
    t512 ^= Bsi;
    E_8x[si] = t512;

    // E##so = XOR512(Bso, ANDnu512(Bsu, Bsa));
    t512 = #VPANDN_512(Bsu, Bsa);
    t512 ^= Bso;
    E_8x[so] = t512;

    // E##su = XOR512(Bsu, ANDnu512(Bsa, Bse));
    t512 = #VPANDN_512(Bsa, Bse);
    t512 ^= Bsu;
    E_8x[su] = t512;

    return A_8x, E_8x;
}

inline fn __theta_rho_pi_chi_iota(
reg ptr u512[25] A_8x, reg ptr u512[25] E_8x, inline int index,
reg u512 Ca, reg u512 Ce, reg u512 Ci, reg u512 Co, reg u512 Cu) 
-> reg ptr u512[25], reg ptr u512[25]
{
    reg u512 Da, De, Di, Do, Du;

    Da, De, Di, Do, Du = __first(Ca, Ce, Ci, Co, Cu);

    A_8x, E_8x = __second_last(A_8x, E_8x, index, Da, De, Di, Do, Du);

    A_8x, E_8x = __third_last(A_8x, E_8x, Da, De, Di, Do, Du);

    A_8x, E_8x = __fourth_last(A_8x, E_8x, Da, De, Di, Do, Du);

    A_8x, E_8x  = __fifth_last(A_8x, E_8x, Da, De, Di, Do, Du);
 
    A_8x, E_8x  = __sixth_last(A_8x, E_8x, Da, De, Di, Do, Du);

    return A_8x, E_8x;
}
fn _KeccakF1600_StatePermute8x(reg ptr u512[25] A_8x) -> reg ptr u512[25]
{
    reg u512 Ca, Ce, Ci, Co, Cu;

    stack u512[25] E_8x;

    /** Rounds24 **/
    Ca, Ce, Ci, Co, Cu = __prepare_theta(A_8x);
    A_8x, E_8x, Ca, Ce, Ci, Co, Cu = __theta_rho_pi_chi_iota_prepare_theta_even(A_8x, E_8x, 0, Ca, Ce, Ci, Co, Cu);
    E_8x, A_8x, Ca, Ce, Ci, Co, Cu = __theta_rho_pi_chi_iota_prepare_theta_odd(E_8x, A_8x, 1, Ca, Ce, Ci, Co, Cu);
    A_8x, E_8x, Ca, Ce, Ci, Co, Cu = __theta_rho_pi_chi_iota_prepare_theta_even(A_8x, E_8x, 2, Ca, Ce, Ci, Co, Cu); 
    E_8x, A_8x, Ca, Ce, Ci, Co, Cu = __theta_rho_pi_chi_iota_prepare_theta_odd(E_8x, A_8x, 3, Ca, Ce, Ci, Co, Cu);
    A_8x, E_8x, Ca, Ce, Ci, Co, Cu = __theta_rho_pi_chi_iota_prepare_theta_even(A_8x, E_8x, 4, Ca, Ce, Ci, Co, Cu);
    E_8x, A_8x, Ca, Ce, Ci, Co, Cu = __theta_rho_pi_chi_iota_prepare_theta_odd(E_8x, A_8x, 5, Ca, Ce, Ci, Co, Cu);
    A_8x, E_8x, Ca, Ce, Ci, Co, Cu = __theta_rho_pi_chi_iota_prepare_theta_even(A_8x, E_8x, 6, Ca, Ce, Ci, Co, Cu);
    E_8x, A_8x, Ca, Ce, Ci, Co, Cu = __theta_rho_pi_chi_iota_prepare_theta_odd(E_8x, A_8x, 7, Ca, Ce, Ci, Co, Cu);
    A_8x, E_8x, Ca, Ce, Ci, Co, Cu = __theta_rho_pi_chi_iota_prepare_theta_even(A_8x, E_8x, 8, Ca, Ce, Ci, Co, Cu);
    E_8x, A_8x, Ca, Ce, Ci, Co, Cu = __theta_rho_pi_chi_iota_prepare_theta_odd(E_8x, A_8x, 9, Ca, Ce, Ci, Co, Cu);
    A_8x, E_8x, Ca, Ce, Ci, Co, Cu = __theta_rho_pi_chi_iota_prepare_theta_even(A_8x, E_8x, 10, Ca, Ce, Ci, Co, Cu);
    E_8x, A_8x, Ca, Ce, Ci, Co, Cu = __theta_rho_pi_chi_iota_prepare_theta_odd(E_8x, A_8x, 11, Ca, Ce, Ci, Co, Cu);
    A_8x, E_8x, Ca, Ce, Ci, Co, Cu = __theta_rho_pi_chi_iota_prepare_theta_even(A_8x, E_8x, 12, Ca, Ce, Ci, Co, Cu);
    E_8x, A_8x, Ca, Ce, Ci, Co, Cu = __theta_rho_pi_chi_iota_prepare_theta_odd(E_8x, A_8x, 13, Ca, Ce, Ci, Co, Cu);
    A_8x, E_8x, Ca, Ce, Ci, Co, Cu = __theta_rho_pi_chi_iota_prepare_theta_even(A_8x, E_8x, 14, Ca, Ce, Ci, Co, Cu);
    E_8x, A_8x, Ca, Ce, Ci, Co, Cu = __theta_rho_pi_chi_iota_prepare_theta_odd(E_8x, A_8x, 15, Ca, Ce, Ci, Co, Cu);
    A_8x, E_8x, Ca, Ce, Ci, Co, Cu = __theta_rho_pi_chi_iota_prepare_theta_even(A_8x, E_8x, 16, Ca, Ce, Ci, Co, Cu);
    E_8x, A_8x, Ca, Ce, Ci, Co, Cu = __theta_rho_pi_chi_iota_prepare_theta_odd(E_8x, A_8x, 17, Ca, Ce, Ci, Co, Cu);
    A_8x, E_8x, Ca, Ce, Ci, Co, Cu = __theta_rho_pi_chi_iota_prepare_theta_even(A_8x, E_8x, 18, Ca, Ce, Ci, Co, Cu);
    E_8x, A_8x, Ca, Ce, Ci, Co, Cu = __theta_rho_pi_chi_iota_prepare_theta_odd(E_8x, A_8x, 19, Ca, Ce, Ci, Co, Cu);
    A_8x, E_8x, Ca, Ce, Ci, Co, Cu = __theta_rho_pi_chi_iota_prepare_theta_even(A_8x, E_8x, 20, Ca, Ce, Ci, Co, Cu);
    E_8x, A_8x, Ca, Ce, Ci, Co, Cu = __theta_rho_pi_chi_iota_prepare_theta_odd(E_8x, A_8x, 21, Ca, Ce, Ci, Co, Cu);
    A_8x, E_8x, Ca, Ce, Ci, Co, Cu = __theta_rho_pi_chi_iota_prepare_theta_even(A_8x, E_8x, 22, Ca, Ce, Ci, Co, Cu);
    E_8x, A_8x = __theta_rho_pi_chi_iota(E_8x, A_8x, 23, Ca, Ce, Ci, Co, Cu);


    return A_8x;
}
u64[8] shake_sep = {9223372036854775808, 9223372036854775808, 9223372036854775808, 9223372036854775808, 9223372036854775808, 9223372036854775808, 9223372036854775808, 9223372036854775808};

fn _shake256_absorb8x_33(reg ptr u512[25] s, reg ptr u8[33] m0 m1 m2 m3 m4 m5 m6 m7) -> reg ptr u512[25]
{
	inline int i;
    reg u512 t0 t1;
	reg u64 t64;
    reg u8 t8;

    for i = 0 to 25
    {
        t0 = #set0_512();
        s[i] = t0;
    }
    () = #spill(m0, m1, m2, m3, m4, m5, m6, m7);
	for i = 0 to 4
    {
        () = #unspill(m0);
        t64 = m0[u64 i];
        s[u64 8 * i] ^= t64;
        () = #unspill(m1);
        t64 = m1[u64 i];
        s[u64 8 * i + 1] ^= t64;
        () = #unspill(m2);
        t64 = m2[u64 i];
        s[u64 8 * i + 2] ^= t64;
        () = #unspill(m3);
        t64 = m3[u64 i];
        s[u64 8 * i + 3] ^= t64;

        () = #unspill(m4);
        t64 = m4[u64 i];
        s[u64 8 * i + 4] ^= t64;
        () = #unspill(m5);
        t64 = m5[u64 i];
        s[u64 8 * i + 5] ^= t64;
        () = #unspill(m6);
        t64 = m6[u64 i];
        s[u64 8 * i + 6] ^= t64;
        () = #unspill(m7);
        t64 = m7[u64 i];
        s[u64 8 * i + 7] ^= t64;
    }
  () = #unspill(m0);
  t8 = m0[32];
  s[u8 128] ^= t8;
  s[u8 129] ^= 0x1F;

  () = #unspill(m1);
  t8 = m1[32];
  s[u8 136] ^= t8;
  s[u8 137] ^= 0x1F;

  () = #unspill(m2);
  t8 = m2[32];
  s[u8 144] ^= t8;
  s[u8 145] ^= 0x1F;

  () = #unspill(m3);
  t8 = m3[32];
  s[u8 152] ^= t8;
  s[u8 153] ^= 0x1F;

  () = #unspill(m4);
  t8 = m4[32];
  s[u8 160] ^= t8;
  s[u8 161] ^= 0x1F;

  () = #unspill(m5);
  t8 = m5[32];
  s[u8 168] ^= t8;
  s[u8 169] ^= 0x1F;
 
  () = #unspill(m6);
  t8 = m6[32];
  s[u8 176] ^= t8;
  s[u8 177] ^= 0x1F;

  () = #unspill(m7);
  t8 = m7[32];
  s[u8 184] ^= t8;
  s[u8 185] ^= 0x1F;


  t0 = shake_sep[u512 0];// ？？
  t1 = s[SHAKE256_RATE / 8 - 1]; 
  t0 = t0 ^ t1;
  s[SHAKE256_RATE / 8 - 1] = t0;

  return s;
}

inline
fn __shake256_squeezeblock8x(reg ptr u512[25] state, reg ptr u8[SHAKE256_RATE] h0 h1 h2 h3 h4 h5 h6 h7) -> reg ptr u512[25], reg ptr u8[SHAKE256_RATE], reg ptr u8[SHAKE256_RATE], reg ptr u8[SHAKE256_RATE], reg ptr u8[SHAKE256_RATE], reg ptr u8[SHAKE256_RATE], reg ptr u8[SHAKE256_RATE], reg ptr u8[SHAKE256_RATE], reg ptr u8[SHAKE256_RATE]
{
  reg u512 t512;
  reg u128 t128;
  inline int i;

//   () = #spill(h0, h1, h2, h3, h4, h5, h6, h7);
  state = _KeccakF1600_StatePermute8x(state);
//   () = #unspill(h0, h1, h2, h3, h4, h5, h6, h7);
	for i = 0 to (SHAKE256_RATE / 8) {
    t512 = state[i];
    t128 = (128u)t512;
		h0[u64 i] = #VMOVLPD(t128);
		h1[u64 i] = #VMOVHPD(t128);
    t128 = #VEXTRACTI64X2(t512, 1);
		h2[u64 i] = #VMOVLPD(t128);
		h3[u64 i] = #VMOVHPD(t128);
    t128 = #VEXTRACTI64X2(t512, 2);
		h4[u64 i] = #VMOVLPD(t128);
		h5[u64 i] = #VMOVHPD(t128);
    t128 = #VEXTRACTI64X2(t512, 3);
		h6[u64 i] = #VMOVLPD(t128);
		h7[u64 i] = #VMOVHPD(t128);
	}

  return state, h0, h1, h2, h3, h4, h5, h6, h7;
}

param int NOISE_NBLOCKS = (MLKEM_ETA1 * MLKEM_N/4 + SHAKE256_RATE - 1)/SHAKE256_RATE;

inline
fn __shake256_squeezenblocks8x(reg ptr u512[25] state, reg ptr u8[NOISE_NBLOCKS * SHAKE256_RATE] buf0 buf1 buf2 buf3 buf4 buf5 buf6 buf7) -> reg ptr u512[25], reg ptr u8[NOISE_NBLOCKS*SHAKE256_RATE], reg ptr u8[NOISE_NBLOCKS*SHAKE256_RATE], reg ptr u8[NOISE_NBLOCKS*SHAKE256_RATE], reg ptr u8[NOISE_NBLOCKS*SHAKE256_RATE], reg ptr u8[NOISE_NBLOCKS*SHAKE256_RATE], reg ptr u8[NOISE_NBLOCKS*SHAKE256_RATE], reg ptr u8[NOISE_NBLOCKS*SHAKE256_RATE], reg ptr u8[NOISE_NBLOCKS*SHAKE256_RATE]
{
  inline int i;

  for i = 0 to NOISE_NBLOCKS
  {
    state, 
    buf0[i*SHAKE256_RATE:SHAKE256_RATE], 
    buf1[i*SHAKE256_RATE:SHAKE256_RATE], 
    buf2[i*SHAKE256_RATE:SHAKE256_RATE], 
    buf3[i*SHAKE256_RATE:SHAKE256_RATE], 
    buf4[i*SHAKE256_RATE:SHAKE256_RATE], 
    buf5[i*SHAKE256_RATE:SHAKE256_RATE], 
    buf6[i*SHAKE256_RATE:SHAKE256_RATE], 
    buf7[i*SHAKE256_RATE:SHAKE256_RATE] 
    = __shake256_squeezeblock8x(state, buf0[i*SHAKE256_RATE:SHAKE256_RATE], buf1[i*SHAKE256_RATE:SHAKE256_RATE], buf2[i*SHAKE256_RATE:SHAKE256_RATE], buf3[i*SHAKE256_RATE:SHAKE256_RATE], buf4[i*SHAKE256_RATE:SHAKE256_RATE], buf5[i*SHAKE256_RATE:SHAKE256_RATE], buf6[i*SHAKE256_RATE:SHAKE256_RATE], buf7[i*SHAKE256_RATE:SHAKE256_RATE]);  
  }

  return state, buf0, buf1, buf2, buf3, buf4, buf5, buf6, buf7;
}
fn _shake128_absorb8x34(reg ptr u512[25] s, reg ptr u8[34] m0 m1 m2 m3 m4 m5 m6 m7) -> reg ptr u512[25]
{
    inline int i;
    reg u512 t0 t1;
    reg u16 t16;
    reg u64 t64;

    for i = 0 to 25
    {
        t0 = #set0_512();
        s[i] = t0;
    }
    for i = 0 to 4
    {
        t64 = m0[u64 i];
        s[u64 4 * i] ^= t64;
        t64 = m1[u64 i];
        s[u64 4 * i + 1] ^= t64;
        t64 = m2[u64 i];
        s[u64 4 * i + 2] ^= t64;
        t64 = m3[u64 i];
        s[u64 4 * i + 3] ^= t64;
    }

    t16 = m0.[u16 32];
    s[u16 64] ^= t16;
    s[u8 130] ^= 0x1F;

    t16 = m1.[u16 32];
    s[u16 68] ^= t16;
    s[u8 138] ^= 0x1F;

    t16 = m2.[u16 32];
    s[u16 72] ^= t16;
    s[u8 146] ^= 0x1F;

    t16 = m3.[u16 32];
    s[u16 76] ^= t16;
    s[u8 154] ^= 0x1F;

    t0 = shake_sep[u512 0];
    t1 = s[SHAKE128_RATE / 8 - 1];
    t0 = t0 ^ t1;
    s[SHAKE128_RATE / 8 - 1] = t0;

    return s;
}

inline
fn __shake128_squeezenblocks8x(reg ptr u512[25] state, reg ptr u8[REJ_UNIFORM_AVX_BUFLEN] h0 h1 h2 h3 h4 h5 h6 h7)
  -> reg ptr u512[25], reg ptr u8[REJ_UNIFORM_AVX_BUFLEN], reg ptr u8[REJ_UNIFORM_AVX_BUFLEN], reg ptr u8[REJ_UNIFORM_AVX_BUFLEN], reg ptr u8[REJ_UNIFORM_AVX_BUFLEN], reg ptr u8[REJ_UNIFORM_AVX_BUFLEN], reg ptr u8[REJ_UNIFORM_AVX_BUFLEN], reg ptr u8[REJ_UNIFORM_AVX_BUFLEN], reg ptr u8[REJ_UNIFORM_AVX_BUFLEN]
{
  inline int i;

  for i = 0 to GENMATRIX_NBLOCKS
  {
    state, h0[i*SHAKE128_RATE:SHAKE128_RATE], h1[i*SHAKE128_RATE:SHAKE128_RATE], h2[i*SHAKE128_RATE:SHAKE128_RATE], h3[i*SHAKE128_RATE:SHAKE128_RATE], h4[i*SHAKE128_RATE:SHAKE128_RATE], h5[i*SHAKE128_RATE:SHAKE128_RATE], h6[i*SHAKE128_RATE:SHAKE128_RATE], h7[i*SHAKE128_RATE:SHAKE128_RATE] = __shake128_squeezeblock8x(state, h0[i*SHAKE128_RATE:SHAKE128_RATE], h1[i*SHAKE128_RATE:SHAKE128_RATE], h2[i*SHAKE128_RATE:SHAKE128_RATE], h3[i*SHAKE128_RATE:SHAKE128_RATE], h4[i*SHAKE128_RATE:SHAKE128_RATE], h5[i*SHAKE128_RATE:SHAKE128_RATE], h6[i*SHAKE128_RATE:SHAKE128_RATE], h7[i*SHAKE128_RATE:SHAKE128_RATE]);
  }

  return state, h0, h1, h2, h3, h4, h5, h6, h7;
}

inline
fn __shake128_squeezeblock8x(reg ptr u512[25] state, reg ptr u8[SHAKE128_RATE] h0 h1 h2 h3 h4 h5 h6 h7) -> reg ptr u512[25], reg ptr u8[SHAKE128_RATE], reg ptr u8[SHAKE128_RATE], reg ptr u8[SHAKE128_RATE], reg ptr u8[SHAKE128_RATE], reg ptr u8[SHAKE128_RATE], reg ptr u8[SHAKE128_RATE], reg ptr u8[SHAKE128_RATE], reg ptr u8[SHAKE128_RATE]
{
  reg u256 t256;
  reg u128 t128;
  inline int i;

  state = _KeccakF1600_StatePermute8x(state);

	for i = 0 to (SHAKE128_RATE / 8) {
    t512 = state[i];
    t128 = (128u)t512;
		h0[u64 i] = #VMOVLPD(t128);
		h1[u64 i] = #VMOVHPD(t128);
    t128 = #VEXTRACTI64X2(t512, 1);
		h2[u64 i] = #VMOVLPD(t128);
		h3[u64 i] = #VMOVHPD(t128);
    t128 = #VEXTRACTI64X2(t512, 2);
		h4[u64 i] = #VMOVLPD(t128);
		h5[u64 i] = #VMOVHPD(t128);
    t128 = #VEXTRACTI64X2(t512, 3);
		h6[u64 i] = #VMOVLPD(t128);
		h7[u64 i] = #VMOVHPD(t128);
	}

  return state, h0, h1, h2, h3, h4, h5, h6, h7;
}