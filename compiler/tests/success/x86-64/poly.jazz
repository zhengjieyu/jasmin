require "gen_fipsavx512asm.jazz"
require "params.jinc"
require "shuffle.jinc"
require "reduce.jinc"
inline
fn mont_red(reg u512 lo hi qx32 qinvx32) -> reg u512 {
  reg u512 m;

  m  = #VPMULL_32u16(lo, qinvx32);
  m  = #VPMULH_32u16(m, qx32);
  lo = #VPSUB_32u16(hi, m);

  return lo;
}

inline 
fn __csubq(reg u512 r qx16) -> reg u512
{
  reg u512 t;
  r = #VPSUB_32u16(r, qx16);
  t = #VPSRA_32u16(r, 15);
  t = #VPAND_512(t, qx16);
  r = #VPADD_32u16(t, r);
  return r;
}
u16[32] jvx32 = {20159, 20159, 20159, 20159, 20159, 20159, 20159, 20159,
                 20159, 20159, 20159, 20159, 20159, 20159, 20159, 20159,
                 20159, 20159, 20159, 20159, 20159, 20159, 20159, 20159,
                 20159, 20159, 20159, 20159, 20159, 20159, 20159, 20159};
u16[32] jqx32 = {MLKEM_Q, MLKEM_Q, MLKEM_Q, MLKEM_Q, MLKEM_Q, MLKEM_Q, MLKEM_Q, MLKEM_Q,
                 MLKEM_Q, MLKEM_Q, MLKEM_Q, MLKEM_Q, MLKEM_Q, MLKEM_Q, MLKEM_Q, MLKEM_Q,
                 MLKEM_Q, MLKEM_Q, MLKEM_Q, MLKEM_Q, MLKEM_Q, MLKEM_Q, MLKEM_Q, MLKEM_Q,
                 MLKEM_Q, MLKEM_Q, MLKEM_Q, MLKEM_Q, MLKEM_Q, MLKEM_Q, MLKEM_Q, MLKEM_Q};
u16[32] jqinvx32 = {MLKEM_QINV, MLKEM_QINV, MLKEM_QINV, MLKEM_QINV, MLKEM_QINV, MLKEM_QINV, MLKEM_QINV, MLKEM_QINV,
                 MLKEM_QINV, MLKEM_QINV, MLKEM_QINV, MLKEM_QINV, MLKEM_QINV, MLKEM_QINV, MLKEM_QINV, MLKEM_QINV,
                 MLKEM_QINV, MLKEM_QINV, MLKEM_QINV, MLKEM_QINV, MLKEM_QINV, MLKEM_QINV, MLKEM_QINV, MLKEM_QINV,
                 MLKEM_QINV, MLKEM_QINV, MLKEM_QINV, MLKEM_QINV, MLKEM_QINV, MLKEM_QINV, MLKEM_QINV, MLKEM_QINV};
u16[128] jzetasmul_exp  = {
  -1103,   1103,    430,   -430,    555,   -555,    843,   -843,  -1251,   1251,    871,   -871,   1550,  -1550,    105,   -105,
    422,   -422,    587,   -587,    177,   -177,   -235,    235,   -291,    291,   -460,    460,   1574,  -1574,   1653,  -1653,
   -246,    246,    778,   -778,   1159,  -1159,   -147,    147,   -777,    777,   1483,  -1483,   -602,    602,   1119,  -1119,
  -1590,   1590,    644,   -644,   -872,    872,    349,   -349,    418,   -418,    329,   -329,   -156,    156,    -75,     75,
    817,   -817,   1097,  -1097,    603,   -603,    610,   -610,   1322,  -1322,  -1285,   1285,  -1465,   1465,    384,   -384,
  -1215,   1215,   -136,    136,   1218,  -1218,  -1335,   1335,   -874,    874,    220,   -220,  -1187,   1187,  -1659,   1659,
  -1185,   1185,  -1530,   1530,  -1278,   1278,    794,   -794,  -1510,   1510,   -854,    854,   -870,    870,    478,   -478,
   -108,    108,   -308,    308,    996,   -996,    991,   -991,    958,   -958,  -1460,   1460,   1522,  -1522,   1628,  -1628
};

inline
fn __poly_reduce(reg ptr u16[MLKEM_N] rp) -> reg ptr u16[MLKEM_N]
{
  inline int i;
  reg u512 r qx32 vx32;
  
  qx32 = jqx32[u512 0];
  vx32 = jvx32[u512 0];

  for i=0 to 8 
  {
    r = rp.[u512 64*i];
    r = __red16(r, vx32, qx32);
    rp.[u512 64*i] = r;
  }
  return rp;
}

fn _poly_csubq(reg ptr u16[MLKEM_N] rp) -> reg ptr u16[MLKEM_N]
{
  reg u512 r qx16;
  inline int i;
  
  qx16 = jqx32[u512 0];

  for i=0 to 8 {
    r = rp.[u512 64*i];
    r = __csubq(r, qx16);
    rp.[u512 64*i] = r;
  }

  return rp;
}
export fn poly_csubq_jazz_avx512(reg u64 rp)
{
  stack u16[MLKEM_N] r;
  reg u16 t;
  inline int i;

  for i = 0 to MLKEM_N {
    t = (u16)[rp + 2*i];
    r[i] = t;
  }

  r = _poly_csubq(r);

  for i = 0 to MLKEM_N {
    t = r[i];
    (u16)[rp + 2*i] = t;
  }
}
fn _poly_frommont(reg ptr u16[MLKEM_N] pr) -> reg ptr u16[MLKEM_N]
{
  reg u512 qx32 qinvx32 dmontx32;
  inline int i;
  reg ptr u16[32] x32p;
  x32p = jqx32;
  qx32 = x32p[u512 0];
  x32p = jqinvx32;
  qinvx32 = x32p[u512 0];
  dmontx32 = #VPBROADCAST_32u16(DMONT);

  for i = 0 to MLKEM_N/32
  {
    t = rp[u512 i];
    t = __montmul(t, dmontx32, qx32, qinvx32);
    rp[u512 i] = t;
  }

}

export fn poly_frommont_jazz(reg u64 rp) 
{
  inline int i;
  reg u16 t;
  stack u16[MLKEM_N] r;

  for i = 0 to MLKEM_N {
    t = (u16)[rp + 2*i];
    r[i] = t;
  }

  r = _poly_frommont(r);

  for i = 0 to MLKEM_N {
    t = r[i];
    (u16)[rp + 2*i] = t;
  }
}

u16 pc_shift1_s = 0x200;
u16 pc_mask_s = 0x0F;
u16 pc_shift2_s = 0x1001;
u32[16] pc_permidx_s = {0, 4, 8, 12, 1, 5, 9, 13, 2, 6, 10, 14, 3, 7, 11, 15};
fn _poly_compress_1(reg ptr u8[MLKEM_POLYCOMPRESSEDBYTES] rp, reg ptr u16[MLKEM_N] a) -> reg ptr u8[MLKEM_POLYCOMPRESSEDBYTES], reg ptr u16[MLKEM_N]
{
    inline int i;
    reg u512 v shift1 shift2 mask permidx f0 f1 f2 f3;
    reg ptr u16[32] x32p;
    a = _poly_csubq(a);
    x32p = jvx32;
    v = x32p[u512 0];
    shift1 = #VPBROADCAST_32u16(pc_shift1_s);
    mask = #VPBROADCAST_32u16(pc_mask_s);
    shift2 = #VPBROADCAST_32u16(pc_shift2_s);
    permidx = pc_permidx_s[u512 0];
    for i=0 to MLKEM_N/128
    {
        f0 = a[u512 4*i];
        f1 = a[u512 4*i + 1];
        f2 = a[u512 4*i + 2];
        f3 = a[u512 4*i + 3];
        f0 = #VPMULH_32u16(f0, v);
        f1 = #VPMULH_32u16(f1, v);
        f2 = #VPMULH_32u16(f2, v);
        f3 = #VPMULH_32u16(f3, v); 
        f0 = #VPMULHRS_32u16(f0, shift1);
        f1 = #VPMULHRS_32u16(f1, shift1);
        f2 = #VPMULHRS_32u16(f2, shift1);
        f3 = #VPMULHRS_32u16(f3, shift1);
        f0 = #VPAND_512(f0, mask);
        f1 = #VPAND_512(f1, mask);
        f2 = #VPAND_512(f2, mask);
        f3 = #VPAND_512(f3, mask);
        f0 = #VPACKUS_32u16(f0, f1);
        f2 = #VPACKUS_32u16(f2, f3);
        f0 = #VPMADDUBSW_512(f0, shift2);
        f2 = #VPMADDUBSW_512(f2, shift2);
        f0 = #VPACKUS_32u16(f0, f2);
        f0 = #VPERMD_512(permidx, f0);
        rp.[u512 64*i] = f0;
    }
    return rp, a;
}
fn _poly_compress(reg u64 rp, reg ptr u16[MLKEM_N] a) -> reg ptr u16[MLKEM_N]
{
    inline int i;
    reg u512 v shift1 shift2 mask permidx f0 f1 f2 f3;
    reg ptr u16[32] x32p;
    a = _poly_csubq(a);
    x32p = jvx32;
    v = x32p[u512 0];
    shift1 = #VPBROADCAST_32u16(pc_shift1_s);
    mask = #VPBROADCAST_32u16(pc_mask_s);
    shift2 = #VPBROADCAST_32u16(pc_shift2_s);
    permidx = pc_permidx_s[u512 0];
    for i=0 to MLKEM_N/128
    {
        f0 = a[u512 4*i];
        f1 = a[u512 4*i + 1];
        f2 = a[u512 4*i + 2];
        f3 = a[u512 4*i + 3];
        f0 = #VPMULH_32u16(f0, v);
        f1 = #VPMULH_32u16(f1, v);
        f2 = #VPMULH_32u16(f2, v);
        f3 = #VPMULH_32u16(f3, v); 
        f0 = #VPMULHRS_32u16(f0, shift1);
        f1 = #VPMULHRS_32u16(f1, shift1);
        f2 = #VPMULHRS_32u16(f2, shift1);
        f3 = #VPMULHRS_32u16(f3, shift1);
        f0 = #VPAND_512(f0, mask);
        f1 = #VPAND_512(f1, mask);
        f2 = #VPAND_512(f2, mask);
        f3 = #VPAND_512(f3, mask);
        f0 = #VPACKUS_32u16(f0, f1);
        f2 = #VPACKUS_32u16(f2, f3);
        f0 = #VPMADDUBSW_512(f0, shift2);
        f2 = #VPMADDUBSW_512(f2, shift2);
        f0 = #VPACKUS_32u16(f0, f2);
        f0 = #VPERMD_512(permidx, f0);
        (u512)[rp + 64*i] = f0;
    }
    return a;
}
export fn poly_compress_jazz(reg u64 rp, reg u64 ap) 
{
  stack u16[MLKEM_N] a;
  reg u16 t;
  inline int i;

  for i = 0 to MLKEM_N {
    t = (u16)[ap + 2*i];
    a[i] = t;
  }

  a = _poly_compress(rp, a);
}
u8[64] pd_jshufbidx = {0,  0,  0,  0,  1,  1,  1,  1,  2,  2,  2,  2,  3,  3,  3,  3,  
4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  
8,  8,  8,  8,  9,  9,  9,  9, 10, 10, 10, 10, 11, 11, 11, 11,  
12, 12, 12, 12, 13, 13, 13, 13, 14, 14, 14, 14, 15, 15, 15, 15};
u32 pd_mask_s = 0x00F0000F;
u32 pd_shift_s = 0x800800;
fn _poly_decompress(reg ptr u16[MLKEM_N] rp, reg u64 ap) -> reg ptr u16[MLKEM_N]
{
  inline int i;
  reg u512 f q shufbidx mask shift;
  reg u128 h;
  reg ptr u16[32] x32p;
  reg ptr u8[64] x64p;
  stack u128 sh;
  x32p = jqx32;
  q = x32p[u512 0];
  x64p = pd_jshufbidx;
  shufbidx = x64p[u512 0];
  mask = #VPBROADCAST_16u32(pd_mask_s);
  shift = #VPBROADCAST_16u32(pd_shift_s);
  f = #set0_512();
  for i = 0 to MLKEM_N/32
  {
    h = (128u)(u128)[ap + 16*i];
    sh = h;
    f = #VBROADCASTI32X4(sh);
    f = #VPERMB_512(shufbidx, f);
    f = #VPAND_512(f, mask);
    f = #VPMULL_32u16(f, shift);
    f = #VPMULHRS_32u16(f, q);
    rp[u512 i] = f;
  }
  return rp;

}
export fn poly_decompress_jazz(reg u64 rp, reg u64 ap) 
{
  stack u16[MLKEM_N] r;
  reg u16 t;
  inline int i;

  r = _poly_decompress(r, ap);

  for i = 0 to MLKEM_N {
    t = r[i];
    (u16)[rp + 2*i] = t;
  }
}

u32[16] pfm_shift_s = {3, 2, 1, 0, 3, 2, 1, 0, 3, 2, 1, 0, 3, 2, 1, 0};
u8[64] pfm_idx_s = {0, 1, 4, 5, 8, 9, 12, 13, 2, 3, 6, 7, 10, 11, 14, 15,
0, 1, 4, 5, 8, 9, 12, 13, 2, 3, 6, 7, 10, 11, 14, 15,
0, 1, 4, 5, 8, 9, 12, 13, 2, 3, 6, 7, 10, 11, 14, 15,
0, 1, 4, 5, 8, 9, 12, 13, 2, 3, 6, 7, 10, 11, 14, 15};
u32[16] pfm_idx32_1_s ={0, 0, 0, 0, 4, 4, 4, 4, 1, 1, 1, 1, 5, 5, 5, 5};
u32[16] pfm_idx32_2_s ={2, 2, 2, 2, 6, 6, 6, 6, 3, 3, 3, 3, 7, 7, 7, 7};
u64[8] pfm_idx_1_s = {0, 1, 8, 9, 4, 5, 12, 13};
u64[8] pfm_idx_2_s = {2, 3, 10, 11, 6, 7, 14, 15};
fn _poly_frommsg(reg ptr u16[MLKEM_N] rp, reg u64 ap) -> reg ptr u16[MLKEM_N]
{
  reg u512 h0_tmp0 h0_tmp1 h2_tmp0 h2_tmp1;
  reg u512 f g0 g1 g2 g3 h0 h1 h2 h3 shift idx8 idx32_1 idx32_2 hqs idx_1 idx_2;
  stack u16 halfq = (MLKEM_Q + 1)/2;

  shift = pfm_shift_s[u512 0];
  idx8 = pfm_idx_s[u512 0];
  idx32_1 = pfm_idx32_1_s[u512 0];
  idx32_2 = pfm_idx32_2_s[u512 0];
  hqs = #VPBROADCAST_32u16(halfq);
  idx_1 = pfm_idx_1_s[u512 0];
  idx_2 = pfm_idx_2_s[u512 0];


  f = #VBROADCASTI64X4((u256)[ap + 0]);

  g3 = #VPERMD_512(idx32_1, f);
  g3 = #VPSLLV_16u32(g3, shift);
  g3 = #VPSHUFB_512(g3, idx8);

  g0 = #VPSLL_32u16(g3, 12);
  g1 = #VPSLL_32u16(g3, 8);
  g2 = #VPSLL_32u16(g3, 4);

  g0 = #VPSRA_32u16(g0, 15);
  g1 = #VPSRA_32u16(g1, 15);
  g2 = #VPSRA_32u16(g2, 15);
  g3 = #VPSRA_32u16(g3, 15);

  g0 = #VPAND_512(g0,hqs);
  g1 = #VPAND_512(g1,hqs);
  g2 = #VPAND_512(g2,hqs);
  g3 = #VPAND_512(g3,hqs);

  h0 = #VPUNPCKL_8u64(g0,g1);
  h2 = #VPUNPCKH_8u64(g0,g1);
  h1 = #VPUNPCKL_8u64(g2,g3);
  h3 = #VPUNPCKH_8u64(g2,g3);
  
  h0_tmp0 = h0;
  h0_tmp1 = h0;
  h2_tmp0 = h2;
  h2_tmp1 = h2;
  h0_tmp0 = #VPERMT2Q_512(h0_tmp0, idx_1, h1);
  h0_tmp1 = #VPERMT2Q_512(h0_tmp1, idx_2, h1);
  h2_tmp0 = #VPERMT2Q_512(h2_tmp0, idx_1, h3);
  h2_tmp1 = #VPERMT2Q_512(h2_tmp1, idx_2, h3);


  h0 = #VSHUFI32X4(h0_tmp0, h2_tmp0, 0x44);
  h1 = #VSHUFI32X4(h0_tmp0, h2_tmp0, 0xEE);
  h2 = #VSHUFI32X4(h0_tmp1, h2_tmp1, 0x44);
  h3 = #VSHUFI32X4(h0_tmp1, h2_tmp1, 0xEE);



  rp[u512 0] = h0;
  rp[u512 1] = h1;
  rp[u512 4] = h2;
  rp[u512 5] = h3;

  g3 = #VPERMD_512(idx32_2, f);
  g3 = #VPSLLV_16u32(g3, shift);
  g3 = #VPSHUFB_512(g3, idx8);

  g0 = #VPSLL_32u16(g3, 12);
  g1 = #VPSLL_32u16(g3, 8);
  g2 = #VPSLL_32u16(g3, 4);

  g0 = #VPSRA_32u16(g0, 15);
  g1 = #VPSRA_32u16(g1, 15);
  g2 = #VPSRA_32u16(g2, 15);
  g3 = #VPSRA_32u16(g3, 15);

  g0 = #VPAND_512(g0,hqs);
  g1 = #VPAND_512(g1,hqs);
  g2 = #VPAND_512(g2,hqs);
  g3 = #VPAND_512(g3,hqs);

  h0 = #VPUNPCKL_8u64(g0,g1);
  h2 = #VPUNPCKH_8u64(g0,g1);
  h1 = #VPUNPCKL_8u64(g2,g3);
  h3 = #VPUNPCKH_8u64(g2,g3);


  h0_tmp0 = h0;
  h0_tmp1 = h0;
  h2_tmp0 = h2;
  h2_tmp1 = h2;
  h0_tmp0 = #VPERMT2Q_512(h0_tmp0, idx_1, h1);
  h0_tmp1 = #VPERMT2Q_512(h0_tmp1, idx_2, h1);
  h2_tmp0 = #VPERMT2Q_512(h2_tmp0, idx_1, h3);
  h2_tmp1 = #VPERMT2Q_512(h2_tmp1, idx_2, h3);

  h0 = #VSHUFI32X4(h0_tmp0, h2_tmp0, 0x44);
  h1 = #VSHUFI32X4(h0_tmp0, h2_tmp0, 0xEE);
  h2 = #VSHUFI32X4(h0_tmp1, h2_tmp1, 0x44);
  h3 = #VSHUFI32X4(h0_tmp1, h2_tmp1, 0xEE);


  rp[u512 2] = h0;
  rp[u512 3] = h1;
  rp[u512 6] = h2;
  rp[u512 7] = h3;
  return rp;

}
export fn poly_frommsg_jazz(reg u64 rp, reg u64 ap) 
{
  stack u16[MLKEM_N] r;
  reg u16 t;
  inline int i;

  r = _poly_frommsg(r, ap);

  for i = 0 to MLKEM_N {
    t = r[i];
    (u16)[rp + 2*i] = t;
  }
}
// not done yet, because of the _mm512_movepi8_mask needs mask register.
u64[8] idx_tmsg = {0, 2, 4, 6, 1, 3, 5, 7};
fn _poly_tomsg(reg u64 rp, reg ptr u16[MLKEM_N] a) -> reg ptr u16[MLKEM_N]
{
  inline int i;
  reg u512 f0 f1 g0 g1 hq hhq idx;
  stack u16 const_hq = (MLKEM_Q - 1)/2;
  stack u16 const_hhq = (MLKEM_Q - 1)/4;
  hq = #VPBROADCAST_32u16(const_hq);
  hhq = #VPBROADCAST_32u16(const_hhq);
  reg u64 s;
  idx = idx_tmsg.[u512 0];
  for i=0 to 4
  {
    f0 = a.[u512 2*i];
    f1 = a.[u512 2*i + 1];
    f0 = #VPSUB_32u16(hq, f0);
    f1 = #VPSUB_32u16(hq, f1);
    g0 = #VPSRA_32u16(f0, 15);
    g1 = #VPSRA_32u16(f1, 15);
    f0 = #VPXOR_512(f0, g0);
    f1 = #VPXOR_512(f1, g1);
    f0 = #VPSUB_32u16(f0, hhq);
    f1 = #VPSUB_32u16(f1, hhq);
    f0 = #VPACKSS_32u16(f0, f1);
    f0 = #VPERMQ512(idx, f0);
    //  s = _mm512_movepi8_mask(f0);
    rp[u64 i] = s;
  }
  return a;

}

fn _poly_tomsg_1(reg ptr u8[MLKEM_INDCPA_MSGBYTES] rp, reg ptr u16[MLKEM_N] a) -> reg ptr u8[MLKEM_INDCPA_MSGBYTES], reg ptr u16[MLKEM_N]
{
  inline int i;
  reg u512 f0 f1 g0 g1 hq hhq idx;
  stack u16 const_hq = (MLKEM_Q - 1)/2;
  stack u16 const_hhq = (MLKEM_Q - 1)/4;
  hq = #VPBROADCAST_32u16(const_hq);
  hhq = #VPBROADCAST_32u16(const_hhq);
  reg u64 s;
  idx = idx_tmsg.[u512 0];
  for i=0 to 4
  {
    f0 = a.[u512 2*i];
    f1 = a.[u512 2*i + 1];
    f0 = #VPSUB_32u16(hq, f0);
    f1 = #VPSUB_32u16(hq, f1);
    g0 = #VPSRA_32u16(f0, 15);
    g1 = #VPSRA_32u16(f1, 15);
    f0 = #VPXOR_512(f0, g0);
    f1 = #VPXOR_512(f1, g1);
    f0 = #VPSUB_32u16(f0, hhq);
    f1 = #VPSUB_32u16(f1, hhq);
    f0 = #VPACKSS_32u16(f0, f1);
    f0 = #VPERMQ512(idx, f0);
    //  s = _mm512_movepi8_mask(f0);
    rp[u64 i] = s;
  }
  return rp, a;

}
u64[8] idx_cbd2 = {0, 1, 4, 5, 2, 3, 6, 7};
inline fn __cbd2(reg ptr u16[MLKEM_N] rp, reg ptr u8[MLKEM_ETA2*MLKEM_N/4] buf) -> reg ptr u16[MLKEM_N]
{
  inline int i;
  reg u512 f0 f1 f2 f3;
  reg u512 mask55 mask33 mask03 mask0F;
  reg u256 t;
  reg u512 idx;
  stack u32 const_mask55 = 0x55555555;
  stack u32 const_mask33 = 0x33333333;
  stack u32 const_mask0F = 0x0F0F0F0F;
  stack u32 const_mask03 = 0x03030303;
  idx = idx_cbd2[u512 0];
  mask55 = #VPBROADCAST_16u32(const_mask55);
  mask33 = #VPBROADCAST_16u32(const_mask33);
  mask0F = #VPBROADCAST_16u32(const_mask0F);
  mask03 = #VPBROADCAST_16u32(const_mask03);
  () = #spill(mask33);
  () = #spill(mask0F);
  () = #spill(mask55);
  () = #spill(mask03);
  for i = 0 to 2
  {
    f0 = buf[u512 i];

    f1 = #VPSRL_32u16(f0, 1);
    () = #unspill(mask55);
    f0 = #VPAND_512(mask55, f0);
    f1 = #VPAND_512(mask55, f1);
    
    f0 = #VPADD_64u8(f0, f1);

    f1 = #VPSRL_32u16(f0, 2);
    () = #unspill(mask33);
    f0 = #VPAND_512(mask33, f0);
    f1 = #VPAND_512(mask33, f1);
    f0 = #VPADD_64u8(f0, mask33);
    f0 = #VPSUB_64u8(f0, f1);

    f1 = #VPSRL_32u16(f0, 4);
    () = #unspill(mask0F);
    f0 = #VPAND_512(mask0F, f0);
    f1 = #VPAND_512(mask0F, f1);
    () = #unspill(mask03);
    f0 = #VPSUB_64u8(f0, mask03);
    f1 = #VPSUB_64u8(f1, mask03);

    f2 = #VPUNPCKL_64u8(f0, f1);
    f3 = #VPUNPCKL_64u8(f0, f1);

    f0 = #VSHUFI32X4(f2,f3,0x44);
    f1 = #VSHUFI32X4(f2,f3,0xEE);
    f2 = #VPERMQ512(idx, f0);
    f3 = #VPERMQ512(idx, f1);

    t = (256u)f2;
    f0 = #VPMOVSX_32u8_32u16(t);
    t = #VEXTRACTI32X8(f2, 1);
    f1 = #VPMOVSX_32u8_32u16(t);
    t = (256u)f3;
    f2 = #VPMOVSX_32u8_32u16(t);
    t = #VEXTRACTI32X8(f3, 1);
    f3 = #VPMOVSX_32u8_32u16(t);

    rp[u512 4*i] = f0;
    rp[u512 4*i + 1] = f1;
    rp[u512 4*i + 2] = f2;
    rp[u512 4*i + 3] = f3;
  }
  return rp;
}
inline
fn __poly_cbd_eta1(reg ptr u16[MLKEM_N] rp, reg ptr u8[MLKEM_ETA1*MLKEM_N/4] buf) -> reg ptr u16[MLKEM_N]
{
  rp = __cbd2(rp, buf[0:MLKEM_ETA1*MLKEM_N/4]);
  return rp;
}
fn _poly_getnoise_eta1_8x(reg ptr u16[MLKEM_N] r0 r1 r2 r3 r4 r5 r6 r7, reg ptr u8[MLKEM_SYMBYTES] seed, reg u8 nonce) -> reg ptr u16[MLKEM_N], reg ptr u16[MLKEM_N], reg ptr u16[MLKEM_N], reg ptr u16[MLKEM_N], reg ptr u16[MLKEM_N], reg ptr u16[MLKEM_N], reg ptr u16[MLKEM_N], reg ptr u16[MLKEM_N]
{
  reg u256 f;
  stack u512[25] state;
  stack u8[NOISE_NBLOCKS * SHAKE256_RATE] buf0 buf1 buf2 buf3 buf4 buf5 buf6 buf7;



  f = seed.[u256 0];
  buf0[u256 0] = f;
  buf1[u256 0] = f;
  buf2[u256 0] = f;
  buf3[u256 0] = f;
  buf4[u256 0] = f;
  buf5[u256 0] = f;
  buf6[u256 0] = f;
  buf7[u256 0] = f;

  buf0.[32] = nonce;
  nonce += 1;
  buf1.[32] = nonce;
  nonce += 1;
  buf2.[32] = nonce;
  nonce += 1;
  buf3.[32] = nonce;

  buf4.[32] = nonce;
  nonce += 1;
  buf5.[32] = nonce;
  nonce += 1;
  buf6.[32] = nonce;
  nonce += 1;
  buf7.[32] = nonce;

  () = #spill(r0, r1, r2, r3, r4, r5, r6, r7);
  state = _shake256_absorb8x_33(state, buf0[0:33], buf1[0:33], buf2[0:33], buf3[0:33], buf4[0:33], buf5[0:33], buf6[0:33], buf7[0:33]);
  state, buf0, buf1, buf2, buf3, buf4, buf5, buf6, buf7 = __shake256_squeezenblocks8x(state, buf0, buf1, buf2, buf3, buf4, buf5, buf6, buf7);
  () = #unspill(r0, r1, r2, r3, r4, r5, r6, r7);

  r0 = __poly_cbd_eta1(r0, buf0[0:MLKEM_ETA1*MLKEM_N/4+(MLKEM_ETA1 - 2)*8]);
  r1 = __poly_cbd_eta1(r1, buf1[0:MLKEM_ETA1*MLKEM_N/4+(MLKEM_ETA1 - 2)*8]);
  r2 = __poly_cbd_eta1(r2, buf2[0:MLKEM_ETA1*MLKEM_N/4+(MLKEM_ETA1 - 2)*8]);
  r3 = __poly_cbd_eta1(r3, buf3[0:MLKEM_ETA1*MLKEM_N/4+(MLKEM_ETA1 - 2)*8]);
  
  r4 = __poly_cbd_eta1(r4, buf4[0:MLKEM_ETA1*MLKEM_N/4+(MLKEM_ETA1 - 2)*8]);
  r5 = __poly_cbd_eta1(r5, buf5[0:MLKEM_ETA1*MLKEM_N/4+(MLKEM_ETA1 - 2)*8]);
  r6 = __poly_cbd_eta1(r6, buf6[0:MLKEM_ETA1*MLKEM_N/4+(MLKEM_ETA1 - 2)*8]);
  r7 = __poly_cbd_eta1(r7, buf7[0:MLKEM_ETA1*MLKEM_N/4+(MLKEM_ETA1 - 2)*8]);
  return r0, r1, r2, r3, r4, r5, r6, r7;

}

export fn poly_getnoise_eta1_8x_jazz(reg u64 rp, reg u64 seedp, reg u8 nonce)
{
  stack u16[8 * MLKEM_N] r;
  stack u8[MLKEM_SYMBYTES] seed;
  stack u64 srp;
  reg u16 t;
  reg u8 d;
  inline int i;

  srp = rp;

  for i = 0 to MLKEM_SYMBYTES {
    d       = (u8)[seedp + i];
    seed[i] = d;
  }

  r[0:MLKEM_N], r[MLKEM_N:MLKEM_N], r[2*MLKEM_N:MLKEM_N], r[3*MLKEM_N:MLKEM_N], r[4*MLKEM_N:MLKEM_N], r[5*MLKEM_N:MLKEM_N], r[6*MLKEM_N:MLKEM_N], r[7*MLKEM_N:MLKEM_N] = _poly_getnoise_eta1_8x(r[0:MLKEM_N], r[MLKEM_N:MLKEM_N], r[2*MLKEM_N:MLKEM_N], r[3*MLKEM_N:MLKEM_N], r[4*MLKEM_N:MLKEM_N], r[5*MLKEM_N:MLKEM_N], r[6*MLKEM_N:MLKEM_N], r[7*MLKEM_N:MLKEM_N], seed, nonce);

  rp = srp;
  for i = 0 to 8*MLKEM_N {
    t = r[i];
    (u16)[rp + 2*i] = t;
  }
}


fn _poly_sub_avx512(reg ptr u16[MLKEM_N] rp bp)  -> reg ptr u16[MLKEM_N]
{
    inline int i;
    reg u512 a;
    reg u512 b;
    reg u512 r;
    for i = 0 to 8 {
        a = rp.[u512 64*i];
        b = bp.[u512 64*i];
        r = #VPSUB_32u16(a, b);
        rp.[u512 64*i] = r;
    }
    return rp;
}
export fn poly_sub_jazz_avx512(reg u64 rp, reg u64 bp)
{
    stack u16[MLKEM_N] r;
    stack u16[MLKEM_N] b;
    reg u16 t;
    inline int i;
    for i = 0 to MLKEM_N {
        t = (u16)[rp + 2*i];
        r[i] = t;
        t = (u16)[bp + 2*i];
        b[i] = t;
    }
    r = _poly_sub_avx512(r, b);
    for i = 0 to MLKEM_N {
        t = r[i];
        (u16)[rp + 2*i] = t;
    }
}
fn _poly_add_avx512(reg ptr u16[MLKEM_N] rp bp) -> reg ptr u16[MLKEM_N]
{
    inline int i;
    reg u512 a;
    reg u512 b;
    reg u512 r;
    for i = 0 to 8 {
        a = rp.[u512 64*i];
        b = bp.[u512 64*i];
        r = #VPADD_32u16(a, b);
        rp.[u512 64*i] = r;
    }
    return rp;
}

export fn poly_add_jazz_avx512(reg u64 rp, reg u64 bp)
{
    stack u16[MLKEM_N] r;
    stack u16[MLKEM_N] b;
    reg u16 t;
    inline int i;
    for i = 0 to MLKEM_N {
        t = (u16)[rp + 2*i];
        r[i] = t;
        t = (u16)[bp + 2*i];
        b[i] = t;
    }
    r = _poly_add_avx512(r, b);
    for i = 0 to MLKEM_N {
        t = r[i];
        (u16)[rp + 2*i] = t;
    }
}

inline fn __karatsuba32x(reg u512 al ah bl bh const_q const_qinv zeta) -> reg u512, reg u512
{
    reg u512 f0 f1 f2 f3 f4 f5 f6;
    f0 = #VPADD_32u16(al, ah); //zmm19
    f1 = #VPADD_32u16(bl, bh); //zmm20
    f2 = #VPMULH_32u16(f0, f1); //zmm21
    f3 = #VPMULH_32u16(al, bl); //zmm22
    f4 = #VPMULH_32u16(ah, bh); //zmm23

    f5 = #VPMULL_32u16(al, const_qinv);
    f6 = #VPMULL_32u16(ah, const_qinv);
    f0 = #VPMULL_32u16(f0, const_qinv);

    f5 = #VPMULL_32u16(bl, f5);
    f6 = #VPMULL_32u16(bh, f6);
    f0 = #VPMULL_32u16(f1, f0);

    f5 = #VPMULH_32u16(f5, const_q);
    f6 = #VPMULH_32u16(f6, const_q);
    f0 = #VPMULH_32u16(f0, const_q);

    f5 = #VPSUB_32u16(f3, f5);
    f6 = #VPSUB_32u16(f4, f6);
    f0 = #VPSUB_32u16(f2, f0);

    f1 = #VPSUB_32u16(f0, f5);
    f1 = #VPSUB_32u16(f1, f6);

    f2 = #VPMULH_32u16(f6, zeta);
    f6 = #VPMULL_32u16(f6, const_qinv);
    f6 = #VPMULL_32u16(f6, zeta);
    f6 = #VPMULH_32u16(f6, const_q);
    f6 = #VPSUB_32u16(f2, f6);

    f2 = #VPADD_32u16(f5, f6);

    return f2, f1;

}
fn _poly_basemul(reg ptr u16[MLKEM_N] rp ap bp) -> reg ptr u16[MLKEM_N]
{
  reg u512 const_q const_qinv;
  const_q = jqx32[u512 0];
  const_qinv = jqinvx32[u512 0];
  reg u512 f0 f1 f2 f3 f4 f5 f6 f7 g0 g1 g2 g3 g4 g5 g6 g7 zeta;
  reg u512 h0 h1;

  f0 = ap.[u512 0*64];
  f1 = ap.[u512 1*64];
  f2 = ap.[u512 2*64];
  f3 = ap.[u512 3*64];
  f4 = ap.[u512 4*64];
  f5 = ap.[u512 5*64];
  f6 = ap.[u512 6*64];
  f7 = ap.[u512 7*64];

  g0 = bp.[u512 0*64];
  g1 = bp.[u512 1*64];
  g2 = bp.[u512 2*64];
  g3 = bp.[u512 3*64];
  g4 = bp.[u512 4*64];
  g5 = bp.[u512 5*64];
  g6 = bp.[u512 6*64];
  g7 = bp.[u512 7*64];

  zeta = jzetasmul_exp.[u512 0];
  h0, h1 = __karatsuba32x(f0, f1, g0, g1, const_q, const_qinv, zeta);
  rp.[u512 0*64] = h0;
  rp.[u512 1*64] = h1;

  zeta = jzetasmul_exp.[u512 1*64];
  h0, h1 = __karatsuba32x(f2, f3, g2, g3, const_q, const_qinv, zeta);
  rp.[u512 2*64] = h0;
  rp.[u512 3*64] = h1;

  zeta = jzetasmul_exp.[u512 2*64];
  h0, h1 = __karatsuba32x(f4, f5, g4, g5, const_q, const_qinv, zeta);
  rp.[u512 4*64] = h0;
  rp.[u512 5*64] = h1;

  zeta = jzetasmul_exp.[u512 3*64];
  h0, h1 = __karatsuba32x(f6, f7, g6, g7, const_q, const_qinv, zeta);
  rp.[u512 6*64] = h0;
  rp.[u512 7*64] = h1;

  return rp;

}
export fn poly_basemul_jazz(reg u64 rp, reg u64 ap, reg u64 bp) 
{
  stack u16[MLKEM_N] a;
  stack u16[MLKEM_N] b;
  stack u16[MLKEM_N] r;
  reg u16 t;
  inline int i;

  for i = 0 to MLKEM_N {
    t = (u16)[ap + 2*i];
    a[i] = t;
    t = (u16)[bp + 2*i];
    b[i] = t;
    t = (u16)[rp + 2*i];
    r[i] = t;
  }

  r = _poly_basemul(r, a, b);

  for i = 0 to MLKEM_N {
    t = r[i];
    (u16)[rp + 2*i] = t;
  }
}

u16[1038] jzetas_exp = {
  31498,   -758, 
  14745,   -359,  
    787,  -1517,
  13525,   1493, 
 -12402,   1422, 
  28191,    287,  
 -16694,    202,
 -20907, -20907, -20907, -20907, -20907, -20907, -20907, -20907, -20907, -20907, -20907, -20907, -20907, -20907, -20907, -20907,
  27758,  27758,  27758,  27758,  27758,  27758,  27758,  27758,  27758,  27758,  27758,  27758,  27758,  27758,  27758,  27758,
   -171,   -171,   -171,   -171,   -171,   -171,   -171,   -171,   -171,   -171,   -171,   -171,   -171,   -171,   -171,   -171,
    622,    622,    622,    622,    622,    622,    622,    622,    622,    622,    622,    622,    622,    622,    622,    622,

  -3799,  -3799,  -3799,  -3799,  -3799,  -3799,  -3799,  -3799,  -3799,  -3799,  -3799,  -3799,  -3799,  -3799,  -3799,  -3799,
 -15690, -15690, -15690, -15690, -15690, -15690, -15690, -15690, -15690, -15690, -15690, -15690, -15690, -15690, -15690, -15690,
   1577,   1577,   1577,   1577,   1577,   1577,   1577,   1577,   1577,   1577,   1577,   1577,   1577,   1577,   1577,   1577,
    182,    182,    182,    182,    182,    182,    182,    182,    182,    182,    182,    182,    182,    182,    182,    182,

  10690,  10690,  10690,  10690,  10690,  10690,  10690,  10690,  10690,  10690,  10690,  10690,  10690,  10690,  10690,  10690,
   1358,   1358,   1358,   1358,   1358,   1358,   1358,   1358,   1358,   1358,   1358,   1358,   1358,   1358,   1358,   1358,
    962,    962,    962,    962,    962,    962,    962,    962,    962,    962,    962,    962,    962,    962,    962,    962,
  -1202,  -1202,  -1202,  -1202,  -1202,  -1202,  -1202,  -1202,  -1202,  -1202,  -1202,  -1202,  -1202,  -1202,  -1202,  -1202,

 -11202, -11202, -11202, -11202, -11202, -11202, -11202, -11202, -11202, -11202, -11202, -11202, -11202, -11202, -11202, -11202,
  31164,  31164,  31164,  31164,  31164,  31164,  31164,  31164,  31164,  31164,  31164,  31164,  31164,  31164,  31164,  31164,
  -1474,  -1474,  -1474,  -1474,  -1474,  -1474,  -1474,  -1474,  -1474,  -1474,  -1474,  -1474,  -1474,  -1474,  -1474,  -1474,
   1468,   1468,   1468,   1468,   1468,   1468,   1468,   1468,   1468,   1468,   1468,   1468,   1468,   1468,   1468,   1468,

  -5827,  -5827,  -5827,  -5827,  -5827,  -5827,  -5827,  -5827,  17363,  17363,  17363,  17363,  17363,  17363,  17363,  17363,
 -26360, -26360, -26360, -26360, -26360, -26360, -26360, -26360, -29057, -29057, -29057, -29057, -29057, -29057, -29057, -29057,
    573,    573,    573,    573,    573,    573,    573,    573,  -1325,  -1325,  -1325,  -1325,  -1325,  -1325,  -1325,  -1325,
    264,    264,    264,    264,    264,    264,    264,    264,    383,    383,    383,    383,    383,    383,    383,    383,

   5571,   5571,   5571,   5571,   5571,   5571,   5571,   5571,  -1102,  -1102,  -1102,  -1102,  -1102,  -1102,  -1102,  -1102,
  21438,  21438,  21438,  21438,  21438,  21438,  21438,  21438, -26242, -26242, -26242, -26242, -26242, -26242, -26242, -26242,
   -829,   -829,   -829,   -829,   -829,   -829,   -829,   -829,   1458,   1458,   1458,   1458,   1458,   1458,   1458,   1458,
  -1602,  -1602,  -1602,  -1602,  -1602,  -1602,  -1602,  -1602,   -130,   -130,   -130,   -130,   -130,   -130,   -130,   -130,

 -28073, -28073, -28073, -28073, -28073, -28073, -28073, -28073,  24313,  24313,  24313,  24313,  24313,  24313,  24313,  24313,
 -10532, -10532, -10532, -10532, -10532, -10532, -10532, -10532,   8800,   8800,   8800,   8800,   8800,   8800,   8800,   8800,
   -681,   -681,   -681,   -681,   -681,   -681,   -681,   -681,   1017,   1017,   1017,   1017,   1017,   1017,   1017,   1017,
    732,    732,    732,    732,    732,    732,    732,    732,    608,    608,    608,    608,    608,    608,    608,    608,

  18426,  18426,  18426,  18426,  18426,  18426,  18426,  18426,   8859,   8859,   8859,   8859,   8859,   8859,   8859,   8859,
  26675,  26675,  26675,  26675,  26675,  26675,  26675,  26675, -16163, -16163, -16163, -16163, -16163, -16163, -16163, -16163,
  -1542,  -1542,  -1542,  -1542,  -1542,  -1542,  -1542,  -1542,    411,    411,    411,    411,    411,    411,    411,    411,
   -205,   -205,   -205,   -205,   -205,   -205,   -205,   -205,  -1571,  -1571,  -1571,  -1571,  -1571,  -1571,  -1571,  -1571,

  -5689,  -5689,  -5689,  -5689,  -6516,  -6516,  -6516,  -6516,   1496,   1496,   1496,   1496,  30967,  30967,  30967,  30967,
 -23565, -23565, -23565, -23565,  20179,  20179,  20179,  20179,  20710,  20710,  20710,  20710,  25080,  25080,  25080,  25080,
   1223,   1223,   1223,   1223,    652,    652,    652,    652,   -552,   -552,   -552,   -552,   1015,   1015,   1015,   1015,
  -1293,  -1293,  -1293,  -1293,   1491,   1491,   1491,   1491,   -282,   -282,   -282,   -282,  -1544,  -1544,  -1544,  -1544,

 -12796, -12796, -12796, -12796,  26616,  26616,  26616,  26616,  16064,  16064,  16064,  16064, -12442, -12442, -12442, -12442,
   9134,   9134,   9134,   9134,   -650,   -650,   -650,   -650, -25986, -25986, -25986, -25986,  27837,  27837,  27837,  27837,
    516,    516,    516,    516,     -8,     -8,     -8,     -8,   -320,   -320,   -320,   -320,   -666,   -666,   -666,   -666,
  -1618,  -1618,  -1618,  -1618,  -1162,  -1162,  -1162,  -1162,    126,    126,    126,    126,   1469,   1469,   1469,   1469,

  19883,  19883,  19883,  19883, -28250, -28250, -28250, -28250, -15887, -15887, -15887, -15887,  -8898,  -8898,  -8898,  -8898,
 -28309, -28309, -28309, -28309,   9075,   9075,   9075,   9075, -30199, -30199, -30199, -30199,  18249,  18249,  18249,  18249,
   -853,   -853,   -853,   -853,    -90,    -90,    -90,    -90,   -271,   -271,   -271,   -271,    830,    830,    830,    830,
    107,    107,    107,    107,  -1421,  -1421,  -1421,  -1421,   -247,   -247,   -247,   -247,   -951,   -951,   -951,   -951,

  13426,  13426,  13426,  13426,  14017,  14017,  14017,  14017, -29156, -29156, -29156, -29156, -12757, -12757, -12757, -12757,
  16832,  16832,  16832,  16832,   4311,   4311,   4311,   4311, -24155, -24155, -24155, -24155, -17915, -17915, -17915, -17915,
   -398,   -398,   -398,   -398,    961,    961,    961,    961,  -1508,  -1508,  -1508,  -1508,   -725,   -725,   -725,   -725,
    448,    448,    448,    448,  -1065,  -1065,  -1065,  -1065,    677,    677,    677,    677,  -1275,  -1275,  -1275,  -1275,

   -335,   -335,  11182,  11182, -11477, -11477,  13387,  13387, -32227, -32227, -14233, -14233,  20494,  20494, -21655, -21655,
 -27738, -27738,  13131,  13131,    945,    945,  -4587,  -4587, -14883, -14883,  23092,  23092,   6182,   6182,   5493,   5493,
  -1103,  -1103,    430,    430,    555,    555,    843,    843,  -1251,  -1251,    871,    871,   1550,   1550,    105,    105,
    422,    422,    587,    587,    177,    177,   -235,   -235,   -291,   -291,   -460,   -460,   1574,   1574,   1653,   1653,

  32010,  32010, -32502, -32502,  10631,  10631,  30317,  30317,  29175,  29175, -18741, -18741, -28762, -28762,  12639,  12639,
 -18486, -18486,  20100,  20100,  17560,  17560,  18525,  18525, -14430, -14430,  19529,  19529,  -5276,  -5276, -12619, -12619,
   -246,   -246,    778,    778,   1159,   1159,   -147,   -147,   -777,   -777,   1483,   1483,   -602,   -602,   1119,   1119,
  -1590,  -1590,    644,    644,   -872,   -872,    349,    349,    418,    418,    329,    329,   -156,   -156,    -75,    -75,

 -31183, -31183,  20297,  20297,  25435,  25435,   2146,   2146,  -7382,  -7382,  15355,  15355,  24391,  24391, -32384, -32384,
 -20927, -20927,  -6280,  -6280,  10946,  10946, -14903, -14903,  24214,  24214, -11044, -11044,  16989,  16989,  14469,  14469,
    817,    817,   1097,   1097,    603,    603,    610,    610,   1322,   1322,  -1285,  -1285,  -1465,  -1465,    384,    384,
  -1215,  -1215,   -136,   -136,   1218,   1218,  -1335,  -1335,   -874,   -874,    220,    220,  -1187,  -1187,  -1659,  -1659,

  10335,  10335, -21498, -21498,  -7934,  -7934, -20198, -20198, -22502, -22502,  23210,  23210,  10906,  10906, -17442, -17442,
  31636,  31636, -23860, -23860,  28644,  28644, -20257, -20257,  23998,  23998,   7756,   7756, -17422, -17422,  23132,  23132,
  -1185,  -1185,  -1530,  -1530,  -1278,  -1278,    794,    794,  -1510,  -1510,   -854,   -854,   -870,   -870,    478,    478,
   -108,   -108,   -308,   -308,    996,    996,    991,    991,    958,    958,  -1460,  -1460,   1522,   1522,   1628,   1628
};
inline fn __butterfly(reg u512 l r zl zh const_q) -> reg u512, reg u512
{
  reg u512 tmp;
  tmp = #VPMULL_32u16(zl, r);
  r = #VPMULH_32u16(zh, r);

  tmp = #VPMULH_32u16(const_q, tmp);
  tmp = #VPSUB_32u16(r, tmp);

  r = #VPSUB_32u16(l, 11);
  l = #VPADD_32u16(l, 11);
  return r, l;

}
fn _poly_ntt(reg ptr u16[MLKEM_N] rp) -> reg ptr u16[MLKEM_N]
{
  reg u512 qx32 r0 r1 r2 r3 r4 r5 r6 r7 zeta zetaqinv;
  reg u512 r8 r12 r13 r14 r15 r16 r17 r18 r19;
  reg ptr u16[1038] zetasp;
  
  zetasp = jzetas_exp;
  qx32 = jqx32[u512 0];

  r0 = rp.[u512 64*0];
  r1 = rp.[u512 64*1];
  r2 = rp.[u512 64*2];
  r3 = rp.[u512 64*3];
  r4 = rp.[u512 64*8];
  r5 = rp.[u512 64*9];
  r6 = rp.[u512 64*10];
  r7 = rp.[u512 64*11];
  //level0
  zeta = #VPBROADCAST_32u16(zetasp[u16 0]);
  zetaqinv = #VPBROADCAST_32u16(zetasp[u16 1]);

  
  r0, r4 = __butterfly(r0, r4, zeta, zetaqinv, qx32);
  r1, r5 = __butterfly(r1, r5, zeta, zetaqinv, qx32);
  r2, r6 = __butterfly(r2, r6, zeta, zetaqinv, qx32);
  r3, r7 = __butterfly(r3, r7, zeta, zetaqinv, qx32);
  //level1
  zeta = #VPBROADCAST_32u16(zetasp[u16 2]);
  zetaqinv = #VPBROADCAST_32u16(zetasp[u16 3]);
  
  r0, r2 = __butterfly(r0, r2, zeta, zetaqinv, qx32);
  r1, r3 =__butterfly(r1, r3, zeta, zetaqinv, qx32);

  zeta = #VPBROADCAST_32u16(zetasp[u16 4]);
  zetaqinv = #VPBROADCAST_32u16(zetasp[u16 5]);

  r4, r6 = __butterfly(r4, r6, zeta, zetaqinv, qx32);
  r5, r7 = __butterfly(r5, r7, zeta, zetaqinv, qx32);

  //level 2
  zeta = #VPBROADCAST_32u16(zetasp[u16 6]);
  zetaqinv = #VPBROADCAST_32u16(zetasp[u16 7]);
  r0, r1 = __butterfly(r0, r1, zeta, zetaqinv, qx32);
  zeta = #VPBROADCAST_32u16(zetasp[u16 8]);
  zetaqinv = #VPBROADCAST_32u16(zetasp[u16 9]);
  r2, r3 = __butterfly(r2, r3, zeta, zetaqinv, qx32);
  zeta = #VPBROADCAST_32u16(zetasp[u16 10]);
  zetaqinv = #VPBROADCAST_32u16(zetasp[u16 11]);
  r4, r5 = __butterfly(r4, r5, zeta, zetaqinv, qx32);
  zeta = #VPBROADCAST_32u16(zetasp[u16 12]);
  zetaqinv = #VPBROADCAST_32u16(zetasp[u16 13]);
  r6, r7 = __butterfly(r6, r7, zeta, zetaqinv, qx32);

  //level 3
  r12, r13 = __shuffle16(r0, r1);
  zeta = zetasp.[u512 14*64];
  zetaqinv = zetasp.[u512 46*64];
  r12, r13 = __butterfly(r12, r13, zeta, zetaqinv, qx32);

  r14, r15 = __shuffle16(r2, r3);
  zeta = zetasp.[u512 78*64];
  zetaqinv = zetasp.[u512 110*64];
  r14, r15 = __butterfly(r14, r15, zeta, zetaqinv, qx32);

  r16, r17 = __shuffle16(r4, r5);
  zeta = zetasp.[u512 142*64];
  zetaqinv = zetasp.[u512 174*64];
  r16, r17 = __butterfly(r16, r17, zeta, zetaqinv, qx32);

  r18, r19 = __shuffle16(r6, r7);
  zeta = zetasp.[u512 206*64];
  zetaqinv = zetasp.[u512 238*64];
  r18, r19 = __butterfly(r18, r19, zeta, zetaqinv, qx32);
  
  // level 4
  r1, r2 = __shuffle16(r12, r13);
  zeta = zetasp.[u512 270*64];
  zetaqinv = zetasp.[u512 302*64];
  r1, r2 = __butterfly(r1, r2, zeta, zetaqinv, qx32);

  r3, r4 = __shuffle16(r14, r15);
  zeta = zetasp.[u512 334*64];
  zetaqinv = zetasp.[u512 366*64];
  r3, r4 = __butterfly(r3, r4, zeta, zetaqinv, qx32);

  r5, r6 = __shuffle16(r16, r17);
  zeta = zetasp.[u512 398*64];
  zetaqinv = zetasp.[u512 430*64];
  r5, r6 = __butterfly(r5, r6, zeta, zetaqinv, qx32);

  r7, r8 = __shuffle16(r18, r19);
  zeta = zetasp.[u512 462*64];
  zetaqinv = zetasp.[u512 494*64];
  r7, r8 = __butterfly(r7, r8, zeta, zetaqinv, qx32);

  //level 5
  r12, r13 = __shuffle4(r1, r2);
  zeta = zetasp.[u512 526*64];
  zetaqinv = zetasp.[u512 558*64];
  r12, r13 = __butterfly(r12, r13, zeta, zetaqinv, qx32);

  r14, r15 = __shuffle4(r3, r4);
  zeta = zetasp.[u512 590*64];
  zetaqinv = zetasp.[u512 622*64];
  r14, r15 = __butterfly(r14, r15, zeta, zetaqinv, qx32);

  r16, r17 = __shuffle4(r5, r6);
  zeta = zetasp.[u512 654*64];
  zetaqinv = zetasp.[u512 686*64];
  r16, r17 = __butterfly(r16, r17, zeta, zetaqinv, qx32);

  r18, r19 = __shuffle4(r7, r8);
  zeta = zetasp.[u512 718*64];
  zetaqinv = zetasp.[u512 750*64];
  r18, r19 =__butterfly(r18, r19, zeta, zetaqinv, qx32);

  //level 6
  r1, r2 = __shuffle2(r12, r13);
  zeta = zetasp.[u512 782*64];
  zetaqinv = zetasp.[u512 814*64];
  r1, r2 = __butterfly(r1, r2, zeta, zetaqinv, qx32);

  r3, r4 = __shuffle2(r14, r15);
  zeta = zetasp.[u512 846*64];
  zetaqinv = zetasp.[u512 878*64];
  r3, r4 = __butterfly(r3, r4, zeta, zetaqinv, qx32);

  r5, r6 = __shuffle2(r16, r17);
  zeta = zetasp.[u512 910*64];
  zetaqinv = zetasp.[u512 942*64];
  r5, r6 = __butterfly(r5, r6, zeta, zetaqinv, qx32);

  r7, r8 = __shuffle2(r18, r19);
  zeta = zetasp.[u512 974*64];
  zetaqinv = zetasp.[u512 1006*64];
  r7, r8 = __butterfly(r7, r8, zeta, zetaqinv, qx32);

  r12, r13 = __shuffle1(r1, r2);
  r14, r15 = __shuffle1(r3, r4);
  r16, r17 = __shuffle1(r5, r6);
  r18, r19 = __shuffle1(r7, r8);

  rp.[u512 64*0] = r12;
  rp.[u512 64*1] = r13;
  rp.[u512 64*2] = r14;
  rp.[u512 64*3] = r15;
  rp.[u512 64*4] = r16;
  rp.[u512 64*5] = r17;
  rp.[u512 64*6] = r18;
  rp.[u512 64*7] = r19;
  return rp;
}
u16[1038] jzetas_inv_exp = { 23132, 23132,-17422,-17422,  7756,  7756, 23998, 23998,-20257,-20257, 28644, 28644,-23860,-23860, 31636, 31636,
-17442,-17442, 10906, 10906, 23210, 23210,-22502,-22502,-20198,-20198, -7934, -7934,-21498,-21498, 10335, 10335,
  1628,  1628,  1522,  1522, -1460, -1460,   958,   958,   991,   991,   996,   996,  -308,  -308,  -108,  -108,
   478,   478,  -870,  -870,  -854,  -854, -1510, -1510,   794,   794, -1278, -1278, -1530, -1530, -1185, -1185,
 14469, 14469, 16989, 16989,-11044,-11044, 24214, 24214,-14903,-14903, 10946, 10946, -6280, -6280,-20927,-20927,
-32384,-32384, 24391, 24391, 15355, 15355, -7382, -7382,  2146,  2146, 25435, 25435, 20297, 20297,-31183,-31183,   
 -1659, -1659, -1187, -1187,   220,   220,  -874,  -874, -1335, -1335,  1218,  1218,  -136,  -136, -1215, -1215,
   384,   384, -1465, -1465, -1285, -1285,  1322,  1322,   610,   610,   603,   603,  1097,  1097,   817,   817,
-12619,-12619, -5276, -5276, 19529, 19529,-14430,-14430, 18525, 18525, 17560, 17560, 20100, 20100,-18486,-18486,
 12639, 12639,-28762,-28762,-18741,-18741, 29175, 29175, 30317, 30317, 10631, 10631,-32502,-32502, 32010, 32010,
   -75,   -75,  -156,  -156,   329,   329,   418,   418,   349,   349,  -872,  -872,   644,   644, -1590, -1590,
  1119,  1119,  -602,  -602,  1483,  1483,  -777,  -777,  -147,  -147,  1159,  1159,   778,   778,  -246,  -246,
  5493,  5493,  6182,  6182, 23092, 23092,-14883,-14883, -4587, -4587,   945,   945, 13131, 13131,-27738,-27738,
-21655,-21655, 20494, 20494,-14233,-14233,-32227,-32227, 13387, 13387,-11477,-11477, 11182, 11182,  -335,  -335,
  1653,  1653,  1574,  1574,  -460,  -460,  -291,  -291,  -235,  -235,   177,   177,   587,   587,   422,   422,
   105,   105,  1550,  1550,   871,   871, -1251, -1251,   843,   843,   555,   555,   430,   430, -1103, -1103,
-17915,-17915,-17915,-17915,-24155,-24155,-24155,-24155,  4311,  4311,  4311,  4311, 16832, 16832, 16832, 16832,
-12757,-12757,-12757,-12757,-29156,-29156,-29156,-29156, 14017, 14017, 14017, 14017, 13426, 13426, 13426, 13426,
 -1275, -1275, -1275, -1275,   677,   677,   677,   677, -1065, -1065, -1065, -1065,   448,   448,   448,   448,
  -725,  -725,  -725,  -725, -1508, -1508, -1508, -1508,   961,   961,   961,   961,  -398,  -398,  -398,  -398,
 18249, 18249, 18249, 18249,-30199,-30199,-30199,-30199,  9075,  9075,  9075,  9075,-28309,-28309,-28309,-28309,
 -8898, -8898, -8898, -8898,-15887,-15887,-15887,-15887,-28250,-28250,-28250,-28250, 19883, 19883, 19883, 19883,
  -951,  -951,  -951,  -951,  -247,  -247,  -247,  -247, -1421, -1421, -1421, -1421,   107,   107,   107,   107,
   830,   830,   830,   830,  -271,  -271,  -271,  -271,   -90,   -90,   -90,   -90,  -853,  -853,  -853,  -853,
 27837, 27837, 27837, 27837,-25986,-25986,-25986,-25986,  -650,  -650,  -650,  -650,  9134,  9134,  9134,  9134,
-12442,-12442,-12442,-12442, 16064, 16064, 16064, 16064, 26616, 26616, 26616, 26616,-12796,-12796,-12796,-12796,
  1469,  1469,  1469,  1469,   126,   126,   126,   126, -1162, -1162, -1162, -1162, -1618, -1618, -1618, -1618,
  -666,  -666,  -666,  -666,  -320,  -320,  -320,  -320,    -8,    -8,    -8,    -8,   516,   516,   516,   516,
 25080, 25080, 25080, 25080, 20710, 20710, 20710, 20710, 20179, 20179, 20179, 20179,-23565,-23565,-23565,-23565,
 30967, 30967, 30967, 30967,  1496,  1496,  1496,  1496, -6516, -6516, -6516, -6516, -5689, -5689, -5689, -5689,
 -1544, -1544, -1544, -1544,  -282,  -282,  -282,  -282,  1491,  1491,  1491,  1491, -1293, -1293, -1293, -1293,
  1015,  1015,  1015,  1015,  -552,  -552,  -552,  -552,   652,   652,   652,   652,  1223,  1223,  1223,  1223,
-16163,-16163,-16163,-16163,-16163,-16163,-16163,-16163, 26675, 26675, 26675, 26675, 26675, 26675, 26675, 26675,
  8859,  8859,  8859,  8859,  8859,  8859,  8859,  8859, 18426, 18426, 18426, 18426, 18426, 18426, 18426, 18426,
 -1571, -1571, -1571, -1571, -1571, -1571, -1571, -1571,  -205,  -205,  -205,  -205,  -205,  -205,  -205,  -205,
   411,   411,   411,   411,   411,   411,   411,   411, -1542, -1542, -1542, -1542, -1542, -1542, -1542, -1542,
  8800,  8800,  8800,  8800,  8800,  8800,  8800,  8800,-10532,-10532,-10532,-10532,-10532,-10532,-10532,-10532,
 24313, 24313, 24313, 24313, 24313, 24313, 24313, 24313,-28073,-28073,-28073,-28073,-28073,-28073,-28073,-28073,
   608,   608,   608,   608,   608,   608,   608,   608,   732,   732,   732,   732,   732,   732,   732,   732,
  1017,  1017,  1017,  1017,  1017,  1017,  1017,  1017,  -681,  -681,  -681,  -681,  -681,  -681,  -681,  -681,
-26242,-26242,-26242,-26242,-26242,-26242,-26242,-26242, 21438, 21438, 21438, 21438, 21438, 21438, 21438, 21438,
 -1102, -1102, -1102, -1102, -1102, -1102, -1102, -1102,  5571,  5571,  5571,  5571,  5571,  5571,  5571,  5571,
  -130,  -130,  -130,  -130,  -130,  -130,  -130,  -130, -1602, -1602, -1602, -1602, -1602, -1602, -1602, -1602,
  1458,  1458,  1458,  1458,  1458,  1458,  1458,  1458,  -829,  -829,  -829,  -829,  -829,  -829,  -829,  -829,
-29057,-29057,-29057,-29057,-29057,-29057,-29057,-29057,-26360,-26360,-26360,-26360,-26360,-26360,-26360,-26360,
 17363, 17363, 17363, 17363, 17363, 17363, 17363, 17363, -5827, -5827, -5827, -5827, -5827, -5827, -5827, -5827,
   383,   383,   383,   383,   383,   383,   383,   383,   264,   264,   264,   264,   264,   264,   264,   264,
 -1325, -1325, -1325, -1325, -1325, -1325, -1325, -1325,   573,   573,   573,   573,   573,   573,   573,   573,
 31164, 31164, 31164, 31164, 31164, 31164, 31164, 31164, 31164, 31164, 31164, 31164, 31164, 31164, 31164, 31164,
-11202,-11202,-11202,-11202,-11202,-11202,-11202,-11202,-11202,-11202,-11202,-11202,-11202,-11202,-11202,-11202,
  1468,  1468,  1468,  1468,  1468,  1468,  1468,  1468,  1468,  1468,  1468,  1468,  1468,  1468,  1468,  1468,
 -1474, -1474, -1474, -1474, -1474, -1474, -1474, -1474, -1474, -1474, -1474, -1474, -1474, -1474, -1474, -1474,
  1358,  1358,  1358,  1358,  1358,  1358,  1358,  1358,  1358,  1358,  1358,  1358,  1358,  1358,  1358,  1358,
 10690, 10690, 10690, 10690, 10690, 10690, 10690, 10690, 10690, 10690, 10690, 10690, 10690, 10690, 10690, 10690,
 -1202, -1202, -1202, -1202, -1202, -1202, -1202, -1202, -1202, -1202, -1202, -1202, -1202, -1202, -1202, -1202,
   962,   962,   962,   962,   962,   962,   962,   962,   962,   962,   962,   962,   962,   962,   962,   962,
-15690,-15690,-15690,-15690,-15690,-15690,-15690,-15690,-15690,-15690,-15690,-15690,-15690,-15690,-15690,-15690,
 -3799, -3799, -3799, -3799, -3799, -3799, -3799, -3799, -3799, -3799, -3799, -3799, -3799, -3799, -3799, -3799,
   182,   182,   182,   182,   182,   182,   182,   182,   182,   182,   182,   182,   182,   182,   182,   182,
  1577,  1577,  1577,  1577,  1577,  1577,  1577,  1577,  1577,  1577,  1577,  1577,  1577,  1577,  1577,  1577,
 27758, 27758, 27758, 27758, 27758, 27758, 27758, 27758, 27758, 27758, 27758, 27758, 27758, 27758, 27758, 27758,
-20907,-20907,-20907,-20907,-20907,-20907,-20907,-20907,-20907,-20907,-20907,-20907,-20907,-20907,-20907,-20907,
   622,   622,   622,   622,   622,   622,   622,   622,   622,   622,   622,   622,   622,   622,   622,   622,
  -171,  -171,  -171,  -171,  -171,  -171,  -171,  -171,  -171,  -171,  -171,  -171,  -171,  -171,  -171,  -171,
 -16694,    202,
  28191,    287,
 -12402,   1422,
  13525,   1493,
    787,  -1517,
  14745,   -359,
  31498,   -758};
inline fn __GSbutterfly(reg u512 l r zl zh const_q) -> reg u512, reg u512
{
  reg u512 tmp tmp2;
  tmp = #VPSUB_32u16(r, l);
  l = #VPADD_32u16(l, r);
  tmp2 = #VPMULH_32u16(zh, tmp);
  tmp = #VPMULL_32u16(zl, tmp);
  tmp = #VPMULH_32u16(tmp, const_q);
  r =#VPSUB_32u16(tmp2, tmp);
  return l, r;

}

fn _poly_invntt(reg ptr u16[MLKEM_N] rp) -> reg ptr u16[MLKEM_N]
{
  reg u512 r1 r2 r3 r4 r5 r6 r7 r8 r9 r10 r11 r12 r13 r14 r15 r16 qx32 zeta zetaqinv vx32;
  reg ptr u16[1038] zetasp;
  zetasp = jzetas_inv_exp;
  qx32 = jqx32[u512 0];
  vx32 = jvx32[u512 0];

  r1 = rp.[u512 64*0];
  r2 = rp.[u512 64*1];
  r3 = rp.[u512 64*2];
  r4 = rp.[u512 64*3];
  r5 = rp.[u512 64*4];
  r6 = rp.[u512 64*5];
  r7 = rp.[u512 64*6];
  r8 = rp.[u512 64*7];

  zeta = #VPBROADCAST_32u16(jzetasmul_exp[u16 0]);
  zetaqinv = #VPBROADCAST_32u16(jzetasmul_exp[u16 1]);

  r1 = __montmul(zeta, zetaqinv, qx32, r1);
  r2 = __montmul(zeta, zetaqinv, qx32, r2);
  r3 = __montmul(zeta, zetaqinv, qx32, r3);
  r4 = __montmul(zeta, zetaqinv, qx32, r4);
  r5 = __montmul(zeta, zetaqinv, qx32, r5);
  r6 = __montmul(zeta, zetaqinv, qx32, r6);
  r7 = __montmul(zeta, zetaqinv, qx32, r7);
  r8 = __montmul(zeta, zetaqinv, qx32, r8);
  // level 0
  r9, r10 = __shuffle1(r1, r2);
  r11, r12 = __shuffle1(r3, r4);
  r13, r14 = __shuffle1(r5, r6);
  r15, r16 = __shuffle1(r7, r8);

  zeta = zetasp.[u512 64*0];
  zetaqinv = zetasp.[u512 64*1];
  r9, r10 = __GSbutterfly(r9, r10, zeta, zetaqinv, qx32);
  zeta = zetasp.[u512 64*2];
  zetaqinv = zetasp.[u512 64*3];
  r11, r12 = __GSbutterfly(r11, r12, zeta, zetaqinv, qx32);
  zeta = zetasp.[u512 64*4];
  zetaqinv = zetasp.[u512 64*5];
  r13, r14 = __GSbutterfly(r13, r14, zeta, zetaqinv, qx32);
  zeta = zetasp.[u512 64*6];
  zetaqinv = zetasp.[u512 64*7];
  r15, r16 = __GSbutterfly(r15, r16, zeta, zetaqinv, qx32);
  // level 1
  r1, r2 = __shuffle2(r9, r10);
  r3, r4 = __shuffle2(r11, r12);
  r5, r6 = __shuffle2(r13, r14);
  r7, r8 = __shuffle2(r15, r16);

  zeta = zetasp.[u512 64*8];
  zetaqinv = zetasp.[u512 64*9];
  r1, r2 = __GSbutterfly(r1, r2, zeta, zetaqinv, qx32);
  zeta = zetasp.[u512 64*10];
  zetaqinv = zetasp.[u512 64*11];
  r3, r4 = __GSbutterfly(r3, r4, zeta, zetaqinv, qx32);
  zeta = zetasp.[u512 64*12];
  zetaqinv = zetasp.[u512 64*13];
  r5, r6 = __GSbutterfly(r5, r6, zeta, zetaqinv, qx32);
  zeta = zetasp.[u512 64*14];
  zetaqinv = zetasp.[u512 64*15];
  r7, r8 = __GSbutterfly(r7, r8, zeta, zetaqinv, qx32);

  //level 2
  r9, r10 = __shuffle4(r1, r2);
  r11, r12 = __shuffle4(r3, r4);
  r13, r14 = __shuffle4(r5, r6);
  r15, r16 = __shuffle4(r7, r8);
  
  zeta = zetasp.[u512 64*16];
  zetaqinv = zetasp.[u512 64*17];
  r9, r10 = __GSbutterfly(r9, r10, zeta, zetaqinv, qx32);
  zeta = zetasp.[u512 64*18];
  zetaqinv = zetasp.[u512 64*19];
  r11, r12 = __GSbutterfly(r11, r12, zeta, zetaqinv, qx32);
  zeta = zetasp.[u512 64*20];
  zetaqinv = zetasp.[u512 64*21];
  r13, r14 = __GSbutterfly(r13, r14, zeta, zetaqinv, qx32);
  zeta = zetasp.[u512 64*22];
  zetaqinv = zetasp.[u512 64*23];
  r15, r16 = __GSbutterfly(r15, r16, zeta, zetaqinv, qx32);

  r9 = __red16(r9, vx32, qx32);
  r11 = __red16(r11, vx32, qx32);
  r13 = __red16(r13, vx32, qx32);
  r15 = __red16(r15, vx32, qx32);

  //level 3

  r1, r2 = __shuffle8(r9, r10);
  r3, r4 = __shuffle8(r11, r12);
  r5, r6 = __shuffle8(r13, r14);
  r7, r8 = __shuffle8(r15, r16);

  zeta = zetasp.[u512 64*24];
  zetaqinv = zetasp.[u512 64*25];
  r1, r2 = __GSbutterfly(r1, r2, zeta, zetaqinv, qx32);
  zeta = zetasp.[u512 64*26];
  zetaqinv = zetasp.[u512 64*27];
  r3, r4 = __GSbutterfly(r3, r4, zeta, zetaqinv, qx32);
  zeta = zetasp.[u512 64*28];
  zetaqinv = zetasp.[u512 64*29];
  r5, r6 = __GSbutterfly(r5, r6, zeta, zetaqinv, qx32);
  zeta = zetasp.[u512 64*30];
  zetaqinv = zetasp.[u512 64*31];
  r7, r8 = __GSbutterfly(r7, r8, zeta, zetaqinv, qx32);
  //level 4
  r9, r10 = __shuffle16(r1, r2);
  r11, r12 = __shuffle16(r3, r4);
  r13, r14 = __shuffle16(r5, r6);
  r15, r16 = __shuffle16(r7, r8);

  zeta = #VPBROADCAST_32u16(zetasp[u16 1024]);
  zetaqinv = #VPBROADCAST_32u16(zetasp[u16 1025]);
  r9, r10 = __GSbutterfly(r9, r10, zeta, zetaqinv, qx32);
  zeta = #VPBROADCAST_32u16(zetasp[u16 1026]);
  zetaqinv = #VPBROADCAST_32u16(zetasp[u16 1027]);
  r11, r12 = __GSbutterfly(r11, r12, zeta, zetaqinv, qx32);
  zeta = #VPBROADCAST_32u16(zetasp[u16 1028]);
  zetaqinv = #VPBROADCAST_32u16(zetasp[u16 1029]);
  r13, r14 = __GSbutterfly(r13, r14, zeta, zetaqinv, qx32);
  zeta = #VPBROADCAST_32u16(zetasp[u16 1030]);
  zetaqinv = #VPBROADCAST_32u16(zetasp[u16 1031]);
  r15, r16 = __GSbutterfly(r15, r16, zeta, zetaqinv, qx32);

  r9 = __red16(r9, vx32, qx32);
  r11 = __red16(r11, vx32, qx32);
  r13 = __red16(r13, vx32, qx32);
  r15 = __red16(r15, vx32, qx32);

  //level 5
  zeta = #VPBROADCAST_32u16(zetasp[u16 1032]);
  zetaqinv = #VPBROADCAST_32u16(zetasp[u16 1033]);
  r9, r11 = __GSbutterfly(r9, r11, zeta, zetaqinv, qx32);
  r10, r12 = __GSbutterfly(r10, r12, zeta, zetaqinv, qx32);

  zeta = #VPBROADCAST_32u16(zetasp[u16 1034]);
  zetaqinv = #VPBROADCAST_32u16(zetasp[u16 1035]);
  r13, r15 = __GSbutterfly(r13, r15, zeta, zetaqinv, qx32);
  r14, r16 = __GSbutterfly(r14, r16, zeta, zetaqinv, qx32);

  //level 6
  zeta = #VPBROADCAST_32u16(zetasp[u16 1036]);
  zetaqinv = #VPBROADCAST_32u16(zetasp[u16 1037]);
  r9, r13 = __GSbutterfly(r9, r13, zeta, zetaqinv, qx32);
  r10, r14 = __GSbutterfly(r10, r14, zeta, zetaqinv, qx32);
  r11, r15 = __GSbutterfly(r11, r15, zeta, zetaqinv, qx32);
  r12, r16 = __GSbutterfly(r12, r16, zeta, zetaqinv, qx32);

  r9 = __red16(r9, vx32, qx32);

  rp.[u512 0*64] = r9;
  rp.[u512 1*64] = r10;
  rp.[u512 2*64] = r11;
  rp.[u512 3*64] = r12;
  rp.[u512 4*64] = r13;
  rp.[u512 5*64] = r14;
  rp.[u512 6*64] = r15;
  rp.[u512 7*64] = r16;
  
  return rp;
}
u16[32] mask_fbts = {0x0FF, 0x0FF, 0x0FF, 0x0FF, 0x0FF, 0x0FF, 0x0FF, 0x0FF, 0x0FF, 0x0FF, 0x0FF, 0x0FF, 0x0FF, 0x0FF, 0x0FF, 0x0FF,0x0FF, 0x0FF, 0x0FF, 0x0FF, 0x0FF, 0x0FF, 0x0FF, 0x0FF, 0x0FF, 0x0FF, 0x0FF, 0x0FF, 0x0FF, 0x0FF, 0x0FF, 0x0FF};
u8[64] idx1_fbts = {0,  1,  3,  4,  6,  7,  9, 10, 12, 13, 15, 16, 18, 19, 21, 22,
24, 25, 27, 28, 30, 31, 33, 34, 36, 37, 39, 40, 42, 43, 45, 46,
-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
};
u8[64] idx2_fbts = {1,  2,  4,  5,  7,  8, 10, 11, 13, 14, 16, 17, 19, 20, 22, 23,
25, 26, 28, 29, 31, 32, 34, 35, 37, 38, 40, 41, 43, 44, 46, 47,
-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
};
fn _poly_frombytes(reg ptr u16[MLKEM_N] rp, reg u64 ap) -> reg ptr u16[MLKEM_N]
{
  inline int i;
  reg u512 f0 f1 f2 f3 f4 f5;
  reg u512 tmp;
  // const __mmask64 mmask1 = _cvtu64_mask64(0x00000000FFFFFFFF);
  // const __mmask64 mmask2 = _cvtu64_mask64(0x0000FFFFFFFFFFFF);
  reg u512 mask = mask_fbts[u512 0];
  reg u512 idx1 idx2;
  idx1 = idx1_fbts[u512 0];
  idx2 = idx2_fbts[u512 0];

  for i = 0 to 2
  {
    f0 = (u512)[ap + i*128];
    f1 = (u512)[ap + i*128 + 64];
    // need mask registers
    // f2 = _mm512_maskz_permutexvar_epi8(mmask1, idx1, f0);
    // f3 = _mm512_maskz_permutexvar_epi8(mmask1, idx2, f0);
    // f4 = _mm512_maskz_permutexvar_epi8(mmask1, idx1, f1);
    // f5 = _mm512_maskz_permutexvar_epi8(mmask1, idx2, f1);

    tmp = #VPEXTRACTI64X4(f4, 0);
    f0 = #VINSERTI64X4(f2, tmp, 1);
    tmp = #VPEXTRACTI64X4(f5, 0);
    f1 = #VINSERTI64X4(f3, tmp, 1);
    f0 = #VPAND_512(f0, mask);
    f1 = #VPSRL_32u16(f1, 4);

    rp[u512 2*i + 0] = f0;
    rp[u512 2*i + 1] = f1;


  }
  // f0 = _mm512_mask_loadu_epi8(f0,mmask2,&a[288]);
  // f1 = _mm512_mask_loadu_epi8(f1,mmask2,&a[336]);
  // f2 = _mm512_maskz_permutexvar_epi8(mmask1, idx1, f0);
  // f3 = _mm512_maskz_permutexvar_epi8(mmask1, idx2, f0);
  // f4 = _mm512_maskz_permutexvar_epi8(mmask1, idx1, f1);
  // f5 = _mm512_maskz_permutexvar_epi8(mmask1, idx2, f1);
  tmp = #VPEXTRACTI64X4(f4, 0);
  f0 = #VINSERTI64X4(f2, tmp, 1);
  tmp = #VPEXTRACTI64X4(f5, 0);
  f1 = #VINSERTI64X4(f3, tmp, 1);
  f0 = #VPAND_512(f0, mask);
  f1 = #VPSRL_32u16(f1, 4);
  rp[u512 6] = f0;
  rp[u512 7] = f1;
}
export fn poly_frombytes_jazz(reg u64 rp, reg u64 ap) 
{
  stack u16[MLKEM_N] r;
  reg u16 t;
  inline int i;

  r = _poly_frombytes(r, ap);

  r = _nttpack(r);

  for i = 0 to MLKEM_N {
    t = r[i];
    (u16)[rp + 2*i] = t;
  }
}

u8[64] idx1_tbts = { 0, 1,-1, 2, 3,-1, 4, 5,-1, 6, 7,-1, 8, 9,-1,10,11,-1,12,13,-1,14,15,-1,16,17,-1,18,19,-1,20,21,-1,22,23,-1,24,25,-1,26,27,-1,28,29,-1,30,31,-1,
-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1};
u8[64] idx2_tbts = {-1,-1, 0,-1,-1, 2,-1,-1, 4,-1,-1, 6,-1,-1, 8,-1,-1,10,-1,-1,12,-1,-1,14,-1,-1,16,-1,-1,18,-1,-1,20,-1,-1,22,-1,-1,24,-1,-1,26,-1,-1,28,-1,-1,30,
-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1};
fn _poly_tobytes(reg u64 rp, reg ptr u16[MLKEM_N] a) -> reg ptr u16[MLKEM_N]
{
  inline int i;
  reg u512 f0 f1 f2 f3 f4 f5;
  reg u512 const_q = jqx32[u512 0];
  reg u512 idx1 idx2;
  idx1 = idx1_tbts[u512 0];
  idx2 = idx2_tbts[u512 0];
  reg u256 g;
  f4 = #set0_512();
  f5 = #set0_512();
  // const __mmask64 mmask_s = _cvtu64_mask64(0x0000FFFFFFFFFFFF);
  // const __mmask64 mmask1 = _cvtu64_mask64(0x00006DB6DB6DB6DB);
  // const __mmask64 mmask2 = _cvtu64_mask64(0x0000924924924924);

  for i= 0 to 2
  {
    f0 = a[u512 2*i + 0];
    f1 = a[u512 2*i + 1];
    f0 = #VPSUB_32u16(f0, const_q);
    f1 = #VPSUB_32u16(f1, const_q);
    f2 = #VPSRA_32u16(f0, 15);
    f3 = #VPSRA_32u16(f1, 15);
    f2 = #VPAND_512(f2, vec_q);
    f3 = #VPAND_512(f3, vec_q);
    f0 = #VPADD_32u16(f0, f2);
    f1 = #VPADD_32u16(f1, f3);

    f2 = #VPSLL_32u16(f1, 12);
    f2 = #VPOR_512(f0, f2);
    f3 = #VPSRL_32u16(f1, 4);
    g = #VPEXTRACTI64X4(f2, 1);
    f4 = #VINSERTI64X4(f4, g, 0);
    g = #VPEXTRACTI64X4(f3, 1);
    f5 = #VINSERTI64X4(f5, g, 0);
    // f0 = _mm512_maskz_permutexvar_epi8(mmask1, idx1, f2);
    // f1 = _mm512_maskz_permutexvar_epi8(mmask2, idx2, f3);
    f0 = #VPOR_512(f0, f1);
    // f2 = _mm512_maskz_permutexvar_epi8(mmask1, idx1, f4);
    // f3 = _mm512_maskz_permutexvar_epi8(mmask2, idx2, f5);
    f2 = #VPOR_512(f2, f3);

    (u512)[rp + 96*i] = f0;
    (u512)[rp + 96*i + 48]  = f0;

  }
  f0 = a[u512 6];
  f1 = a[u512 7];
  f0 = #VPSUB_32u16(f0, const_q);
  f1 = #VPSUB_32u16(f1, const_q);
  f2 = #VPSRA_32u16(f0, 15);
  f3 = #VPSRA_32u16(f1, 15);
  f2 = #VPAND_512(f2, const_q);
  f3 = #VPAND_512(f3, const_q);
  f0 = #VPADD_32u16(f0, f2);
  f1 = #VPADD_32u16(f1, f3);
  f2 = #VPSLL_32u16(f1, 12);
  f2 = #VPOR_512(f0, f2);
  f3 = #VPSRL_32u16(f1, 4);
  g = #VPEXTRACTI64X4(f2, 1);
  f4 = #VINSERTI64X4(f4, g, 0);
  g = #VPEXTRACTI64X4(f3, 1);
  f5 = #VINSERTI64X4(f5, g, 0);
  // f0 = _mm512_maskz_permutexvar_epi8(mmask1, idx1, f2);
  // f1 = _mm512_maskz_permutexvar_epi8(mmask2, idx2, f3);
  f0 = #VPOR_512(f0, f1);
  // f2 = _mm512_maskz_permutexvar_epi8(mmask1, idx1, f4);
  // f3 = _mm512_maskz_permutexvar_epi8(mmask2, idx2, f5);
  f2 = #VPOR_512(f2, f3);

  (u512)[rp + 576] = f0;
  (u512)[rp + 672]  = f0;

}

export fn poly_tobytes_jazz(reg u64 rp, reg u64 ap) 
{
  stack u16[MLKEM_N] a;
  reg u16 t;
  inline int i;

  for i = 0 to MLKEM_N {
    t = (u16)[ap + 2*i];
    a[i] = t;
  }
  
  a = _nttunpack(a);
  a = _poly_tobytes(rp, a);
}
// Transform from AVX order to bitreversed order
inline 
fn __nttpack256(reg u512 r0 r1 r2 r3 r4 r5 r6 r7)
    -> reg u512, reg u512, reg u512, reg u512, reg u512, reg u512, reg u512, reg u512
{
  reg u512 r8, r9;
  r8, r9 = __shuffle1(r0, r1);
  r0, r1 = __shuffle1(r2, r3);
  r2, r3 = __shuffle1(r4, r5);
  r4, r5 = __shuffle1(r6, r7);

  r6, r7 = __shuffle2(r8, r9);
  r8, r9 = __shuffle2(r0, r1);
  r0, r1 = __shuffle2(r2, r3);
  r2, r3 = __shuffle2(r4, r5);

  r4, r5 = __shuffle4(r6, r7);
  r6, r7 = __shuffle4(r8, r9);
  r8, r9 = __shuffle4(r0, r1);
  r0, r1 = __shuffle4(r2, r3);

  r2, r3 = __shuffle8(r4, r5);
  r4, r5 = __shuffle8(r6, r7);
  r6, r7 = __shuffle8(r8, r9);
  r8, r9 = __shuffle8(r0, r1);

  r0, r1 = __shuffle16(r2, r3);
  r2, r3 = __shuffle16(r4, r5);
  r4, r5 = __shuffle16(r6, r7);
  r6, r7 = __shuffle16(r8, r9);

  return r0, r1, r2, r3, r4, r5, r6, r7;
}
// Transform from bitreversed order to AVX order
inline
fn __nttunpack256(reg u512 r0 r1 r2 r3 r4 r5 r6 r7)
    -> reg u512, reg u512, reg u512, reg u512, reg u512, reg u512, reg u512, reg u512
{
  reg u512 r8 r9;
  r8, r9 = __shuffle16(r0, r1);
  r0, r1 = __shuffle16(r2, r3);
  r2, r3 = __shuffle16(r4, r5);
  r4, r5 = __shuffle16(r6, r7);

  r6, r7 = __shuffle8(r8, r9);
  r8, r9 = __shuffle8(r0, r1);
  r0, r1 = __shuffle8(r2, r3);
  r2, r3 = __shuffle8(r4, r5);

  r4, r5 = __shuffle4(r6, r7);
  r6, r7 = __shuffle4(r8, r9);
  r8, r9 = __shuffle4(r0, r1);
  r0, r1 = __shuffle4(r2, r3);

  r2, r3 = __shuffle2(r4, r5);
  r4, r5 = __shuffle2(r6, r7);
  r6, r7 = __shuffle2(r8, r9);
  r8, r9 = __shuffle2(r0, r1);

  r0, r1 = __shuffle1(r2, r3);
  r2, r3 = __shuffle1(r4, r5);
  r4, r5 = __shuffle1(r6, r7);
  r6, r7 = __shuffle1(r8, r9);

  return r0, r1, r2, r3, r4, r5, r6, r7;
}

export fn poly_nttpack(reg ptr u16[MLKEM_N] rp) -> reg ptr u16[MLKEM_N]
{
  reg u512 r0 r1 r2 r3 r4 r5 r6 r7;

  r0 = rp.[u512 64*0];
  r1 = rp.[u512 64*1];
  r2 = rp.[u512 64*2];
  r3 = rp.[u512 64*3];
  r4 = rp.[u512 64*4];
  r5 = rp.[u512 64*5];
  r6 = rp.[u512 64*6];
  r7 = rp.[u512 64*7];

  r0, r1, r2, r3, r4, r5, r6, r7 = __nttpack256(r0, r1, r2, r3, r4, r5, r6, r7);

  rp.[u512 64*0] = r0;
  rp.[u512 64*1] = r1;
  rp.[u512 64*2] = r2;
  rp.[u512 64*3] = r3;
  rp.[u512 64*4] = r4;
  rp.[u512 64*5] = r5;
  rp.[u512 64*6] = r6;
  rp.[u512 64*7] = r7;

  return rp;
}

export fn poly_nttunpack(reg ptr u16[MLKEM_N] rp) -> reg ptr u16[MLKEM_N]
{
  reg u512 r0 r1 r2 r3 r4 r5 r6 r7;

  r0 = rp.[u512 64*0];
  r1 = rp.[u512 64*1];
  r2 = rp.[u512 64*2];
  r3 = rp.[u512 64*3];
  r4 = rp.[u512 64*4];
  r5 = rp.[u512 64*5];
  r6 = rp.[u512 64*6];
  r7 = rp.[u512 64*7];

  r0, r1, r2, r3, r4, r5, r6, r7 = __nttunpack256(r0, r1, r2, r3, r4, r5, r6, r7);

  rp.[u512 64*0] = r0;
  rp.[u512 64*1] = r1;
  rp.[u512 64*2] = r2;
  rp.[u512 64*3] = r3;
  rp.[u512 64*4] = r4;
  rp.[u512 64*5] = r5;
  rp.[u512 64*6] = r6;
  rp.[u512 64*7] = r7;

  return rp;
}