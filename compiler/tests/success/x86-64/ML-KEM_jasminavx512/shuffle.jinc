inline fn __shuffle16(reg u512 r0 r1) -> reg u512, reg u512
{
    reg u512 r2 r3;
    r2 = #VSHUFI32X4(r0, r1, 0x44);
    r3 = #VSHUFI32X4(r0, r1, 0xEE);
    return r2, r3;
}
//not finish yet 
inline fn __shuffle8(reg u512 r0 r1) -> reg u512, reg u512
{
    reg u512 tmp r2 r3;
    tmp = #VPERMQ_512(r1, 0x44);
    //vpblendmq       %zmm20, %zmm\r0, %zmm\r2{%k7}
    tmp = #VPERMQ_512(r0, 0xEE);
    //vpblendmq       %zmm20, %zmm\r1, %zmm\r3{%k6}
    return r2, r3;
}
inline fn __shuffle4(reg u512 r0 r1) -> reg u512, reg u512
{
    reg u512 r2 r3;
    r2 = #VPUNPCKL_64u8(r0, r1);
    r3 = #VPUNPCKH_64u8(r0, r1);
    return r2, r3;
}

inline fn __shuffle2(reg u512 r0 r1) -> reg u512, reg u512
{
    reg u512 r2 r3 tmp;
    tmp = #VMOVSLDUP_512(r1);
    //vpblendmd       %zmm20, %zmm\r0, %zmm\r2{%k5}
    tmp = #VMOVSHDUP_512(r0);
    //vpblendmd       %zmm20, %zmm\r1, %zmm\r3{%k4}
    return r2, r3;
}
inline fn __shuffle1(reg u512 r0 r1) -> reg u512, reg u512
{
    // reg u512 r2 r3 tmp;
    reg u512 tmp;
    tmp = #VPSLL_16u32(r1, 16);
    //vpblendmw       %zmm20, %zmm\r0, %zmm\r2{%k3}
    tmp = #VPSRL_16u32(r0, 16);
    //vpblendmw       %zmm20, %zmm\r1, %zmm\r3{%k2}
    return r0, r1;
}