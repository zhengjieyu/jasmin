require "params.jinc"
require "shuffle.jinc"
require "consts.jinc"
require "reduce.jinc"
require "fips202.jinc"
require "fips202x8.jinc"
require "consts.jinc"

inline
fn mont_red(reg u512 lo hi qx32 qinvx32) -> reg u512 {
  reg u512 m;

  m  = #VPMULL_32u16(lo, qinvx32);
  m  = #VPMULH_32u16(m, qx32);
  lo = #VPSUB_32u16(hi, m);

  return lo;
}
inline 
fn __csubq(reg u512 r qx16) -> reg u512
{
  reg u512 t;
  r = #VPSUB_32u16(r, qx16);
  t = #VPSRA_32u16(r, 15);
  t = #VPAND_512(t, qx16);
  r = #VPADD_32u16(t, r);
  return r;
}
inline
fn __poly_reduce(reg ptr u16[MLKEM_N] rp) -> reg ptr u16[MLKEM_N]
{
  inline int i;
  reg u512 r qx32 vx32;
  
  qx32 = jqx32[u512 0];
  vx32 = jvx32[u512 0];

  for i=0 to 8 
  {
    r = rp.[u512 64*i];
    r = __red16(r, vx32, qx32);
    rp.[u512 64*i] = r;
  }
  return rp;
}
fn _poly_csubq(reg ptr u16[MLKEM_N] rp) -> reg ptr u16[MLKEM_N]
{
  reg u512 r qx16;
  inline int i;
  
  qx16 = jqx32[u512 0];

  for i=0 to 8 {
    r = rp.[u512 64*i];
    r = __csubq(r, qx16);
    rp.[u512 64*i] = r;
  }

  return rp;
}

fn _poly_frommont(reg ptr u16[MLKEM_N] rp) -> reg ptr u16[MLKEM_N]
{
  reg u512 qx32 qinvx32 dmontx32 t;
  inline int i;
  reg ptr u16[32] x32p;
  x32p = jqx32;
  qx32 = x32p[u512 0];
  x32p = jqinvx32;
  qinvx32 = x32p[u512 0];
  dmontx32 = #VPBROADCAST_32u16(DMONT);

  for i = 0 to MLKEM_N/32
  {
    t = rp[u512 i];
    t = __montmul(t, dmontx32, qx32, qinvx32);
    rp[u512 i] = t;
  }
  return rp;

}

u16 pc_shift1_s = 0x200;
u16 pc_mask_s = 0x0F;
u16 pc_shift2_s = 0x1001;
u32[16] pc_permidx_s = {0, 4, 8, 12, 1, 5, 9, 13, 2, 6, 10, 14, 3, 7, 11, 15};
fn _poly_compress_1(reg ptr u8[MLKEM_POLYCOMPRESSEDBYTES] rp, reg ptr u16[MLKEM_N] a) -> reg ptr u8[MLKEM_POLYCOMPRESSEDBYTES], reg ptr u16[MLKEM_N]
{
    inline int i;
    reg u512 v shift1 shift2 mask permidx f0 f1 f2 f3;
    reg ptr u16[32] x32p;
    a = _poly_csubq(a);
    x32p = jvx32;
    v = x32p[u512 0];
    shift1 = #VPBROADCAST_32u16(pc_shift1_s);
    mask = #VPBROADCAST_32u16(pc_mask_s);
    shift2 = #VPBROADCAST_32u16(pc_shift2_s);
    permidx = pc_permidx_s[u512 0];
    for i=0 to MLKEM_N/128
    {
        f0 = a[u512 4*i];
        f1 = a[u512 4*i + 1];
        f2 = a[u512 4*i + 2];
        f3 = a[u512 4*i + 3];
        f0 = #VPMULH_32u16(f0, v);
        f1 = #VPMULH_32u16(f1, v);
        f2 = #VPMULH_32u16(f2, v);
        f3 = #VPMULH_32u16(f3, v); 
        f0 = #VPMULHRS_32u16(f0, shift1);
        f1 = #VPMULHRS_32u16(f1, shift1);
        f2 = #VPMULHRS_32u16(f2, shift1);
        f3 = #VPMULHRS_32u16(f3, shift1);
        f0 = #VPAND_512(f0, mask);
        f1 = #VPAND_512(f1, mask);
        f2 = #VPAND_512(f2, mask);
        f3 = #VPAND_512(f3, mask);
        f0 = #VPACKUS_32u16(f0, f1);
        f2 = #VPACKUS_32u16(f2, f3);
        f0 = #VPMADDUBSW_512(f0, shift2);
        f2 = #VPMADDUBSW_512(f2, shift2);
        f0 = #VPACKUS_32u16(f0, f2);
        f0 = #VPERMD_512(permidx, f0);
        rp.[u512 64*i] = f0;
    }
    return rp, a;
}
fn _poly_compress(reg u64 rp, reg ptr u16[MLKEM_N] a) -> reg ptr u16[MLKEM_N]
{
    inline int i;
    reg u512 v shift1 shift2 mask permidx f0 f1 f2 f3;
    reg ptr u16[32] x32p;
    a = _poly_csubq(a);
    x32p = jvx32;
    v = x32p[u512 0];
    shift1 = #VPBROADCAST_32u16(pc_shift1_s);
    mask = #VPBROADCAST_32u16(pc_mask_s);
    shift2 = #VPBROADCAST_32u16(pc_shift2_s);
    permidx = pc_permidx_s[u512 0];
    for i=0 to MLKEM_N/128
    {
        f0 = a[u512 4*i];
        f1 = a[u512 4*i + 1];
        f2 = a[u512 4*i + 2];
        f3 = a[u512 4*i + 3];
        f0 = #VPMULH_32u16(f0, v);
        f1 = #VPMULH_32u16(f1, v);
        f2 = #VPMULH_32u16(f2, v);
        f3 = #VPMULH_32u16(f3, v); 
        f0 = #VPMULHRS_32u16(f0, shift1);
        f1 = #VPMULHRS_32u16(f1, shift1);
        f2 = #VPMULHRS_32u16(f2, shift1);
        f3 = #VPMULHRS_32u16(f3, shift1);
        f0 = #VPAND_512(f0, mask);
        f1 = #VPAND_512(f1, mask);
        f2 = #VPAND_512(f2, mask);
        f3 = #VPAND_512(f3, mask);
        f0 = #VPACKUS_32u16(f0, f1);
        f2 = #VPACKUS_32u16(f2, f3);
        f0 = #VPMADDUBSW_512(f0, shift2);
        f2 = #VPMADDUBSW_512(f2, shift2);
        f0 = #VPACKUS_32u16(f0, f2);
        f0 = #VPERMD_512(permidx, f0);
        (u512)[rp + 64*i] = f0;
    }
    return a;
}

u8[64] pd_jshufbidx = {0,  0,  0,  0,  1,  1,  1,  1,  2,  2,  2,  2,  3,  3,  3,  3,  
4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  
8,  8,  8,  8,  9,  9,  9,  9, 10, 10, 10, 10, 11, 11, 11, 11,  
12, 12, 12, 12, 13, 13, 13, 13, 14, 14, 14, 14, 15, 15, 15, 15};
u32 pd_mask_s = 0x00F0000F;
u32 pd_shift_s = 0x800800;
fn _poly_decompress(reg ptr u16[MLKEM_N] rp, reg u64 ap) -> reg ptr u16[MLKEM_N]
{
  inline int i;
  reg u512 f q shufbidx mask shift;
  reg u128 h;
  reg ptr u16[32] x32p;
  reg ptr u8[64] x64p;
  stack u128 sh;
  x32p = jqx32;
  q = x32p[u512 0];
  x64p = pd_jshufbidx;
  shufbidx = x64p[u512 0];
  mask = #VPBROADCAST_16u32(pd_mask_s);
  shift = #VPBROADCAST_16u32(pd_shift_s);
  f = #set0_512();
  for i = 0 to MLKEM_N/32
  {
    h = (128u)(u128)[ap + 16*i];
    sh = h;
    f = #VBROADCASTI32X4(sh);
    f = #VPERMB_512(shufbidx, f);
    f = #VPAND_512(f, mask);
    f = #VPMULL_32u16(f, shift);
    f = #VPMULHRS_32u16(f, q);
    rp[u512 i] = f;
  }
  return rp;

}

u32[16] pfm_shift_s = {3, 2, 1, 0, 3, 2, 1, 0, 3, 2, 1, 0, 3, 2, 1, 0};
u8[64] pfm_idx_s = {0, 1, 4, 5, 8, 9, 12, 13, 2, 3, 6, 7, 10, 11, 14, 15,
0, 1, 4, 5, 8, 9, 12, 13, 2, 3, 6, 7, 10, 11, 14, 15,
0, 1, 4, 5, 8, 9, 12, 13, 2, 3, 6, 7, 10, 11, 14, 15,
0, 1, 4, 5, 8, 9, 12, 13, 2, 3, 6, 7, 10, 11, 14, 15};
u32[16] pfm_idx32_1_s ={0, 0, 0, 0, 4, 4, 4, 4, 1, 1, 1, 1, 5, 5, 5, 5};
u32[16] pfm_idx32_2_s ={2, 2, 2, 2, 6, 6, 6, 6, 3, 3, 3, 3, 7, 7, 7, 7};
u64[8] pfm_idx_1_s = {0, 1, 8, 9, 4, 5, 12, 13};
u64[8] pfm_idx_2_s = {2, 3, 10, 11, 6, 7, 14, 15};
fn _poly_frommsg(reg ptr u16[MLKEM_N] rp, reg u64 ap) -> reg ptr u16[MLKEM_N]
{
  reg u512 h0_tmp0 h0_tmp1 h2_tmp0 h2_tmp1;
  reg u512 f g0 g1 g2 g3 h0 h1 h2 h3 shift idx8 idx32_1 idx32_2 hqs idx_1 idx_2;
  stack u16 halfq = (MLKEM_Q + 1)/2;

  shift = pfm_shift_s[u512 0];
  idx8 = pfm_idx_s[u512 0];
  idx32_1 = pfm_idx32_1_s[u512 0];
  idx32_2 = pfm_idx32_2_s[u512 0];
  hqs = #VPBROADCAST_32u16(halfq);
  idx_1 = pfm_idx_1_s[u512 0];
  idx_2 = pfm_idx_2_s[u512 0];


  f = #VBROADCASTI64X4((u256)[ap + 0]);

  g3 = #VPERMD_512(idx32_1, f);
  g3 = #VPSLLV_16u32(g3, shift);
  g3 = #VPSHUFB_512(g3, idx8);

  g0 = #VPSLL_32u16(g3, 12);
  g1 = #VPSLL_32u16(g3, 8);
  g2 = #VPSLL_32u16(g3, 4);

  g0 = #VPSRA_32u16(g0, 15);
  g1 = #VPSRA_32u16(g1, 15);
  g2 = #VPSRA_32u16(g2, 15);
  g3 = #VPSRA_32u16(g3, 15);

  g0 = #VPAND_512(g0,hqs);
  g1 = #VPAND_512(g1,hqs);
  g2 = #VPAND_512(g2,hqs);
  g3 = #VPAND_512(g3,hqs);

  h0 = #VPUNPCKL_8u64(g0,g1);
  h2 = #VPUNPCKH_8u64(g0,g1);
  h1 = #VPUNPCKL_8u64(g2,g3);
  h3 = #VPUNPCKH_8u64(g2,g3);
  
  h0_tmp0 = h0;
  h0_tmp1 = h0;
  h2_tmp0 = h2;
  h2_tmp1 = h2;
  h0_tmp0 = #VPERMT2Q_512(h0_tmp0, idx_1, h1);
  h0_tmp1 = #VPERMT2Q_512(h0_tmp1, idx_2, h1);
  h2_tmp0 = #VPERMT2Q_512(h2_tmp0, idx_1, h3);
  h2_tmp1 = #VPERMT2Q_512(h2_tmp1, idx_2, h3);


  h0 = #VSHUFI32X4(h0_tmp0, h2_tmp0, 0x44);
  h1 = #VSHUFI32X4(h0_tmp0, h2_tmp0, 0xEE);
  h2 = #VSHUFI32X4(h0_tmp1, h2_tmp1, 0x44);
  h3 = #VSHUFI32X4(h0_tmp1, h2_tmp1, 0xEE);



  rp[u512 0] = h0;
  rp[u512 1] = h1;
  rp[u512 4] = h2;
  rp[u512 5] = h3;

  g3 = #VPERMD_512(idx32_2, f);
  g3 = #VPSLLV_16u32(g3, shift);
  g3 = #VPSHUFB_512(g3, idx8);

  g0 = #VPSLL_32u16(g3, 12);
  g1 = #VPSLL_32u16(g3, 8);
  g2 = #VPSLL_32u16(g3, 4);

  g0 = #VPSRA_32u16(g0, 15);
  g1 = #VPSRA_32u16(g1, 15);
  g2 = #VPSRA_32u16(g2, 15);
  g3 = #VPSRA_32u16(g3, 15);

  g0 = #VPAND_512(g0,hqs);
  g1 = #VPAND_512(g1,hqs);
  g2 = #VPAND_512(g2,hqs);
  g3 = #VPAND_512(g3,hqs);

  h0 = #VPUNPCKL_8u64(g0,g1);
  h2 = #VPUNPCKH_8u64(g0,g1);
  h1 = #VPUNPCKL_8u64(g2,g3);
  h3 = #VPUNPCKH_8u64(g2,g3);


  h0_tmp0 = h0;
  h0_tmp1 = h0;
  h2_tmp0 = h2;
  h2_tmp1 = h2;
  h0_tmp0 = #VPERMT2Q_512(h0_tmp0, idx_1, h1);
  h0_tmp1 = #VPERMT2Q_512(h0_tmp1, idx_2, h1);
  h2_tmp0 = #VPERMT2Q_512(h2_tmp0, idx_1, h3);
  h2_tmp1 = #VPERMT2Q_512(h2_tmp1, idx_2, h3);

  h0 = #VSHUFI32X4(h0_tmp0, h2_tmp0, 0x44);
  h1 = #VSHUFI32X4(h0_tmp0, h2_tmp0, 0xEE);
  h2 = #VSHUFI32X4(h0_tmp1, h2_tmp1, 0x44);
  h3 = #VSHUFI32X4(h0_tmp1, h2_tmp1, 0xEE);


  rp[u512 2] = h0;
  rp[u512 3] = h1;
  rp[u512 6] = h2;
  rp[u512 7] = h3;
  return rp;

}

// not done yet
u64[8] idx_tmsg = {0, 2, 4, 6, 1, 3, 5, 7};
fn _poly_tomsg(reg u64 rp, reg ptr u16[MLKEM_N] a) -> reg ptr u16[MLKEM_N]
{
  inline int i;
  reg u512 f0 f1 g0 g1 hq hhq idx;
  stack u16 const_hq = (MLKEM_Q - 1)/2;
  stack u16 const_hhq = (MLKEM_Q - 1)/4;
  hq = #VPBROADCAST_32u16(const_hq);
  hhq = #VPBROADCAST_32u16(const_hhq);
  reg u64 s;
  idx = idx_tmsg.[u512 0];
  for i=0 to 4
  {
    f0 = a.[u512 2*i];
    f1 = a.[u512 2*i + 1];
    f0 = #VPSUB_32u16(hq, f0);
    f1 = #VPSUB_32u16(hq, f1);
    g0 = #VPSRA_32u16(f0, 15);
    g1 = #VPSRA_32u16(f1, 15);
    f0 = #VPXOR_512(f0, g0);
    f1 = #VPXOR_512(f1, g1);
    f0 = #VPSUB_32u16(f0, hhq);
    f1 = #VPSUB_32u16(f1, hhq);
    f0 = #VPACKSS_32u16(f0, f1);
    f0 = #VPERMQ512(idx, f0);
    //  s = _mm512_movepi8_mask(f0);
    (u64)[rp + 8*i] = s;
  }
  return a;

}
// not done yet
fn _poly_tomsg_1(reg ptr u8[MLKEM_INDCPA_MSGBYTES] rp, reg ptr u16[MLKEM_N] a) -> reg ptr u8[MLKEM_INDCPA_MSGBYTES], reg ptr u16[MLKEM_N]
{
  inline int i;
  reg u512 f0 f1 g0 g1 hq hhq idx;
  stack u16 const_hq = (MLKEM_Q - 1)/2;
  stack u16 const_hhq = (MLKEM_Q - 1)/4;
  hq = #VPBROADCAST_32u16(const_hq);
  hhq = #VPBROADCAST_32u16(const_hhq);
  reg u64 s;
  idx = idx_tmsg.[u512 0];
  for i=0 to 4
  {
    f0 = a.[u512 2*i];
    f1 = a.[u512 2*i + 1];
    f0 = #VPSUB_32u16(hq, f0);
    f1 = #VPSUB_32u16(hq, f1);
    g0 = #VPSRA_32u16(f0, 15);
    g1 = #VPSRA_32u16(f1, 15);
    f0 = #VPXOR_512(f0, g0);
    f1 = #VPXOR_512(f1, g1);
    f0 = #VPSUB_32u16(f0, hhq);
    f1 = #VPSUB_32u16(f1, hhq);
    f0 = #VPACKSS_32u16(f0, f1);
    f0 = #VPERMQ512(idx, f0);
    //  s = _mm512_movepi8_mask(f0);
    rp[u64 i] = s;
  }
  return rp, a;

}
u64[8] idx_cbd2 = {0, 1, 4, 5, 2, 3, 6, 7};
inline fn __cbd2(reg ptr u16[MLKEM_N] rp, reg ptr u8[MLKEM_ETA2*MLKEM_N/4] buf) -> reg ptr u16[MLKEM_N]
{
  inline int i;
  reg u512 f0 f1 f2 f3;
  reg u512 mask55 mask33 mask03 mask0F;
  reg u256 t;
  reg u512 idx;
  stack u32 const_mask55 = 0x55555555;
  stack u32 const_mask33 = 0x33333333;
  stack u32 const_mask0F = 0x0F0F0F0F;
  stack u32 const_mask03 = 0x03030303;
  idx = idx_cbd2[u512 0];
  mask55 = #VPBROADCAST_16u32(const_mask55);
  mask33 = #VPBROADCAST_16u32(const_mask33);
  mask0F = #VPBROADCAST_16u32(const_mask0F);
  mask03 = #VPBROADCAST_16u32(const_mask03);
  () = #spill(mask33);
  () = #spill(mask0F);
  () = #spill(mask55);
  () = #spill(mask03);
  for i = 0 to 2
  {
    f0 = buf[u512 i];

    f1 = #VPSRL_32u16(f0, 1);
    () = #unspill(mask55);
    f0 = #VPAND_512(mask55, f0);
    f1 = #VPAND_512(mask55, f1);
    
    f0 = #VPADD_64u8(f0, f1);

    f1 = #VPSRL_32u16(f0, 2);
    () = #unspill(mask33);
    f0 = #VPAND_512(mask33, f0);
    f1 = #VPAND_512(mask33, f1);
    f0 = #VPADD_64u8(f0, mask33);
    f0 = #VPSUB_64u8(f0, f1);

    f1 = #VPSRL_32u16(f0, 4);
    () = #unspill(mask0F);
    f0 = #VPAND_512(mask0F, f0);
    f1 = #VPAND_512(mask0F, f1);
    () = #unspill(mask03);
    f0 = #VPSUB_64u8(f0, mask03);
    f1 = #VPSUB_64u8(f1, mask03);

    f2 = #VPUNPCKL_64u8(f0, f1);
    f3 = #VPUNPCKL_64u8(f0, f1);

    f0 = #VSHUFI32X4(f2,f3,0x44);
    f1 = #VSHUFI32X4(f2,f3,0xEE);
    f2 = #VPERMQ512(idx, f0);
    f3 = #VPERMQ512(idx, f1);

    t = (256u)f2;
    f0 = #VPMOVSX_32u8_32u16(t);
    t = #VEXTRACTI32X8(f2, 1);
    f1 = #VPMOVSX_32u8_32u16(t);
    t = (256u)f3;
    f2 = #VPMOVSX_32u8_32u16(t);
    t = #VEXTRACTI32X8(f3, 1);
    f3 = #VPMOVSX_32u8_32u16(t);

    rp[u512 4*i] = f0;
    rp[u512 4*i + 1] = f1;
    rp[u512 4*i + 2] = f2;
    rp[u512 4*i + 3] = f3;
  }
  return rp;
}

inline
fn __poly_cbd_eta1(reg ptr u16[MLKEM_N] rp, reg ptr u8[MLKEM_ETA1*MLKEM_N/4] buf) -> reg ptr u16[MLKEM_N]
{
  rp = __cbd2(rp, buf[0:MLKEM_ETA1*MLKEM_N/4]);
  return rp;
}
fn _poly_getnoise_eta1_8x(reg ptr u16[MLKEM_N] r0 r1 r2 r3 r4 r5 r6 r7, reg ptr u8[MLKEM_SYMBYTES] seed, reg u8 nonce) -> reg ptr u16[MLKEM_N], reg ptr u16[MLKEM_N], reg ptr u16[MLKEM_N], reg ptr u16[MLKEM_N], reg ptr u16[MLKEM_N], reg ptr u16[MLKEM_N], reg ptr u16[MLKEM_N], reg ptr u16[MLKEM_N]
{
  reg u256 f;
  stack u512[25] state;
  stack u8[NOISE_NBLOCKS * SHAKE256_RATE] buf0 buf1 buf2 buf3 buf4 buf5 buf6 buf7;



  f = seed.[u256 0];
  buf0[u256 0] = f;
  buf1[u256 0] = f;
  buf2[u256 0] = f;
  buf3[u256 0] = f;
  buf4[u256 0] = f;
  buf5[u256 0] = f;
  buf6[u256 0] = f;
  buf7[u256 0] = f;

  buf0.[32] = nonce;
  nonce += 1;
  buf1.[32] = nonce;
  nonce += 1;
  buf2.[32] = nonce;
  nonce += 1;
  buf3.[32] = nonce;

  buf4.[32] = nonce;
  nonce += 1;
  buf5.[32] = nonce;
  nonce += 1;
  buf6.[32] = nonce;
  nonce += 1;
  buf7.[32] = nonce;

  () = #spill(r0, r1, r2, r3, r4, r5, r6, r7);
  state = _shake256_absorb8x_33(state, buf0[0:33], buf1[0:33], buf2[0:33], buf3[0:33], buf4[0:33], buf5[0:33], buf6[0:33], buf7[0:33]);
  state, buf0, buf1, buf2, buf3, buf4, buf5, buf6, buf7 = __shake256_squeezenblocks8x(state, buf0, buf1, buf2, buf3, buf4, buf5, buf6, buf7);
  () = #unspill(r0, r1, r2, r3, r4, r5, r6, r7);

  r0 = __poly_cbd_eta1(r0, buf0[0:MLKEM_ETA1*MLKEM_N/4+(MLKEM_ETA1 - 2)*8]);
  r1 = __poly_cbd_eta1(r1, buf1[0:MLKEM_ETA1*MLKEM_N/4+(MLKEM_ETA1 - 2)*8]);
  r2 = __poly_cbd_eta1(r2, buf2[0:MLKEM_ETA1*MLKEM_N/4+(MLKEM_ETA1 - 2)*8]);
  r3 = __poly_cbd_eta1(r3, buf3[0:MLKEM_ETA1*MLKEM_N/4+(MLKEM_ETA1 - 2)*8]);
  
  r4 = __poly_cbd_eta1(r4, buf4[0:MLKEM_ETA1*MLKEM_N/4+(MLKEM_ETA1 - 2)*8]);
  r5 = __poly_cbd_eta1(r5, buf5[0:MLKEM_ETA1*MLKEM_N/4+(MLKEM_ETA1 - 2)*8]);
  r6 = __poly_cbd_eta1(r6, buf6[0:MLKEM_ETA1*MLKEM_N/4+(MLKEM_ETA1 - 2)*8]);
  r7 = __poly_cbd_eta1(r7, buf7[0:MLKEM_ETA1*MLKEM_N/4+(MLKEM_ETA1 - 2)*8]);
  return r0, r1, r2, r3, r4, r5, r6, r7;

}

fn _poly_sub(reg ptr u16[MLKEM_N] rp bp)  -> reg ptr u16[MLKEM_N]
{
    inline int i;
    reg u512 a;
    reg u512 b;
    reg u512 r;
    for i = 0 to 8 {
        a = rp.[u512 64*i];
        b = bp.[u512 64*i];
        r = #VPSUB_32u16(a, b);
        rp.[u512 64*i] = r;
    }
    return rp;
}
fn _poly_add(reg ptr u16[MLKEM_N] rp bp) -> reg ptr u16[MLKEM_N]
{
    inline int i;
    reg u512 a;
    reg u512 b;
    reg u512 r;
    for i = 0 to 8 {
        a = rp.[u512 64*i];
        b = bp.[u512 64*i];
        r = #VPADD_32u16(a, b);
        rp.[u512 64*i] = r;
    }
    return rp;
}

inline fn __karatsuba32x(reg u512 al ah bl bh const_q const_qinv zeta) -> reg u512, reg u512
{
    reg u512 f0 f1 f2 f3 f4 f5 f6;
    f0 = #VPADD_32u16(al, ah); //zmm19
    f1 = #VPADD_32u16(bl, bh); //zmm20
    f2 = #VPMULH_32u16(f0, f1); //zmm21
    f3 = #VPMULH_32u16(al, bl); //zmm22
    f4 = #VPMULH_32u16(ah, bh); //zmm23

    f5 = #VPMULL_32u16(al, const_qinv);
    f6 = #VPMULL_32u16(ah, const_qinv);
    f0 = #VPMULL_32u16(f0, const_qinv);

    f5 = #VPMULL_32u16(bl, f5);
    f6 = #VPMULL_32u16(bh, f6);
    f0 = #VPMULL_32u16(f1, f0);

    f5 = #VPMULH_32u16(f5, const_q);
    f6 = #VPMULH_32u16(f6, const_q);
    f0 = #VPMULH_32u16(f0, const_q);

    f5 = #VPSUB_32u16(f3, f5);
    f6 = #VPSUB_32u16(f4, f6);
    f0 = #VPSUB_32u16(f2, f0);

    f1 = #VPSUB_32u16(f0, f5);
    f1 = #VPSUB_32u16(f1, f6);

    f2 = #VPMULH_32u16(f6, zeta);
    f6 = #VPMULL_32u16(f6, const_qinv);
    f6 = #VPMULL_32u16(f6, zeta);
    f6 = #VPMULH_32u16(f6, const_q);
    f6 = #VPSUB_32u16(f2, f6);

    f2 = #VPADD_32u16(f5, f6);

    return f2, f1;

}
fn _poly_basemul(reg ptr u16[MLKEM_N] rp ap bp) -> reg ptr u16[MLKEM_N]
{
  reg u512 const_q const_qinv;
  const_q = jqx32[u512 0];
  const_qinv = jqinvx32[u512 0];
  reg u512 f0 f1 f2 f3 f4 f5 f6 f7 g0 g1 g2 g3 g4 g5 g6 g7 zeta;
  reg u512 h0 h1;

  f0 = ap.[u512 0*64];
  f1 = ap.[u512 1*64];
  f2 = ap.[u512 2*64];
  f3 = ap.[u512 3*64];
  f4 = ap.[u512 4*64];
  f5 = ap.[u512 5*64];
  f6 = ap.[u512 6*64];
  f7 = ap.[u512 7*64];

  g0 = bp.[u512 0*64];
  g1 = bp.[u512 1*64];
  g2 = bp.[u512 2*64];
  g3 = bp.[u512 3*64];
  g4 = bp.[u512 4*64];
  g5 = bp.[u512 5*64];
  g6 = bp.[u512 6*64];
  g7 = bp.[u512 7*64];

  zeta = jzetasmul_exp.[u512 0];
  h0, h1 = __karatsuba32x(f0, f1, g0, g1, const_q, const_qinv, zeta);
  rp.[u512 0*64] = h0;
  rp.[u512 1*64] = h1;

  zeta = jzetasmul_exp.[u512 1*64];
  h0, h1 = __karatsuba32x(f2, f3, g2, g3, const_q, const_qinv, zeta);
  rp.[u512 2*64] = h0;
  rp.[u512 3*64] = h1;

  zeta = jzetasmul_exp.[u512 2*64];
  h0, h1 = __karatsuba32x(f4, f5, g4, g5, const_q, const_qinv, zeta);
  rp.[u512 4*64] = h0;
  rp.[u512 5*64] = h1;

  zeta = jzetasmul_exp.[u512 3*64];
  h0, h1 = __karatsuba32x(f6, f7, g6, g7, const_q, const_qinv, zeta);
  rp.[u512 6*64] = h0;
  rp.[u512 7*64] = h1;

  return rp;

}

inline fn __butterfly(reg u512 l r zl zh const_q) -> reg u512, reg u512
{
  reg u512 tmp;
  tmp = #VPMULL_32u16(zl, r);
  r = #VPMULH_32u16(zh, r);

  tmp = #VPMULH_32u16(const_q, tmp);
  tmp = #VPSUB_32u16(r, tmp);

  r = #VPSUB_32u16(l, 11);
  l = #VPADD_32u16(l, 11);
  return r, l;

}
fn _poly_ntt(reg ptr u16[MLKEM_N] rp) -> reg ptr u16[MLKEM_N]
{
  reg u512 qx32 r0 r1 r2 r3 r4 r5 r6 r7 zeta zetaqinv;
  reg u512 r8 r12 r13 r14 r15 r16 r17 r18 r19;
  reg ptr u16[1038] zetasp;
  
  zetasp = jzetas_exp;
  qx32 = jqx32[u512 0];

  r0 = rp.[u512 64*0];
  r1 = rp.[u512 64*1];
  r2 = rp.[u512 64*2];
  r3 = rp.[u512 64*3];
  r4 = rp.[u512 64*8];
  r5 = rp.[u512 64*9];
  r6 = rp.[u512 64*10];
  r7 = rp.[u512 64*11];
  //level0
  zeta = #VPBROADCAST_32u16(zetasp[u16 0]);
  zetaqinv = #VPBROADCAST_32u16(zetasp[u16 1]);

  
  r0, r4 = __butterfly(r0, r4, zeta, zetaqinv, qx32);
  r1, r5 = __butterfly(r1, r5, zeta, zetaqinv, qx32);
  r2, r6 = __butterfly(r2, r6, zeta, zetaqinv, qx32);
  r3, r7 = __butterfly(r3, r7, zeta, zetaqinv, qx32);
  //level1
  zeta = #VPBROADCAST_32u16(zetasp[u16 2]);
  zetaqinv = #VPBROADCAST_32u16(zetasp[u16 3]);
  
  r0, r2 = __butterfly(r0, r2, zeta, zetaqinv, qx32);
  r1, r3 =__butterfly(r1, r3, zeta, zetaqinv, qx32);

  zeta = #VPBROADCAST_32u16(zetasp[u16 4]);
  zetaqinv = #VPBROADCAST_32u16(zetasp[u16 5]);

  r4, r6 = __butterfly(r4, r6, zeta, zetaqinv, qx32);
  r5, r7 = __butterfly(r5, r7, zeta, zetaqinv, qx32);

  //level 2
  zeta = #VPBROADCAST_32u16(zetasp[u16 6]);
  zetaqinv = #VPBROADCAST_32u16(zetasp[u16 7]);
  r0, r1 = __butterfly(r0, r1, zeta, zetaqinv, qx32);
  zeta = #VPBROADCAST_32u16(zetasp[u16 8]);
  zetaqinv = #VPBROADCAST_32u16(zetasp[u16 9]);
  r2, r3 = __butterfly(r2, r3, zeta, zetaqinv, qx32);
  zeta = #VPBROADCAST_32u16(zetasp[u16 10]);
  zetaqinv = #VPBROADCAST_32u16(zetasp[u16 11]);
  r4, r5 = __butterfly(r4, r5, zeta, zetaqinv, qx32);
  zeta = #VPBROADCAST_32u16(zetasp[u16 12]);
  zetaqinv = #VPBROADCAST_32u16(zetasp[u16 13]);
  r6, r7 = __butterfly(r6, r7, zeta, zetaqinv, qx32);

  //level 3
  r12, r13 = __shuffle16(r0, r1);
  zeta = zetasp.[u512 14*64];
  zetaqinv = zetasp.[u512 46*64];
  r12, r13 = __butterfly(r12, r13, zeta, zetaqinv, qx32);

  r14, r15 = __shuffle16(r2, r3);
  zeta = zetasp.[u512 78*64];
  zetaqinv = zetasp.[u512 110*64];
  r14, r15 = __butterfly(r14, r15, zeta, zetaqinv, qx32);

  r16, r17 = __shuffle16(r4, r5);
  zeta = zetasp.[u512 142*64];
  zetaqinv = zetasp.[u512 174*64];
  r16, r17 = __butterfly(r16, r17, zeta, zetaqinv, qx32);

  r18, r19 = __shuffle16(r6, r7);
  zeta = zetasp.[u512 206*64];
  zetaqinv = zetasp.[u512 238*64];
  r18, r19 = __butterfly(r18, r19, zeta, zetaqinv, qx32);
  
  // level 4
  r1, r2 = __shuffle16(r12, r13);
  zeta = zetasp.[u512 270*64];
  zetaqinv = zetasp.[u512 302*64];
  r1, r2 = __butterfly(r1, r2, zeta, zetaqinv, qx32);

  r3, r4 = __shuffle16(r14, r15);
  zeta = zetasp.[u512 334*64];
  zetaqinv = zetasp.[u512 366*64];
  r3, r4 = __butterfly(r3, r4, zeta, zetaqinv, qx32);

  r5, r6 = __shuffle16(r16, r17);
  zeta = zetasp.[u512 398*64];
  zetaqinv = zetasp.[u512 430*64];
  r5, r6 = __butterfly(r5, r6, zeta, zetaqinv, qx32);

  r7, r8 = __shuffle16(r18, r19);
  zeta = zetasp.[u512 462*64];
  zetaqinv = zetasp.[u512 494*64];
  r7, r8 = __butterfly(r7, r8, zeta, zetaqinv, qx32);

  //level 5
  r12, r13 = __shuffle4(r1, r2);
  zeta = zetasp.[u512 526*64];
  zetaqinv = zetasp.[u512 558*64];
  r12, r13 = __butterfly(r12, r13, zeta, zetaqinv, qx32);

  r14, r15 = __shuffle4(r3, r4);
  zeta = zetasp.[u512 590*64];
  zetaqinv = zetasp.[u512 622*64];
  r14, r15 = __butterfly(r14, r15, zeta, zetaqinv, qx32);

  r16, r17 = __shuffle4(r5, r6);
  zeta = zetasp.[u512 654*64];
  zetaqinv = zetasp.[u512 686*64];
  r16, r17 = __butterfly(r16, r17, zeta, zetaqinv, qx32);

  r18, r19 = __shuffle4(r7, r8);
  zeta = zetasp.[u512 718*64];
  zetaqinv = zetasp.[u512 750*64];
  r18, r19 =__butterfly(r18, r19, zeta, zetaqinv, qx32);

  //level 6
  r1, r2 = __shuffle2(r12, r13);
  zeta = zetasp.[u512 782*64];
  zetaqinv = zetasp.[u512 814*64];
  r1, r2 = __butterfly(r1, r2, zeta, zetaqinv, qx32);

  r3, r4 = __shuffle2(r14, r15);
  zeta = zetasp.[u512 846*64];
  zetaqinv = zetasp.[u512 878*64];
  r3, r4 = __butterfly(r3, r4, zeta, zetaqinv, qx32);

  r5, r6 = __shuffle2(r16, r17);
  zeta = zetasp.[u512 910*64];
  zetaqinv = zetasp.[u512 942*64];
  r5, r6 = __butterfly(r5, r6, zeta, zetaqinv, qx32);

  r7, r8 = __shuffle2(r18, r19);
  zeta = zetasp.[u512 974*64];
  zetaqinv = zetasp.[u512 1006*64];
  r7, r8 = __butterfly(r7, r8, zeta, zetaqinv, qx32);

  r12, r13 = __shuffle1(r1, r2);
  r14, r15 = __shuffle1(r3, r4);
  r16, r17 = __shuffle1(r5, r6);
  r18, r19 = __shuffle1(r7, r8);

  rp.[u512 64*0] = r12;
  rp.[u512 64*1] = r13;
  rp.[u512 64*2] = r14;
  rp.[u512 64*3] = r15;
  rp.[u512 64*4] = r16;
  rp.[u512 64*5] = r17;
  rp.[u512 64*6] = r18;
  rp.[u512 64*7] = r19;
  return rp;
}
inline fn __GSbutterfly(reg u512 l r zl zh const_q) -> reg u512, reg u512
{
  reg u512 tmp tmp2;
  tmp = #VPSUB_32u16(r, l);
  l = #VPADD_32u16(l, r);
  tmp2 = #VPMULH_32u16(zh, tmp);
  tmp = #VPMULL_32u16(zl, tmp);
  tmp = #VPMULH_32u16(tmp, const_q);
  r =#VPSUB_32u16(tmp2, tmp);
  return l, r;

}

fn _poly_invntt(reg ptr u16[MLKEM_N] rp) -> reg ptr u16[MLKEM_N]
{
  reg u512 r1 r2 r3 r4 r5 r6 r7 r8 r9 r10 r11 r12 r13 r14 r15 r16 qx32 zeta zetaqinv vx32;
  reg ptr u16[1038] zetasp;
  zetasp = jzetas_inv_exp;
  qx32 = jqx32[u512 0];
  vx32 = jvx32[u512 0];

  r1 = rp.[u512 64*0];
  r2 = rp.[u512 64*1];
  r3 = rp.[u512 64*2];
  r4 = rp.[u512 64*3];
  r5 = rp.[u512 64*4];
  r6 = rp.[u512 64*5];
  r7 = rp.[u512 64*6];
  r8 = rp.[u512 64*7];

  zeta = #VPBROADCAST_32u16(jzetasmul_exp[u16 0]);
  zetaqinv = #VPBROADCAST_32u16(jzetasmul_exp[u16 1]);

  r1 = __montmul(zeta, zetaqinv, qx32, r1);
  r2 = __montmul(zeta, zetaqinv, qx32, r2);
  r3 = __montmul(zeta, zetaqinv, qx32, r3);
  r4 = __montmul(zeta, zetaqinv, qx32, r4);
  r5 = __montmul(zeta, zetaqinv, qx32, r5);
  r6 = __montmul(zeta, zetaqinv, qx32, r6);
  r7 = __montmul(zeta, zetaqinv, qx32, r7);
  r8 = __montmul(zeta, zetaqinv, qx32, r8);
  // level 0
  r9, r10 = __shuffle1(r1, r2);
  r11, r12 = __shuffle1(r3, r4);
  r13, r14 = __shuffle1(r5, r6);
  r15, r16 = __shuffle1(r7, r8);

  zeta = zetasp.[u512 64*0];
  zetaqinv = zetasp.[u512 64*1];
  r9, r10 = __GSbutterfly(r9, r10, zeta, zetaqinv, qx32);
  zeta = zetasp.[u512 64*2];
  zetaqinv = zetasp.[u512 64*3];
  r11, r12 = __GSbutterfly(r11, r12, zeta, zetaqinv, qx32);
  zeta = zetasp.[u512 64*4];
  zetaqinv = zetasp.[u512 64*5];
  r13, r14 = __GSbutterfly(r13, r14, zeta, zetaqinv, qx32);
  zeta = zetasp.[u512 64*6];
  zetaqinv = zetasp.[u512 64*7];
  r15, r16 = __GSbutterfly(r15, r16, zeta, zetaqinv, qx32);
  // level 1
  r1, r2 = __shuffle2(r9, r10);
  r3, r4 = __shuffle2(r11, r12);
  r5, r6 = __shuffle2(r13, r14);
  r7, r8 = __shuffle2(r15, r16);

  zeta = zetasp.[u512 64*8];
  zetaqinv = zetasp.[u512 64*9];
  r1, r2 = __GSbutterfly(r1, r2, zeta, zetaqinv, qx32);
  zeta = zetasp.[u512 64*10];
  zetaqinv = zetasp.[u512 64*11];
  r3, r4 = __GSbutterfly(r3, r4, zeta, zetaqinv, qx32);
  zeta = zetasp.[u512 64*12];
  zetaqinv = zetasp.[u512 64*13];
  r5, r6 = __GSbutterfly(r5, r6, zeta, zetaqinv, qx32);
  zeta = zetasp.[u512 64*14];
  zetaqinv = zetasp.[u512 64*15];
  r7, r8 = __GSbutterfly(r7, r8, zeta, zetaqinv, qx32);

  //level 2
  r9, r10 = __shuffle4(r1, r2);
  r11, r12 = __shuffle4(r3, r4);
  r13, r14 = __shuffle4(r5, r6);
  r15, r16 = __shuffle4(r7, r8);
  
  zeta = zetasp.[u512 64*16];
  zetaqinv = zetasp.[u512 64*17];
  r9, r10 = __GSbutterfly(r9, r10, zeta, zetaqinv, qx32);
  zeta = zetasp.[u512 64*18];
  zetaqinv = zetasp.[u512 64*19];
  r11, r12 = __GSbutterfly(r11, r12, zeta, zetaqinv, qx32);
  zeta = zetasp.[u512 64*20];
  zetaqinv = zetasp.[u512 64*21];
  r13, r14 = __GSbutterfly(r13, r14, zeta, zetaqinv, qx32);
  zeta = zetasp.[u512 64*22];
  zetaqinv = zetasp.[u512 64*23];
  r15, r16 = __GSbutterfly(r15, r16, zeta, zetaqinv, qx32);

  r9 = __red16(r9, vx32, qx32);
  r11 = __red16(r11, vx32, qx32);
  r13 = __red16(r13, vx32, qx32);
  r15 = __red16(r15, vx32, qx32);

  //level 3

  r1, r2 = __shuffle8(r9, r10);
  r3, r4 = __shuffle8(r11, r12);
  r5, r6 = __shuffle8(r13, r14);
  r7, r8 = __shuffle8(r15, r16);

  zeta = zetasp.[u512 64*24];
  zetaqinv = zetasp.[u512 64*25];
  r1, r2 = __GSbutterfly(r1, r2, zeta, zetaqinv, qx32);
  zeta = zetasp.[u512 64*26];
  zetaqinv = zetasp.[u512 64*27];
  r3, r4 = __GSbutterfly(r3, r4, zeta, zetaqinv, qx32);
  zeta = zetasp.[u512 64*28];
  zetaqinv = zetasp.[u512 64*29];
  r5, r6 = __GSbutterfly(r5, r6, zeta, zetaqinv, qx32);
  zeta = zetasp.[u512 64*30];
  zetaqinv = zetasp.[u512 64*31];
  r7, r8 = __GSbutterfly(r7, r8, zeta, zetaqinv, qx32);
  //level 4
  r9, r10 = __shuffle16(r1, r2);
  r11, r12 = __shuffle16(r3, r4);
  r13, r14 = __shuffle16(r5, r6);
  r15, r16 = __shuffle16(r7, r8);

  zeta = #VPBROADCAST_32u16(zetasp[u16 1024]);
  zetaqinv = #VPBROADCAST_32u16(zetasp[u16 1025]);
  r9, r10 = __GSbutterfly(r9, r10, zeta, zetaqinv, qx32);
  zeta = #VPBROADCAST_32u16(zetasp[u16 1026]);
  zetaqinv = #VPBROADCAST_32u16(zetasp[u16 1027]);
  r11, r12 = __GSbutterfly(r11, r12, zeta, zetaqinv, qx32);
  zeta = #VPBROADCAST_32u16(zetasp[u16 1028]);
  zetaqinv = #VPBROADCAST_32u16(zetasp[u16 1029]);
  r13, r14 = __GSbutterfly(r13, r14, zeta, zetaqinv, qx32);
  zeta = #VPBROADCAST_32u16(zetasp[u16 1030]);
  zetaqinv = #VPBROADCAST_32u16(zetasp[u16 1031]);
  r15, r16 = __GSbutterfly(r15, r16, zeta, zetaqinv, qx32);

  r9 = __red16(r9, vx32, qx32);
  r11 = __red16(r11, vx32, qx32);
  r13 = __red16(r13, vx32, qx32);
  r15 = __red16(r15, vx32, qx32);

  //level 5
  zeta = #VPBROADCAST_32u16(zetasp[u16 1032]);
  zetaqinv = #VPBROADCAST_32u16(zetasp[u16 1033]);
  r9, r11 = __GSbutterfly(r9, r11, zeta, zetaqinv, qx32);
  r10, r12 = __GSbutterfly(r10, r12, zeta, zetaqinv, qx32);

  zeta = #VPBROADCAST_32u16(zetasp[u16 1034]);
  zetaqinv = #VPBROADCAST_32u16(zetasp[u16 1035]);
  r13, r15 = __GSbutterfly(r13, r15, zeta, zetaqinv, qx32);
  r14, r16 = __GSbutterfly(r14, r16, zeta, zetaqinv, qx32);

  //level 6
  zeta = #VPBROADCAST_32u16(zetasp[u16 1036]);
  zetaqinv = #VPBROADCAST_32u16(zetasp[u16 1037]);
  r9, r13 = __GSbutterfly(r9, r13, zeta, zetaqinv, qx32);
  r10, r14 = __GSbutterfly(r10, r14, zeta, zetaqinv, qx32);
  r11, r15 = __GSbutterfly(r11, r15, zeta, zetaqinv, qx32);
  r12, r16 = __GSbutterfly(r12, r16, zeta, zetaqinv, qx32);

  r9 = __red16(r9, vx32, qx32);

  rp.[u512 0*64] = r9;
  rp.[u512 1*64] = r10;
  rp.[u512 2*64] = r11;
  rp.[u512 3*64] = r12;
  rp.[u512 4*64] = r13;
  rp.[u512 5*64] = r14;
  rp.[u512 6*64] = r15;
  rp.[u512 7*64] = r16;
  
  return rp;
}

u16[32] mask_fbts = {0x0FF, 0x0FF, 0x0FF, 0x0FF, 0x0FF, 0x0FF, 0x0FF, 0x0FF, 0x0FF, 0x0FF, 0x0FF, 0x0FF, 0x0FF, 0x0FF, 0x0FF, 0x0FF,0x0FF, 0x0FF, 0x0FF, 0x0FF, 0x0FF, 0x0FF, 0x0FF, 0x0FF, 0x0FF, 0x0FF, 0x0FF, 0x0FF, 0x0FF, 0x0FF, 0x0FF, 0x0FF};
u8[64] idx1_fbts = {0,  1,  3,  4,  6,  7,  9, 10, 12, 13, 15, 16, 18, 19, 21, 22,
24, 25, 27, 28, 30, 31, 33, 34, 36, 37, 39, 40, 42, 43, 45, 46,
-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
};
u8[64] idx2_fbts = {1,  2,  4,  5,  7,  8, 10, 11, 13, 14, 16, 17, 19, 20, 22, 23,
25, 26, 28, 29, 31, 32, 34, 35, 37, 38, 40, 41, 43, 44, 46, 47,
-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
};
fn _poly_frombytes(reg ptr u16[MLKEM_N] rp, reg u64 ap) -> reg ptr u16[MLKEM_N]
{
  inline int i;
  reg u512 f0 f1 f2 f3 f4 f5;
  reg u256 tmp;
  // const __mmask64 mmask1 = _cvtu64_mask64(0x00000000FFFFFFFF);
  // const __mmask64 mmask2 = _cvtu64_mask64(0x0000FFFFFFFFFFFF);
  reg u512 mask = mask_fbts[u512 0];
  reg u512 idx1 idx2;
  idx1 = idx1_fbts[u512 0];
  idx2 = idx2_fbts[u512 0];

  for i = 0 to 2
  {
    f0 = (u512)[ap + i*128];
    f1 = (u512)[ap + i*128 + 64];
    // need mask registers
    // f2 = _mm512_maskz_permutexvar_epi8(mmask1, idx1, f0);
    // f3 = _mm512_maskz_permutexvar_epi8(mmask1, idx2, f0);
    // f4 = _mm512_maskz_permutexvar_epi8(mmask1, idx1, f1);
    // f5 = _mm512_maskz_permutexvar_epi8(mmask1, idx2, f1);

    tmp = #VEXTRACTI64X4(f4, 0);
    f0 = #VINSERTI64X4(f2, tmp, 1);
    tmp = #VEXTRACTI64X4(f5, 0);
    f1 = #VINSERTI64X4(f3, tmp, 1);
    f0 = #VPAND_512(f0, mask);
    f1 = #VPSRL_32u16(f1, 4);

    rp[u512 2*i + 0] = f0;
    rp[u512 2*i + 1] = f1;


  }
  // f0 = _mm512_mask_loadu_epi8(f0,mmask2,&a[288]);
  // f1 = _mm512_mask_loadu_epi8(f1,mmask2,&a[336]);
  // f2 = _mm512_maskz_permutexvar_epi8(mmask1, idx1, f0);
  // f3 = _mm512_maskz_permutexvar_epi8(mmask1, idx2, f0);
  // f4 = _mm512_maskz_permutexvar_epi8(mmask1, idx1, f1);
  // f5 = _mm512_maskz_permutexvar_epi8(mmask1, idx2, f1);
  tmp = #VEXTRACTI64X4(f4, 0);
  f0 = #VINSERTI64X4(f2, tmp, 1);
  tmp = #VEXTRACTI64X4(f5, 0);
  f1 = #VINSERTI64X4(f3, tmp, 1);
  f0 = #VPAND_512(f0, mask);
  f1 = #VPSRL_32u16(f1, 4);
  rp[u512 6] = f0;
  rp[u512 7] = f1;
  return rp;
}

u8[64] idx1_tbts = { 0, 1,-1, 2, 3,-1, 4, 5,-1, 6, 7,-1, 8, 9,-1,10,11,-1,12,13,-1,14,15,-1,16,17,-1,18,19,-1,20,21,-1,22,23,-1,24,25,-1,26,27,-1,28,29,-1,30,31,-1,
-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1};
u8[64] idx2_tbts = {-1,-1, 0,-1,-1, 2,-1,-1, 4,-1,-1, 6,-1,-1, 8,-1,-1,10,-1,-1,12,-1,-1,14,-1,-1,16,-1,-1,18,-1,-1,20,-1,-1,22,-1,-1,24,-1,-1,26,-1,-1,28,-1,-1,30,
-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1};
fn _poly_tobytes(reg u64 rp, reg ptr u16[MLKEM_N] a) -> reg ptr u16[MLKEM_N]
{
  inline int i;
  reg u512 f0 f1 f2 f3 f4 f5;
  reg u512 const_q = jqx32[u512 0];
  reg u512 idx1 idx2;
  idx1 = idx1_tbts[u512 0];
  idx2 = idx2_tbts[u512 0];
  reg u256 g;
  f4 = #set0_512();
  f5 = #set0_512();
  // const __mmask64 mmask_s = _cvtu64_mask64(0x0000FFFFFFFFFFFF);
  // const __mmask64 mmask1 = _cvtu64_mask64(0x00006DB6DB6DB6DB);
  // const __mmask64 mmask2 = _cvtu64_mask64(0x0000924924924924);

  for i= 0 to 2
  {
    f0 = a[u512 2*i + 0];
    f1 = a[u512 2*i + 1];
    f0 = #VPSUB_32u16(f0, const_q);
    f1 = #VPSUB_32u16(f1, const_q);
    f2 = #VPSRA_32u16(f0, 15);
    f3 = #VPSRA_32u16(f1, 15);
    f2 = #VPAND_512(f2, const_q);
    f3 = #VPAND_512(f3, const_q);
    f0 = #VPADD_32u16(f0, f2);
    f1 = #VPADD_32u16(f1, f3);

    f2 = #VPSLL_32u16(f1, 12);
    f2 = #VPOR_512(f0, f2);
    f3 = #VPSRL_32u16(f1, 4);
    g = #VEXTRACTI64X4(f2, 1);
    f4 = #VINSERTI64X4(f4, g, 0);
    g = #VEXTRACTI64X4(f3, 1);
    f5 = #VINSERTI64X4(f5, g, 0);
    // f0 = _mm512_maskz_permutexvar_epi8(mmask1, idx1, f2);
    // f1 = _mm512_maskz_permutexvar_epi8(mmask2, idx2, f3);
    f0 = #VPOR_512(f0, f1);
    // f2 = _mm512_maskz_permutexvar_epi8(mmask1, idx1, f4);
    // f3 = _mm512_maskz_permutexvar_epi8(mmask2, idx2, f5);
    f2 = #VPOR_512(f2, f3);

    (u512)[rp + 96*i] = f0;
    (u512)[rp + 96*i + 48]  = f0;

  }
  f0 = a[u512 6];
  f1 = a[u512 7];
  f0 = #VPSUB_32u16(f0, const_q);
  f1 = #VPSUB_32u16(f1, const_q);
  f2 = #VPSRA_32u16(f0, 15);
  f3 = #VPSRA_32u16(f1, 15);
  f2 = #VPAND_512(f2, const_q);
  f3 = #VPAND_512(f3, const_q);
  f0 = #VPADD_32u16(f0, f2);
  f1 = #VPADD_32u16(f1, f3);
  f2 = #VPSLL_32u16(f1, 12);
  f2 = #VPOR_512(f0, f2);
  f3 = #VPSRL_32u16(f1, 4);
  g = #VEXTRACTI64X4(f2, 1);
  f4 = #VINSERTI64X4(f4, g, 0);
  g = #VEXTRACTI64X4(f3, 1);
  f5 = #VINSERTI64X4(f5, g, 0);
  // f0 = _mm512_maskz_permutexvar_epi8(mmask1, idx1, f2);
  // f1 = _mm512_maskz_permutexvar_epi8(mmask2, idx2, f3);
  f0 = #VPOR_512(f0, f1);
  // f2 = _mm512_maskz_permutexvar_epi8(mmask1, idx1, f4);
  // f3 = _mm512_maskz_permutexvar_epi8(mmask2, idx2, f5);
  f2 = #VPOR_512(f2, f3);

  (u512)[rp + 576] = f0;
  (u512)[rp + 672]  = f0;
  return a;

}
// Transform from AVX order to bitreversed order
inline 
fn __nttpack256(reg u512 r0 r1 r2 r3 r4 r5 r6 r7)
    -> reg u512, reg u512, reg u512, reg u512, reg u512, reg u512, reg u512, reg u512
{
  reg u512 r8, r9;
  r8, r9 = __shuffle1(r0, r1);
  r0, r1 = __shuffle1(r2, r3);
  r2, r3 = __shuffle1(r4, r5);
  r4, r5 = __shuffle1(r6, r7);

  r6, r7 = __shuffle2(r8, r9);
  r8, r9 = __shuffle2(r0, r1);
  r0, r1 = __shuffle2(r2, r3);
  r2, r3 = __shuffle2(r4, r5);

  r4, r5 = __shuffle4(r6, r7);
  r6, r7 = __shuffle4(r8, r9);
  r8, r9 = __shuffle4(r0, r1);
  r0, r1 = __shuffle4(r2, r3);

  r2, r3 = __shuffle8(r4, r5);
  r4, r5 = __shuffle8(r6, r7);
  r6, r7 = __shuffle8(r8, r9);
  r8, r9 = __shuffle8(r0, r1);

  r0, r1 = __shuffle16(r2, r3);
  r2, r3 = __shuffle16(r4, r5);
  r4, r5 = __shuffle16(r6, r7);
  r6, r7 = __shuffle16(r8, r9);

  return r0, r1, r2, r3, r4, r5, r6, r7;
}
// Transform from bitreversed order to AVX order
inline
fn __nttunpack256(reg u512 r0 r1 r2 r3 r4 r5 r6 r7)
    -> reg u512, reg u512, reg u512, reg u512, reg u512, reg u512, reg u512, reg u512
{
  reg u512 r8 r9;
  r8, r9 = __shuffle16(r0, r1);
  r0, r1 = __shuffle16(r2, r3);
  r2, r3 = __shuffle16(r4, r5);
  r4, r5 = __shuffle16(r6, r7);

  r6, r7 = __shuffle8(r8, r9);
  r8, r9 = __shuffle8(r0, r1);
  r0, r1 = __shuffle8(r2, r3);
  r2, r3 = __shuffle8(r4, r5);

  r4, r5 = __shuffle4(r6, r7);
  r6, r7 = __shuffle4(r8, r9);
  r8, r9 = __shuffle4(r0, r1);
  r0, r1 = __shuffle4(r2, r3);

  r2, r3 = __shuffle2(r4, r5);
  r4, r5 = __shuffle2(r6, r7);
  r6, r7 = __shuffle2(r8, r9);
  r8, r9 = __shuffle2(r0, r1);

  r0, r1 = __shuffle1(r2, r3);
  r2, r3 = __shuffle1(r4, r5);
  r4, r5 = __shuffle1(r6, r7);
  r6, r7 = __shuffle1(r8, r9);

  return r0, r1, r2, r3, r4, r5, r6, r7;
}
