require "gen_fipsavx512asm.jazz"
require "params.jinc"
inline
fn mont_red(reg u512 lo hi qx32 qinvx32) -> reg u512 {
  reg u512 m;

  m  = #VPMULL_32u16(lo, qinvx32);
  m  = #VPMULH_32u16(m, qx32);
  lo = #VPSUB_32u16(hi, m);

  return lo;
}

inline 
fn __csubq(reg u512 r qx16) -> reg u512
{
  reg u512 t;
  r = #VPSUB_32u16(r, qx16);
  t = #VPSRA_32u16(r, 15);
  t = #VPAND_512(t, qx16);
  r = #VPADD_32u16(t, r);
  return r;
}
u16[32] jqx32 = {MLKEM_Q, MLKEM_Q, MLKEM_Q, MLKEM_Q, MLKEM_Q, MLKEM_Q, MLKEM_Q, MLKEM_Q,
                 MLKEM_Q, MLKEM_Q, MLKEM_Q, MLKEM_Q, MLKEM_Q, MLKEM_Q, MLKEM_Q, MLKEM_Q,
                 MLKEM_Q, MLKEM_Q, MLKEM_Q, MLKEM_Q, MLKEM_Q, MLKEM_Q, MLKEM_Q, MLKEM_Q,
                 MLKEM_Q, MLKEM_Q, MLKEM_Q, MLKEM_Q, MLKEM_Q, MLKEM_Q, MLKEM_Q, MLKEM_Q};
u16[32] jqinvx32 = {MLKEM_QINV, MLKEM_QINV, MLKEM_QINV, MLKEM_QINV, MLKEM_QINV, MLKEM_QINV, MLKEM_QINV, MLKEM_QINV,
                 MLKEM_QINV, MLKEM_QINV, MLKEM_QINV, MLKEM_QINV, MLKEM_QINV, MLKEM_QINV, MLKEM_QINV, MLKEM_QINV,
                 MLKEM_QINV, MLKEM_QINV, MLKEM_QINV, MLKEM_QINV, MLKEM_QINV, MLKEM_QINV, MLKEM_QINV, MLKEM_QINV,
                 MLKEM_QINV, MLKEM_QINV, MLKEM_QINV, MLKEM_QINV, MLKEM_QINV, MLKEM_QINV, MLKEM_QINV, MLKEM_QINV};
u16[128] jzetasmul_exp  = {
  -1103,   1103,    430,   -430,    555,   -555,    843,   -843,  -1251,   1251,    871,   -871,   1550,  -1550,    105,   -105,
    422,   -422,    587,   -587,    177,   -177,   -235,    235,   -291,    291,   -460,    460,   1574,  -1574,   1653,  -1653,
   -246,    246,    778,   -778,   1159,  -1159,   -147,    147,   -777,    777,   1483,  -1483,   -602,    602,   1119,  -1119,
  -1590,   1590,    644,   -644,   -872,    872,    349,   -349,    418,   -418,    329,   -329,   -156,    156,    -75,     75,
    817,   -817,   1097,  -1097,    603,   -603,    610,   -610,   1322,  -1322,  -1285,   1285,  -1465,   1465,    384,   -384,
  -1215,   1215,   -136,    136,   1218,  -1218,  -1335,   1335,   -874,    874,    220,   -220,  -1187,   1187,  -1659,   1659,
  -1185,   1185,  -1530,   1530,  -1278,   1278,    794,   -794,  -1510,   1510,   -854,    854,   -870,    870,    478,   -478,
   -108,    108,   -308,    308,    996,   -996,    991,   -991,    958,   -958,  -1460,   1460,   1522,  -1522,   1628,  -1628
};
fn _poly_csubq(reg ptr u16[MLKEM_N] rp) -> reg ptr u16[MLKEM_N]
{
  reg u512 r qx16;
  inline int i;
  
  qx16 = jqx32[u512 0];

  for i=0 to 8 {
    r = rp.[u512 64*i];
    r = __csubq(r, qx16);
    rp.[u512 64*i] = r;
  }

  return rp;
}
u16[32] jvx32 = {20159, 20159, 20159, 20159, 20159, 20159, 20159, 20159,
                 20159, 20159, 20159, 20159, 20159, 20159, 20159, 20159,
                 20159, 20159, 20159, 20159, 20159, 20159, 20159, 20159,
                 20159, 20159, 20159, 20159, 20159, 20159, 20159, 20159};
u16 pc_shift1_s = 0x200;
u16 pc_mask_s = 0x0F;
u16 pc_shift2_s = 0x1001;
u32[16] pc_permidx_s = {0, 4, 8, 12, 1, 5, 9, 13, 2, 6, 10, 14, 3, 7, 11, 15};
fn _poly_compress(reg u64 rp, reg ptr u16[MLKEM_N] a) -> reg ptr u16[MLKEM_N]
{
    inline int i;
    reg u512 v shift1 shift2 mask permidx f0 f1 f2 f3;
    reg ptr u16[32] x32p;
    a = _poly_csubq(a);
    x32p = jvx32;
    v = x32p[u512 0];
    shift1 = #VPBROADCAST_32u16(pc_shift1_s);
    mask = #VPBROADCAST_32u16(pc_mask_s);
    shift2 = #VPBROADCAST_32u16(pc_shift2_s);
    permidx = pc_permidx_s[u512 0];
    for i=0 to MLKEM_N/128
    {
        f0 = a[u512 4*i];
        f1 = a[u512 4*i + 1];
        f2 = a[u512 4*i + 2];
        f3 = a[u512 4*i + 3];
        f0 = #VPMULH_32u16(f0, v);
        f1 = #VPMULH_32u16(f1, v);
        f2 = #VPMULH_32u16(f2, v);
        f3 = #VPMULH_32u16(f3, v); 
        f0 = #VPMULHRS_32u16(f0, shift1);
        f1 = #VPMULHRS_32u16(f1, shift1);
        f2 = #VPMULHRS_32u16(f2, shift1);
        f3 = #VPMULHRS_32u16(f3, shift1);
        f0 = #VPAND_512(f0, mask);
        f1 = #VPAND_512(f1, mask);
        f2 = #VPAND_512(f2, mask);
        f3 = #VPAND_512(f3, mask);
        f0 = #VPACKUS_32u16(f0, f1);
        f2 = #VPACKUS_32u16(f2, f3);
        f0 = #VPMADDUBSW_512(f0, shift2);
        f2 = #VPMADDUBSW_512(f2, shift2);
        f0 = #VPACKUS_32u16(f0, f2);
        f0 = #VPERMD_512(permidx, f0);
        (u512)[rp + 64*i] = f0;
    }
    return a;
}
export fn poly_compress_jazz(reg u64 rp, reg u64 ap) 
{
  stack u16[MLKEM_N] a;
  reg u16 t;
  inline int i;

  for i = 0 to MLKEM_N {
    t = (u16)[ap + 2*i];
    a[i] = t;
  }

  a = _poly_compress(rp, a);
}
u8[64] pd_jshufbidx = {0,  0,  0,  0,  1,  1,  1,  1,  2,  2,  2,  2,  3,  3,  3,  3,  
4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  
8,  8,  8,  8,  9,  9,  9,  9, 10, 10, 10, 10, 11, 11, 11, 11,  
12, 12, 12, 12, 13, 13, 13, 13, 14, 14, 14, 14, 15, 15, 15, 15};
u32 pd_mask_s = 0x00F0000F;
u32 pd_shift_s = 0x800800;
fn _poly_decompress(reg ptr u16[MLKEM_N] rp, reg u64 ap) -> reg ptr u16[MLKEM_N]
{
  inline int i;
  reg u512 f q shufbidx mask shift;
  reg u128 h;
  reg ptr u16[32] x32p;
  reg ptr u8[64] x64p;
  stack u128 sh;
  x32p = jqx32;
  q = x32p[u512 0];
  x64p = pd_jshufbidx;
  shufbidx = x64p[u512 0];
  mask = #VPBROADCAST_16u32(pd_mask_s);
  shift = #VPBROADCAST_16u32(pd_shift_s);
  f = #set0_512();
  for i = 0 to MLKEM_N/32
  {
    h = (128u)(u128)[ap + 16*i];
    sh = h;
    f = #VBROADCASTI32X4(sh);
    f = #VPERMB_512(shufbidx, f);
    f = #VPAND_512(f, mask);
    f = #VPMULL_32u16(f, shift);
    f = #VPMULHRS_32u16(f, q);
    rp[u512 i] = f;
  }
  return rp;

}
u32[16] pfm_shift_s = {3, 2, 1, 0, 3, 2, 1, 0, 3, 2, 1, 0, 3, 2, 1, 0};
u8[64] pfm_idx_s = {0, 1, 4, 5, 8, 9, 12, 13, 2, 3, 6, 7, 10, 11, 14, 15,
0, 1, 4, 5, 8, 9, 12, 13, 2, 3, 6, 7, 10, 11, 14, 15,
0, 1, 4, 5, 8, 9, 12, 13, 2, 3, 6, 7, 10, 11, 14, 15,
0, 1, 4, 5, 8, 9, 12, 13, 2, 3, 6, 7, 10, 11, 14, 15};
u32[16] pfm_idx32_1_s ={0, 0, 0, 0, 4, 4, 4, 4, 1, 1, 1, 1, 5, 5, 5, 5};
u32[16] pfm_idx32_2_s ={2, 2, 2, 2, 6, 6, 6, 6, 3, 3, 3, 3, 7, 7, 7, 7};
u64[8] pfm_idx_1_s = {0, 1, 8, 9, 4, 5, 12, 13};
u64[8] pfm_idx_2_s = {2, 3, 10, 11, 6, 7, 14, 15};
fn _poly_frommsg(reg ptr u16[MLKEM_N] rp, reg u64 ap) -> reg ptr u16[MLKEM_N]
{
  reg u512 h0_tmp0 h0_tmp1 h2_tmp0 h2_tmp1;
  reg u512 f g0 g1 g2 g3 h0 h1 h2 h3 shift idx8 idx32_1 idx32_2 hqs idx_1 idx_2;
  stack u16 halfq = (MLKEM_Q + 1)/2;

  shift = pfm_shift_s[u512 0];
  idx8 = pfm_idx_s[u512 0];
  idx32_1 = pfm_idx32_1_s[u512 0];
  idx32_2 = pfm_idx32_2_s[u512 0];
  hqs = #VPBROADCAST_32u16(halfq);
  idx_1 = pfm_idx_1_s[u512 0];
  idx_2 = pfm_idx_2_s[u512 0];


  f = #VBROADCASTI64X4((u256)[ap + 0]);

  g3 = #VPERMD_512(idx32_1, f);
  g3 = #VPSLLV_16u32(g3, shift);
  g3 = #VPSHUFB_512(g3, idx8);

  g0 = #VPSLL_32u16(g3, 12);
  g1 = #VPSLL_32u16(g3, 8);
  g2 = #VPSLL_32u16(g3, 4);

  g0 = #VPSRA_32u16(g0, 15);
  g1 = #VPSRA_32u16(g1, 15);
  g2 = #VPSRA_32u16(g2, 15);
  g3 = #VPSRA_32u16(g3, 15);

  g0 = #VPAND_512(g0,hqs);
  g1 = #VPAND_512(g1,hqs);
  g2 = #VPAND_512(g2,hqs);
  g3 = #VPAND_512(g3,hqs);

  h0 = #VPUNPCKL_8u64(g0,g1);
  h2 = #VPUNPCKH_8u64(g0,g1);
  h1 = #VPUNPCKL_8u64(g2,g3);
  h3 = #VPUNPCKH_8u64(g2,g3);
  
  h0_tmp0 = h0;
  h0_tmp1 = h0;
  h2_tmp0 = h2;
  h2_tmp1 = h2;
  h0_tmp0 = #VPERMT2Q_512(h0_tmp0, idx_1, h1);
  h0_tmp1 = #VPERMT2Q_512(h0_tmp1, idx_2, h1);
  h2_tmp0 = #VPERMT2Q_512(h2_tmp0, idx_1, h3);
  h2_tmp1 = #VPERMT2Q_512(h2_tmp1, idx_2, h3);


  h0 = #VSHUFI32X4(h0_tmp0, h2_tmp0, 0x44);
  h1 = #VSHUFI32X4(h0_tmp0, h2_tmp0, 0xEE);
  h2 = #VSHUFI32X4(h0_tmp1, h2_tmp1, 0x44);
  h3 = #VSHUFI32X4(h0_tmp1, h2_tmp1, 0xEE);



  rp[u512 0] = h0;
  rp[u512 1] = h1;
  rp[u512 4] = h2;
  rp[u512 5] = h3;

  g3 = #VPERMD_512(idx32_2, f);
  g3 = #VPSLLV_16u32(g3, shift);
  g3 = #VPSHUFB_512(g3, idx8);

  g0 = #VPSLL_32u16(g3, 12);
  g1 = #VPSLL_32u16(g3, 8);
  g2 = #VPSLL_32u16(g3, 4);

  g0 = #VPSRA_32u16(g0, 15);
  g1 = #VPSRA_32u16(g1, 15);
  g2 = #VPSRA_32u16(g2, 15);
  g3 = #VPSRA_32u16(g3, 15);

  g0 = #VPAND_512(g0,hqs);
  g1 = #VPAND_512(g1,hqs);
  g2 = #VPAND_512(g2,hqs);
  g3 = #VPAND_512(g3,hqs);

  h0 = #VPUNPCKL_8u64(g0,g1);
  h2 = #VPUNPCKH_8u64(g0,g1);
  h1 = #VPUNPCKL_8u64(g2,g3);
  h3 = #VPUNPCKH_8u64(g2,g3);


  h0_tmp0 = h0;
  h0_tmp1 = h0;
  h2_tmp0 = h2;
  h2_tmp1 = h2;
  h0_tmp0 = #VPERMT2Q_512(h0_tmp0, idx_1, h1);
  h0_tmp1 = #VPERMT2Q_512(h0_tmp1, idx_2, h1);
  h2_tmp0 = #VPERMT2Q_512(h2_tmp0, idx_1, h3);
  h2_tmp1 = #VPERMT2Q_512(h2_tmp1, idx_2, h3);

  h0 = #VSHUFI32X4(h0_tmp0, h2_tmp0, 0x44);
  h1 = #VSHUFI32X4(h0_tmp0, h2_tmp0, 0xEE);
  h2 = #VSHUFI32X4(h0_tmp1, h2_tmp1, 0x44);
  h3 = #VSHUFI32X4(h0_tmp1, h2_tmp1, 0xEE);


  rp[u512 2] = h0;
  rp[u512 3] = h1;
  rp[u512 6] = h2;
  rp[u512 7] = h3;
  return rp;

}
export fn poly_frommsg_jazz(reg u64 rp, reg u64 ap) 
{
  stack u16[MLKEM_N] r;
  reg u16 t;
  inline int i;

  r = _poly_frommsg(r, ap);

  for i = 0 to MLKEM_N {
    t = r[i];
    (u16)[rp + 2*i] = t;
  }
}

u64[8] idx_cbd2 = {0, 1, 4, 5, 2, 3, 6, 7};
inline fn __cbd2(reg ptr u16[MLKEM_N] rp, reg ptr u8[MLKEM_ETA2*MLKEM_N/4] buf) -> reg ptr u16[MLKEM_N]
{
  inline int i;
  reg u512 f0 f1 f2 f3;
  reg u512 mask55 mask33 mask03 mask0F;
  reg u256 t;
  reg u512 idx;
  stack u32 const_mask55 = 0x55555555;
  stack u32 const_mask33 = 0x33333333;
  stack u32 const_mask0F = 0x0F0F0F0F;
  idx = idx_cbd2[u512 0];
  for i = 0 to 2
  {
    f0 = buf[u512 i];

    f1 = #VPSRL_32u16(f0, 1);
    f0 = #VPAND_512(mask55, f0);
    f1 = #VPAND_512(mask55, f1);
    f0 = #VPADD_64u8(f0, f1);

    f1 = #VPSRL_32u16(f0, 2);
    f0 = #VPAND_512(mask33, f0);
    f1 = #VPAND_512(mask33, f1);
    f0 = #VPADD_64u8(f0, mask33);
    f0 = #VPSUB_64u8(f0, f1);

    f1 = #VPSRL_32u16(f0, 4);
    f0 = #VPAND_512(mask0F, f0);
    f1 = #VPAND_512(mask0F, f1);
    f0 = #VPSUB_64u8(f0, mask03);
    f1 = #VPSUB_64u8(f1, mask03);

    f2 = #VPUNPCKL_64u8(f0, f1);
    f3 = #VPUNPCKL_64u8(f0, f1);

    f0 = #VSHUFI32X4(f2,f3,0x44);
    f1 = #VSHUFI32X4(f2,f3,0xEE);
    f2 = #VPERMQ512(idx, f0);
    f3 = #VPERMQ512(idx, f1);

    t = (256u)f2;
    f0 = #VPMOVSX_32u8_32u16(t);
    t = #VEXTRACTI32X8(f2, 1);
    f1 = #VPMOVSX_32u8_32u16(t);
    t = (256u)f3;
    f2 = #VPMOVSX_32u8_32u16(t);
    t = #VEXTRACTI32X8(f3, 1);
    f3 = #VPMOVSX_32u8_32u16(t);
    rp[u512 4*i] = f0;
    rp[u512 4*i + 1] = f1;
    rp[u512 4*i + 2] = f2;
    rp[u512 4*i + 3] = f3;
  }
  return rp;
}
inline
fn __poly_cbd_eta1(reg ptr u16[MLKEM_N] rp, reg ptr u8[MLKEM_ETA1*MLKEM_N/4] buf) -> reg ptr u16[MLKEM_N]
{
  rp = __cbd2(rp, buf[0:MLKEM_ETA1*MLKEM_N/4]);
  return rp;
}
fn _poly_getnoise_eta1_8x(reg ptr u16[MLKEM_N] r0 r1 r2 r3 r4 r5 r6 r7, reg ptr u8[MLKEM_SYMBYTES] seed, reg u8 nonce) -> reg ptr u16[MLKEM_N], reg ptr u16[MLKEM_N], reg ptr u16[MLKEM_N], reg ptr u16[MLKEM_N], reg ptr u16[MLKEM_N], reg ptr u16[MLKEM_N], reg ptr u16[MLKEM_N], reg ptr u16[MLKEM_N]
{
  reg u512 f;
  stack u512[25] state;
  stack u8[NOISE_NBLOCKS * SHAKE256_RATE] buf0 buf1 buf2 buf3 buf4 buf5 buf6 buf7;
  f = seed[u512 0];
  buf0[u512 0] = f;
  buf1[u512 0] = f;
  buf2[u512 0] = f;
  buf3[u512 0] = f;
  buf4[u512 0] = f;
  buf5[u512 0] = f;
  buf6[u512 0] = f;
  buf7[u512 0] = f;

  buf0.[32] = nonce;
  nonce += 1;
  buf1.[32] = nonce;
  nonce += 1;
  buf2.[32] = nonce;
  nonce += 1;
  buf3.[32] = nonce;

  buf4.[32] = nonce;
  nonce += 1;
  buf5.[32] = nonce;
  nonce += 1;
  buf6.[32] = nonce;
  nonce += 1;
  buf7.[32] = nonce;

  state = _shake256_absorb8x_33(state, buf0[0:33], buf1[0:33], buf2[0:33], buf3[0:33], buf4[0:33], buf5[0:33], buf6[0:33], buf7[0:33]);
  state, buf0, buf1, buf2, buf3, buf4, buf5, buf6, buf7 = __shake256_squeezenblocks8x(state, buf0, buf1, buf2, buf3, buf4, buf5, buf6, buf7);

  r0 = __poly_cbd_eta1(r0, buf0[0:MLKEM_ETA1*MLKEM_N/4+(MLKEM_ETA1 - 2)*8]);
  r1 = __poly_cbd_eta1(r1, buf1[0:MLKEM_ETA1*MLKEM_N/4+(MLKEM_ETA1 - 2)*8]);
  r2 = __poly_cbd_eta1(r2, buf2[0:MLKEM_ETA1*MLKEM_N/4+(MLKEM_ETA1 - 2)*8]);
  r3 = __poly_cbd_eta1(r3, buf3[0:MLKEM_ETA1*MLKEM_N/4+(MLKEM_ETA1 - 2)*8]);
  r4 = __poly_cbd_eta1(r4, buf0[0:MLKEM_ETA1*MLKEM_N/4+(MLKEM_ETA1 - 2)*8]);
  r5 = __poly_cbd_eta1(r5, buf1[0:MLKEM_ETA1*MLKEM_N/4+(MLKEM_ETA1 - 2)*8]);
  r6 = __poly_cbd_eta1(r6, buf2[0:MLKEM_ETA1*MLKEM_N/4+(MLKEM_ETA1 - 2)*8]);
  r7 = __poly_cbd_eta1(r7, buf3[0:MLKEM_ETA1*MLKEM_N/4+(MLKEM_ETA1 - 2)*8]);

  return r0, r1, r2, r3, r4, r5, r6, r7;

}
export fn poly_decompress_jazz(reg u64 rp, reg u64 ap) 
{
  stack u16[MLKEM_N] r;
  reg u16 t;
  inline int i;

  r = _poly_decompress(r, ap);

  for i = 0 to MLKEM_N {
    t = r[i];
    (u16)[rp + 2*i] = t;
  }
}
export fn poly_csubq_jazz_avx512(reg u64 rp)
{
  stack u16[MLKEM_N] r;
  reg u16 t;
  inline int i;

  for i = 0 to MLKEM_N {
    t = (u16)[rp + 2*i];
    r[i] = t;
  }

  r = _poly_csubq(r);

  for i = 0 to MLKEM_N {
    t = r[i];
    (u16)[rp + 2*i] = t;
  }
}

fn _poly_sub_avx512(reg ptr u16[MLKEM_N] rp bp)  -> reg ptr u16[MLKEM_N]
{
    inline int i;
    reg u512 a;
    reg u512 b;
    reg u512 r;
    for i = 0 to 8 {
        a = rp.[u512 64*i];
        b = bp.[u512 64*i];
        r = #VPSUB_32u16(a, b);
        rp.[u512 64*i] = r;
    }
    return rp;
}
export fn poly_sub_jazz_avx512(reg u64 rp, reg u64 bp)
{
    stack u16[MLKEM_N] r;
    stack u16[MLKEM_N] b;
    reg u16 t;
    inline int i;
    for i = 0 to MLKEM_N {
        t = (u16)[rp + 2*i];
        r[i] = t;
        t = (u16)[bp + 2*i];
        b[i] = t;
    }
    r = _poly_sub_avx512(r, b);
    for i = 0 to MLKEM_N {
        t = r[i];
        (u16)[rp + 2*i] = t;
    }
}
fn _poly_add_avx512(reg ptr u16[MLKEM_N] rp bp)  -> reg ptr u16[MLKEM_N]
{
    inline int i;
    reg u512 a;
    reg u512 b;
    reg u512 r;
    for i = 0 to 8 {
        a = rp.[u512 64*i];
        b = bp.[u512 64*i];
        r = #VPADD_32u16(a, b);
        rp.[u512 64*i] = r;
    }
    return rp;
}

export fn poly_add_jazz_avx512(reg u64 rp, reg u64 bp)
{
    stack u16[MLKEM_N] r;
    stack u16[MLKEM_N] b;
    reg u16 t;
    inline int i;
    for i = 0 to MLKEM_N {
        t = (u16)[rp + 2*i];
        r[i] = t;
        t = (u16)[bp + 2*i];
        b[i] = t;
    }
    r = _poly_add_avx512(r, b);
    for i = 0 to MLKEM_N {
        t = r[i];
        (u16)[rp + 2*i] = t;
    }
}
fn _poly_add2(reg ptr u16[MLKEM_N] rp bp) -> reg ptr u16[MLKEM_N]
{
  inline int i;
  reg u256 a;
  reg u256 b;
  reg u256 r;

  for i = 0 to 16 {
    a = rp.[u256 32*i];
    b = bp.[u256 32*i];
    r = #VPADD_16u16(a, b);
    rp.[u256 32*i] = r;
  }

  return rp;
}

export fn poly_add2_jazz(reg u64 rp,  reg u64 bp) 
{
  stack u16[MLKEM_N] r;
  stack u16[MLKEM_N] b;
  reg u16 t;
  inline int i;

  for i = 0 to MLKEM_N {
    t = (u16)[rp + 2*i];
    r[i] = t;
    t = (u16)[bp + 2*i];
    b[i] = t;
  }

  r = _poly_add2(r, b);

  for i = 0 to MLKEM_N {
    t = r[i];
    (u16)[rp + 2*i] = t;
  }
}
inline fn __karatsuba32x(reg u512 al ah bl bh const_q const_qinv zeta) -> reg u512, reg u512
{
    reg u512 f0 f1 f2 f3 f4 f5 f6;
    f0 = #VPADD_32u16(al, ah); //zmm19
    f1 = #VPADD_32u16(bl, bh); //zmm20
    f2 = #VPMULH_32u16(f0, f1); //zmm21
    f3 = #VPMULH_32u16(al, bl); //zmm22
    f4 = #VPMULH_32u16(ah, bh); //zmm23

    f5 = #VPMULL_32u16(al, const_qinv);
    f6 = #VPMULL_32u16(ah, const_qinv);
    f0 = #VPMULL_32u16(f0, const_qinv);

    f5 = #VPMULL_32u16(bl, f5);
    f6 = #VPMULL_32u16(bh, f6);
    f0 = #VPMULL_32u16(f1, f0);

    f5 = #VPMULH_32u16(f5, const_q);
    f6 = #VPMULH_32u16(f6, const_q);
    f0 = #VPMULH_32u16(f0, const_q);

    f5 = #VPSUB_32u16(f3, f5);
    f6 = #VPSUB_32u16(f4, f6);
    f0 = #VPSUB_32u16(f2, f0);

    f1 = #VPSUB_32u16(f0, f5);
    f1 = #VPSUB_32u16(f1, f6);

    f2 = #VPMULH_32u16(f6, zeta);
    f6 = #VPMULL_32u16(f6, const_qinv);
    f6 = #VPMULL_32u16(f6, zeta);
    f6 = #VPMULH_32u16(f6, const_q);
    f6 = #VPSUB_32u16(f2, f6);

    f2 = #VPADD_32u16(f5, f6);

    return f2, f1;

}
fn _poly_basemul(reg ptr u16[MLKEM_N] rp ap bp) -> reg ptr u16[MLKEM_N]
{
  reg u512 const_q const_qinv;
  const_q = jqx32[u512 0];
  const_qinv = jqinvx32[u512 0];
  reg u512 f0 f1 f2 f3 f4 f5 f6 f7 g0 g1 g2 g3 g4 g5 g6 g7 zeta;
  reg u512 h0 h1;

  f0 = ap.[u512 0*64];
  f1 = ap.[u512 1*64];
  f2 = ap.[u512 2*64];
  f3 = ap.[u512 3*64];
  f4 = ap.[u512 4*64];
  f5 = ap.[u512 5*64];
  f6 = ap.[u512 6*64];
  f7 = ap.[u512 7*64];

  g0 = bp.[u512 0*64];
  g1 = bp.[u512 1*64];
  g2 = bp.[u512 2*64];
  g3 = bp.[u512 3*64];
  g4 = bp.[u512 4*64];
  g5 = bp.[u512 5*64];
  g6 = bp.[u512 6*64];
  g7 = bp.[u512 7*64];

  zeta = jzetasmul_exp.[u512 0];
  h0, h1 = __karatsuba32x(f0, f1, g0, g1, const_q, const_qinv, zeta);
  rp.[u512 0*64] = h0;
  rp.[u512 1*64] = h1;

  zeta = jzetasmul_exp.[u512 1*64];
  h0, h1 = __karatsuba32x(f2, f3, g2, g3, const_q, const_qinv, zeta);
  rp.[u512 2*64] = h0;
  rp.[u512 3*64] = h1;

  zeta = jzetasmul_exp.[u512 2*64];
  h0, h1 = __karatsuba32x(f4, f5, g4, g5, const_q, const_qinv, zeta);
  rp.[u512 4*64] = h0;
  rp.[u512 5*64] = h1;

  zeta = jzetasmul_exp.[u512 3*64];
  h0, h1 = __karatsuba32x(f6, f7, g6, g7, const_q, const_qinv, zeta);
  rp.[u512 6*64] = h0;
  rp.[u512 7*64] = h1;

  return rp;

}
export fn poly_basemul_jazz(reg u64 rp, reg u64 ap, reg u64 bp) 
{
  stack u16[MLKEM_N] a;
  stack u16[MLKEM_N] b;
  stack u16[MLKEM_N] r;
  reg u16 t;
  inline int i;

  for i = 0 to MLKEM_N {
    t = (u16)[ap + 2*i];
    a[i] = t;
    t = (u16)[bp + 2*i];
    b[i] = t;
    t = (u16)[rp + 2*i];
    r[i] = t;
  }

  r = _poly_basemul(r, a, b);

  for i = 0 to MLKEM_N {
    t = r[i];
    (u16)[rp + 2*i] = t;
  }
}

